Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

MA333 Introduction to Big Data Analysis Course Introduction

Zhen Zhang

Southern University of Science and Technology

Conclusion

Course Syllabus

What Is Data Science

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Conclusion

Machine Learning

Outlines

Mathematical Representation

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Course Info

Semester 2023-2024 Spring • Instructor : ZHANG, Zhen £(cid:220)(cid:8)⁄ • Oﬃce : Room M5014, School of Science

Phone : 88018753

Email : zhangz@sustech.edu.cn

Oﬃce hours : Wednesday afternoon, 15 :00-17 :00; or send email to make an appointment for other time.

Lecture : 3 credits, 3 hours per week.

Prerequisite : Calculus I&II, MA101b&MA102b, (or Mathematical Analysis I&II, MA101a&MA102a); Linear Algebra I, MA103b; Probability Theory, MA215 (or Probability Theory and Mathematical Statistics).

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Grading Policy

Homework : Approximately 6 homework assignments (including programming assignments and written problems). The written homework could be handed in after class.

In-class quizzes : typically once every two weeks, test how well you learned about the basic concepts, including ﬁll-in-the-blank, single and multiple choices, and simple Q & A

Programming projects : need to write codes and reports

One closed-book ﬁnal exam

Grading policy : assignments (30%), quizzes (15%), programming projects (20%), and the ﬁnal exam (35%).

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Contents

Intended for undergraduate students who are interested in pursuing industrial work and research in big data science.

Concise and self-contained introduction to mathematical aspect of big data science, including theoretical analysis, algorithms and programming with python

Major topics :

Introduction to python programming and data preprocessing • Three fundamental problems : classiﬁcation, regression, clustering

Model selection, dimensionality reduction • Hot topics : text analysis, social network analysis, neural network and deep learning, distributed computing, and recommender systems if time permits

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

References

Œ(cid:226)(cid:137)˘(cid:19)(cid:218)§(cid:238)p(cid:246)(cid:31)˝§p(cid:31)(cid:19)(cid:152)(cid:209)(cid:135)(cid:22)§2017. • ¯(cid:236)˘S§–(cid:147)u˝§(cid:152)u(cid:140)˘(cid:209)(cid:135)(cid:22)§2016. • An Introduction to Statistical Learning with Applications in R, by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, Springer, 2013. http ://web.stanford.edu/ hastie/pub.htm

Pattern Recognition and Machine Learning, by Christopher M. Bishop, Springer, 2006.

The Elements of Statistical Machine Learning : Data mining, Inference and Prediction, 2nd Edition, by Trevor Hastie, Robert Tibshirani, and Jerome Friedman, Springer, 2009.

Understanding Machine Learning, by Shai Shalev-Shwartz and Shai Ben-David, Cambridge University Press, 2018.

Deep learning, by Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016.

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

References

Mathematical Representation

Conclusion

Course Syllabus

What Is Data Science

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Conclusion

Machine Learning

Outlines

Mathematical Representation

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Some Examples of Data

Can you give some examples of data?

Table, 1D signal (audio, stock price), 2D signal (image), 3D signal (video), etc.

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Big Data : 4 Big “V”

Volume : KB, MB, GB (109 bytes), TB, PB, EB (1018 bytes), ZB, YB

Variety : diﬀerent sources from business to industry, diﬀerent types

Value : redundant information contained in the data, need to retrieve useful information

Velocity : fast speed for information transfer

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

What is data science

Retrieve information from data with the help of computational power

Transfer the information into knowledge • Two perspectives of data sciences :

Study science with the help of data : bioinformatics, astrophysics, geosciences, etc.

Use scientiﬁc methods to exploit data : statistics, machine learning, data mining, pattern recognition, data base, etc.

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Study Science with the Help of Data

A pioneering work of data science : Kepler’s Laws

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Scientiﬁc Study of Data

Grabbing data : business and industrial problem, professional areas

Storing data : engineering problem, computer science, electronic engineering

Analyzing data (key problem) : scientiﬁc problem, mathematics, statistics, computer science

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Data Analysis

Ordinary data types :

Table : classical data (could be treated as matrix) • Set of points : mathematical description • Time series : text, audio, stock prices, DNA sequences, etc. • Image : 2D signal (or matrix equivalently, e.g., pixels), MRI, CT, supersonic imaging

video : 2D in space and 1D in time (another kind of time series)

Webpage and newspaper : time series with spatial structure • Network : relational data, graph (nodes and edges)

Basic assumption : the data are generated from an underlying model, which is unknown in practice

Set of points : probability distribution • Time series : stochastic processes, e.g., Hidden Markov Model (HMM)

Image : random ﬁelds, e.g., Gibbs random ﬁelds • Network : graphical models, Beyesian models

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Diﬃculties

Huge volume of data • Extremely high dimensions

Curse of dimensionality : the model complexity and computational complexity become exponentially increasing with the growth of dimension

Solutions :

Make use of prior information • Restrict to simple models • Make use of special structures, e.g., sparsity, low rank, smoothness

Dimensionality reduction, e.g., PCA, LDA, etc.

Complex variety of data

Large noise : data are always contaminated with noises

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Solution - Algorithms

Algorithms are in the interdisciplinary part of computer science and mathematics : establish mathematical models, solve it numerically, implement it in the computer languages

Reduce the algorithmic complexity, with the help of techniques from mathematics or computer science

Distributional and parallel computing : divide-and-conquer, e.g., MapReduce, GPU

IEEE 2006 top 10 algorithms in data mining : C4.5, K-Means, SVM, Apriori, EM, PageRank, NaiveBayes, K-Nearest Neighbors, AdaBoost, CART

Conclusion

Course Syllabus

What Is Data Science

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Conclusion

Machine Learning

Outlines

Mathematical Representation

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Deﬁnition

Artiﬁcial Intelligence (AI) : learning from experiences (data), and improve the computer program adaptively

Mathematics : Learning the underlying model from data, and generalize the model to adapt new data

Mathematical Representation

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Related Areas

Control theory : optimize the cost with optimal control parameters

Information theory : entropy, optimal coding with best information

Psycology : reference for machine learning algorithms

Neuroscience : artiﬁcial neural network

Biology : genetic algorithms

Theory of Computing : study the computational complexity

Statistics : large-sample limiting behavior, statistical learning theory

Artiﬁcial Intelligence : symbolic computing

Bayesian theory : conditionally probabilistic network

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Applications

Mathematical Representation

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Supervised and Unsupervised Learning

Supervised learning : classiﬁcation, regression

Unsupervised learning : density estimation, clustering, dimensionality reduction

Semi-supervised learning : with missing data, e.g., EM; self-supervised learning, learn the missing part of images, inpainting

Reinforcement learning : play games, e.g., Go, StarCraft; robotics; auto-steering

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Supervised Learning

Given labels of data : the labels could be symbols (spam or non-spam), integers (0 or 1), real numbers, etc.

Training : ﬁnd the optimal parameters (or model) to minimize the error between the prediction and target

Classiﬁcation : SVM, KNN, Desicion tree, etc.

Regression : linear regression, CART, etc.

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Classiﬁcation

Output is discrete • Examples : given the study hours, in-class performance, and ﬁnal grades (Pass or Fail) of past students, can you predict the ﬁnal grades of the current students based on their study hours and in-class performance?

Applications : Credit risk evaluation, clinical prediction of tumor, classiﬁcation of protein functions, etc.

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Regression

Output is continuous • Examples : given the study hours, in-class performance, and ﬁnal scores of past students, can you predict the ﬁnal scores of the current students based on their study hours and in-class performance?

Applications : epidemiology, ﬁnance, investment analysis, etc.

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Unsupervised Learning

No labels

Optimize the parameters based on some natural rules, e.g., cohesion or divergence

Clutering : K-Means, SOM

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Representation of Data

Input space X = {All possible samples}; x ∈ X is an input vector, also called feature, predictor, independent variable, etc.; typically multi-dimensional; e.g., x ∈ Rp is a weight vector or coding vector

Output space Y = {All possible results}; y ∈ Y is an output vector, also called response, dependent variable, etc.; typically one-dimensional; e.g., y = 0 or 1 for classiﬁcation problems, y ∈ R for regression problems.

For supervised learning, assume that (x,y) ∼ P, a joint distribution on the sample space X × Y

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Supervised Learning

Goal : given x, predict what is y ; in deterministic settings, ﬁnd the dependence relation y = f (x); in probabilistic settings, ﬁnd the conditional distribution P(y|x) of y given x

Training dataset : {(xi,yi)}n i=1 approximation ˆf (x) or ˆP(y|x)

i.i.d.∼ P, used to learn an

Test dataset : {(xj,yj)}n+m j=n+1

i.i.d.∼ P, used to make a

prediction ˆyj = ˆf (xj) or ˆyj = argmax yj accurate the approximation is

ˆP(yj|xj), and verify how

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Unsupervised Learning

Goal : in probabilistic settings, ﬁnd the distribution P(x) of x and approximate it; there is no y

Training dataset : {xi}n i=1

i.i.d.∼ P, used to learn an

approximation ˆP(x); no test data in general

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Learning Models

Decision function (hypothesis) space :

F = {fθ|fθ = fθ(x),θ ∈ Θ} or F = {Pθ|Pθ = Pθ(y|x),θ ∈ Θ}

Loss function : a measure for the “goodness” of the prediction, L(y,f (x))

0-1 loss : L(y,f (x)) = Iy(cid:54)=f (x) = 1 − Iy=f (x) • Square loss : L(y,f (x)) = (y − f (x))2 • Absolute loss : L(y,f (x)) = |y − f (x)| • Cross-entropy loss : L(y,f (x)) = −y logf (x) − (1 − y)log(1 − f (x))

Risk : in average sense, R(f ) = EP[L(y,f (x))] = (cid:82)

X×Y L(y,f (x))P(x,y)dxdy

Target of learning : choose the best f ∗ to minimize Rexp(f ), Rexp(f )

f

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Risk Minimization Strategy

Empirical risk minimization (ERM) : given training set n (cid:80) i=1

Empirical risk minimization (ERM) : given training set n (cid:80) i=1

i=1, Remp(f ) = 1 n

{(xi,yi)}n

By law of large number,

lim n→∞

Optimization problem : min f ∈F 1 n

Remp(f ) = Rexp(f ) n (cid:80) i=1

L(yi,f (xi))

Structural risk minimization (SRM) : given training set {(xi,yi)}n Rsrm(f ) = 1 n

n (cid:80) i=1

L(yi,f (xi)) + λJ(f )

J(f ) measures how complex the model f is, typically the degree of complexity

λ (cid:62) 0 is a tradeoﬀ between the empirical risk and model complexity

Optimization problem : min f ∈F 1 n

n (cid:80) i=1

L(yi,f (xi)) + λJ(f )

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Algorithms

Computational methods to solve the problem for f • Numerical methods to solve the optimization problems • Gradient descent method, including coordinate descent, sequential minimal optimization (SMO), etc. • Newton’s method and quasi-Newton’s method • Combinatorial optimization • Genetic algorithms • Monte Carlo methods • ...

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Model Assessment

Assume we have learned the model y = ˆf (x), what is the error?

Training error : Remp(ˆf ) = 1 n

n (cid:80) i=1

L(yi,ˆf (xi)), tells the diﬃculty

of learning problem

Test error : etest(ˆf ) = 1 m

n+m (cid:80) j=n+1

L(yj,ˆf (xj)), tells the capability

of prediction; in particular, if 0-1 loss is used

Error rate : etest(ˆf ) = 1 m

Accuracy : rtest(ˆf ) = 1 m

n+m (cid:80) j=n+1 n+m (cid:80) j=n+1

Iyj(cid:54)=ˆf (xj)

Iyj=ˆf (xj)

etest + rtest = 1

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Model Assessment (Cont’)

Generalization error :

Rexp(ˆf ) = EP[L(y,ˆf (x))] = (cid:82)

L(y,ˆf (x))P(x,y)dxdy, tells

X×Y the capability for predicting unknown data from the same distribution, its upper bound M deﬁnes the generalization ability

As n → ∞, M → 0 • As F becomes larger, M increases

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Overﬁtting

Too many model parameters

Better for training set, but worse for test set

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Model Selection

Regularization : min f ∈F 1 n

n (cid:80) i=1

L(yi,f (xi)) + λJ(f ) (cid:124) (cid:123)(cid:122) (cid:125) penalty

, choose λ to

minimize empirical risk and model complexity simultaneously

Cross-validation (CV) : split the training set into training subset and validation subset, use training set to train diﬀerent models repeatedly, use validation set to select the best model with the smallest (validation) error

Simple CV : randomly split the data into two subsets • K-fold CV : randomly split the data into K disjoint subsets with the same size, treat the union of K −1 subsets as training set, the other one as validation set, do this repeatedly and select the best model with smallest mean (validation) error

Leave-one-out CV : K = n in the previous case

Conclusion

Course Syllabus

What Is Data Science

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Conclusion

Machine Learning

Outlines

Mathematical Representation

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Data Science VS. Other Techniques

Data science is an interdisciplinary area using mathematics, statistics, computer science and engineering, and other profession techniques

Conclusion

Course Syllabus

What Is Data Science

Machine Learning

Mathematical Representation

Where Math and Statistics Emerge

Conclusion

Basic Concepts

MA333 Introduction to Big Data Analysis Data Preprocessing

Zhen Zhang

Southern University of Science and Technology

Data Preprocessing

Basic Concepts

Basic Concepts

Data Preprocessing

Outlines

Data Preprocessing

Basic Concepts

Data Types

Tabular data : matrices, vectors, objects, relations, etc.

Data objects : also called samples, examples, instances, data points, objects, tuples, vectors

Attributes : each row of a table, also called dimensions, features, variables

Graphical data : networks, graphs, etc.

Multi-media data : texts, images, videos, audios, etc.

Data Preprocessing

Basic Concepts

Types of Attributes

Discrete : x ∈ some countable sets, e.g., N

Nominal : Countries={China, US, UK, France, Germany}, Universities={Peking U, Tsinghua U, SUSTech, Shenzhen U, HIT}, not comparable

Boolean : 0 or 1, male or female, spam or non-spam, etc. • Ordinal : Heights={tall, short}, Scores={A+, A, A-, B+, B, B-, C, C-, D, F}, can be compared, but cannot operated arithmetically

Continuous : x ∈ some subset in Rn

Numerical : Income, exact marks, weights, etc., can be operated arithmetically

Data Preprocessing

Basic Concepts

Data Preprocessing

Basic Statistics

Mean : EX = minc E(X − c)2 ≈ 1 n • Median : (cid:40)

Mean : EX = minc E(X − c)2 ≈ 1 n • Median : (cid:80)n

i=1 xi

x( n+1 2 ) 2) + x( n (x( n

E|X − c| =

min c

2+1))/2 otherwise

Maximum : max i

xi ; Minimum : min

i

xi

Quantile : a generalization of median, k-th q-quantile xq : P[X < xq] (cid:54) k/q ; interquartile range (IQR)=Q3(75%) − Q1(25%)

Variance : Var(X) = E[X − EX]2 ≈ 1 n (cid:80)n

i=1(xi − ¯x)2 ;

Mode : minc E|X − c|0 = the most frequently occurring value (deﬁne 00 = 0)

Basic Concepts

Central Tendency

For one-peak skewed density distribution, empirical formula : Mean − Mode = 3 × (Mean − Median)

Data Preprocessing

Basic Concepts

Measure the dispersion of data

Box Plot

Data Preprocessing

Basic Concepts

Metrics

Proximity :

Similarity : range is [0,1] • Dissimilarity : range is [0,∞], sometimes distance

For nominal data, d(xi,xj) = encoding into Boolean data (cid:80)

k I(xi,k(cid:54)=xj,k) p

; or one-hot

For Boolean data, symmetric distance d(xi,xj) = r+s q+r+s+t ; non-symmetric q+r+s+t or

Data Preprocessing

Basic Concepts

Data Preprocessing

Metrics : Distance

Example : Let H = F = 1 and L = S = 0, 4+1+0 =

Example : Let H = F = 1 and L = S = 0, d(LandRover,Jeep) = 1+0 0.20,d(LandRover,TOYOTA) = 3+1 0.80,d(Jeep,TOYOTA) = 3+2

Minkowski distance : d(xi,xj) = h Lh-norm

Minkowski distance : d(xi,xj) = h Lh-norm

(cid:113)(cid:80)p

Positive deﬁniteness d(xi,xj) (cid:62) 0 and “=” if and only if i = j ; • Symmetry d(xi,xj) = d(xj,xi); • Triangle inequality d(xi,xj) (cid:54) d(xi,xk) + d(xk,xj)

Basic Concepts

Metrics : Distance (Cont’)

Manhattan distance : h = 1, and d(xi,xj) = (cid:80)p

k=1 |xik − xjk| • Euclidean distance : h = 2,

and d(xi,xj) = (cid:113)(cid:80)p

k=1 |xik − xjk|2 • Supremum distance :

(a) Manhattan

h = ∞, and d(xi,xj) = maxp

k=1 |xik −xjk|

(b) Euclidean

(c) Supremum

Data Preprocessing

Basic Concepts

Data Preprocessing

Metrics : Cosine Similarity

Euclidean vs. Cosine :

Deﬁnition : cos(xi,xj) = k=1 xikxjk (cid:113)(cid:80)p xi·xj (cid:107)xi(cid:107)(cid:107)xj(cid:107)

Euclidean : measures the distance in absolute value, many applications

Cosine : insensitive to

Example : cos(x1,x2) = 0.94

absolute value, e.g., analyze users’ interests based on movie ratings

Basic Concepts

Data Preprocessing

Metrics : Other Distances

For ordinal data, mapping the data to numerical data : n−1 ∈ [0,1]

For mixed type, use weighted distance with prescribed weights :

d(xi,xj) =

(cid:80)G

g=1 w(g) (cid:80)G

ij d(g)

ij

g=1 w(g)

ij

Put the attributes of the same type into groups, for each data type g, use the corresponding distance d(g)

ij

Basic Concepts

Basic Concepts

Data Preprocessing

Outlines

Data Preprocessing

Basic Concepts

Why Data Preprocessing?

Missing values

Noisy with outliers

Inconsistent representations

Redundancy

Errors may come during data input, data gathering, and data transferring

Errors occur in about 5% of the data

Data Preprocessing

Basic Concepts

Four Types of Data Preprocessing

(a) Data cleaning

(b) Data integration

(c) Data conversion

(d) Data reduction

Data Preprocessing

Basic Concepts

Data Preprocessing

Data Scaling

Why scaling :

For better performance : e.g., RBF in SVM and penalty in Lasso/ridge regression assume the zero mean and unit variance • Normalize diﬀerent dimensions : many algorithms are sensitive to the variables with large variances, e.g., height (1.75m) and weight (70kg) in distance calculation i = xi−ˆµ

Z-score scaling : x∗

ˆσ , ˆµ : sample mean, ˆσ : sample

variance, applicable if max and min are unknown and the data distributes well

Basic Concepts

Data Preprocessing

Data Scaling (Cont’)

i = xi−mink xk

0-1 scaling : x∗

∈ [0,1], applicable for

maxk xk−mink xk

bounded data sets, need to recompute the max and min when new data arrive • Decimal scaling : x∗ many magnitudes

i = xi

10k , applicable for data varying across

Logistic scaling : sigmoid transform x∗ for data concentrating nearby origin i = 1

1+e−xi , applicable

Basic Concepts

Data Preprocessing

Data Discretization

Why discretization :

Improve the robustness : removing the outliers by putting them into certain intervals • For better interpretation • Reduce the storage and computational power

Unsupervised discretization : equal-distance discretization, equal-frequency discretization, clustering-based discretization, 3σ-based discretization

Supervised discretization : information gain based discretization, χ2-based discretization

Basic Concepts

Data Preprocessing

Unsupervised Discretization

Equal-distance discretization : split the range to n intervals (bins) with the same length, group the data into each bin, sensitive to outliers

Equal-frenquency discretization : group the data into n subset so that each subset has the same number of points, tend to separate samples with similar values and produce uniform distribution

Clustering-based discretization : do hierarchical clustering and form a hierarchical structure (e.g., using K-Means), and put the samples in the same branch into the same interval (a natural example is family tree)

3σ-based discretization : put the samples into 8 intervals, need to take logarithm ﬁrst

Basic Concepts

Supervised Discretization - Information Gain

Top-down splitting, similar to create a decision tree • Do a decision tree classiﬁcation using information gain, ﬁnd a proper splitting point for each continuous variable such that the information gain increases the most

The ﬁnal leaf nodes summarize the discrete intervals

Data Preprocessing

Basic Concepts

Data Preprocessing

Supervised Discretization - ChiMerge

Bottom-up : similar to hierarchical clustering • ˆχ2 statistics proposed by Karl Pearson, is used to test whether the observations dramatically deviate from theoretical (Ai−EAi)2 distribution : ˆχ2 = (cid:80)k EAi is the number of samples in the i-th interval Ai = [ai−1,ai] (frequency of observations), (cid:83)k i=1 Ai covers the range of the variable, and EAi = pi is its expectation computed from the theoretical distribution; it can be shown that ˆχ2 → χ2

(Ai−npi)2 npi

= (cid:80)k

, where ni

i=1

i=1

ChiMerge : Given a threshold level t,

k−1

1. Treat each value of the continuous variable as an interval and sort them in increasing order;

2. For each pair of adjacent intervals, compute its ˆχ2 statistics, if ˆχ2 < t, merge them into a new interval;

3. Repeat the above steps until no adjacent intervals can be merged.

Two shortcomings : t is hard to set appropriately; too long loop for large sample set, computationally intensive

Basic Concepts

Data Preprocessing

ChiMerge : Iris Data Example

(Aij−Eij)2 Eij

ˆχ2 = (cid:80)m i=1

(cid:80)k

ˆχ2 = (cid:80)m i=1

, where

j=1 m = 2 (two adjacent intervals) k is the number of classes Aij is the number of samples in i-th interval and in class k Ri = (cid:80)k of samples in i-th interval Cj = (cid:80)m of samples in class j N = (cid:80)m i=1 number of samples Eij = Ri · Cj

i=1 Aij is the total number

(cid:80)k

j=1 Aij is the total

N

χ2 of 4.3 and 4.4 : C1 = 4, C2 = 0, C3 = 0, N = 4, A11 = 1, A12 = A13 = 0, A21 = 3, A22 = A23 = 0, R1 = 1, R2 = 3, E11 = 1, E12 = E13 = 0, E21 = 3, E22 = E23 = 0, ˆχ2 = 0.

Figure: Sepal lengths of 3 types of iris

Basic Concepts

ChiMerge Results

Left : signiﬁcance level is 0.5 and the threshold for χ2 is 1.4; Right : signiﬁcance level is 0.9 and the threshold for χ2 is 4.6; The ﬁnal results keep the intervals with χ2 larger than the thresholds

Data Preprocessing

Basic Concepts

Data Preprocessing

Data Redundancy

When strong correlations exist among diﬀerent attributes, then we say that the some attributes can be derived from the others (Recall linear dependency for vectors)

E.g., two attributes “Age” and “Birthday”, then “Age” can be calculated from “Birthday”

Determine the data redundancy by correlation analysis • For continuous variables A and B, compute the correlation i=1(ai−¯A)(bi−¯B) kˆσAˆσB (cid:80)k

1. If r > 0, A and B are positively correlated; 2. If r < 0, A and B are negatively correlated; 3. If r = 0, A and B are uncorrelated.

Note that the correlation between A and B does not imply the causal inference.

For discrete variables A and B, compute the χ2 statistics : large ˆχ2 value implies small correlation

Basic Concepts

Missing Data

Where missing data come from?

Missing Completely At Random (MCAR) : the occurrence of missing data is a random event

Missing At Random (MAR) : depending on some control

variables, e.g., the age > 20 is not acceptable in an investigation for teenager and thus is replaced by MAR • Missing Not At Random (MNAR) : missing data for bad

performed employees after they are ﬁred

Data Preprocessing

Basic Concepts

Simple Methods

Deleting samples : for small size of samples with missing values

Deleting variables : for series missing values in variables

Data Preprocessing

Basic Concepts

Filling Methods

Filling with zero

Filling with means for numerical type, and with modes for non-numerical type, applicable for MCAR; drawback : concentrating in the mean and underestimating the variance; solution : ﬁlling in diﬀerent groups

Filling with similar variables : auto-correlation is introduced

Filling with past data

Filling by K-Means : Compute the pairwise distances of the data using good variables (no missing values), then ﬁll the missing values with the mean of the ﬁrst K most similar good data, auto-correlation is introduced

Filling with Expectation-Maximization (EM) : introduce hidden variables and use MLE to estimate the parameters (missing values)

Data Preprocessing

Basic Concepts

Data Preprocessing

Filling Methods (Cont’)

Random ﬁlling :

Bayesian Bootstrap : for discrete data with range {xi}k randomly sample k − 1 numbers from U(0,1) as {a(i)}k a(0) = 0 and a(k) = 1; then randomly sample from {xi}k with probability distribution {a(i) − a(i−1)}k ﬁll in the missing values i=1, i=0 with i=1

Approximate Bayesian Bootstrap(cid:181)Sample with replacement i }k∗

Model based methods : treat missing variable as y, other variables as x; take the data without missing values as our training set to train a classiﬁcation or regression model; take the data with missing values as our test set to predict the miss values

Basic Concepts

Filling by Interpolation

For the data of numeric type, each attribute (column vector) can be viewed as the function values zi = f (xi) at the points xi, where xi is a reference attribute (the reference attribute usually has no missing values, it can be chosen as the index)

We can interpolate a function f using the existing values (xi,zi), and then ﬁll in the missing values zk with f (xk)

Linear interpolation : treat z = f (x) as linear function between the neighboring points xk−1 and xk+1 of xk

Lagrange interpolation : interpolate the m + 1 existing values i=1 by a degree m polynomial Lm(x) {(xli,zli)}m+1

Data Preprocessing

Basic Concepts

Data Preprocessing

Special Values and Dummy Variables

In Python, “np.nan” means missing values (Not a Number, missing ﬂoat value)

“None” is a Python object, used to represent missing values of the object type

Dummy variables : e.g., missing values in gender (“Male” or “Female”),

then deﬁne a third value “unknown” for the missing values

Basic Concepts

Outliers

Outliers : the data points seem to come from diﬀerent distribution, or noisy data

Outlier detection : unsupervised, e.g., Credit cheating detection, medical analysis, and information security, etc.

Data Preprocessing

Basic Concepts

Outliers Detection - Statistics Based Methods

The samples outside the upper and lower α-quantile for some small α (usually 1%)

Observe from box plot

Data Preprocessing

Basic Concepts

Data Preprocessing

Outliers Detection - Local Outlier Factor

Local Outlier Factor (LOF) is a density based method :

1. We could compute the density at each position x, e.g., p(x) (how to deﬁne the density if we only have data samples);

2. We could compare the density of each point x with the density of its neighbors, i.e., compare p(x) with p(xk) where xk is close to x (in a neighborhood of x, but how to deﬁne the neighborhood)

Basic Concepts

Data Preprocessing

Computing Density by Distance

Some deﬁnitions :

d(A,B) : distance between A and B;

dk(A) : k-distance of A, or the distance between A and the k-th nearest point from A

Nk(A) : k-distance neighborhood of A, or the points within dk(A) from A;

rdk(B,A) : k-reach-distance from A to B, the repulsive distance from A to B as if A has a hard-core with radius dk(A), rdk(B,A) = max{dk(A),d(A,B)}; note that rdk(A,B) (cid:54)= rdk(B,A), which implies that k-reach-distance is not symmetric.

Figure: rd5(B,A) = d5(A) and rd5(B,C) = d(B,C)

Basic Concepts

Local Outlier Factor

Some deﬁnitions :

lrdk(A) : local reachability density is inversely proportional to the average distancep, lrdk(A) = 1/ intuitively, if for most O ∈ Nk(A), more than k points are closer to O than A is, then the denominator is much larger than dk(A) and lrdk(A) is small; e.g., k = 3 in the ﬁgure

LOFk(A) : local outlier factor, (cid:80) O∈Nk(A)

LOFk(A) : local outlier factor, (cid:80) LOFk(A) =

LOFk(A) (cid:28) 1, the density of A is locally higher, dense point; LOFk(A) (cid:29) 1, the density of A is locally lower, probably outlier

Data Preprocessing

Basic Concepts

Further Study

Other methods for outlier detection :

K-Means • K Nearest Neighbors • Isolation Forest • One-class support vector machine • Robust covariance

Outlier processing :

Delete outliers (treat them as missing values) • Robust regression • Theil-Sen regression

Data Preprocessing

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Introduction to Big Data Analysis Classiﬁcation : Part 1

Zhen Zhang

Southern University of Science and Technology

Model Assessment

References

Introduction

k-Nearest Neighbor

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

References

Decision Trees

Naive Bayes

Outlines

Model Assessment

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Why We Need Classiﬁcation

Knowing the classes of the data, we could easily manage the data and react to the possible outcomes

Predict whether users would default in the future based on their basic information and historical transaction records

Predict whether a tumor is benign or malignant based on their physical and geometrical features

Predict the users’ interests in the new products based on their historical purchasing records and behaviorial preferences

Separate spams and advertisements from emails

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

What is Classiﬁcation

Supervised learning : predict label y from features x • Training stage : Given a data set D = {(x,y)}, including both (cid:83)Dtest, ﬁnd a classiﬁer features and labels, split D = Dtrain (function y = f (x)) that best relates ytrain with xtrain, then evaluate how close f (xtest) is to ytest

Predicting stage : apply the predictor to the unlabeled data xpred (only features) to ﬁnd the proper labels ypred = f (xpred)

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Classiﬁcation Methods

Diﬀerent assumptions on f lead to diﬀerent models • Basic classiﬁcation models

k-nearest neighbor (kNN) • Decision trees • Naive Bayes • Support vector machines (SVM) • Logistic regression • Linear discriminant analysis (LDA) • Artiﬁcial neural network (ANN) • ...

Ensemble learning : Random forest and Adaboost

References

Introduction

k-Nearest Neighbor

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

References

Decision Trees

Naive Bayes

Outlines

Model Assessment

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Introduction

k-nearest neighbor (kNN) is the simplest supervised learning method, especially useful when prior knowledge on the data is very limited

Do training and test simultaneously

When classifying a test sample x, scan the training set and ﬁnd the closest k samples Dk = {x1,...,xk} to the test sample; make vote based on the labels of the samples in Dk ; the majority vote is the label of the test sample

Low bias, high variance

Advantages : not sensitive to outliers, easy to implement and parallelize, good for large training set

Drawbacks : need to tune k, take large storage, computationally intensive

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Algorithm

Input : training set Dtrain = {(x1,y1),...,(xN,yN)}, a test sample x without label y, k and distance metric d(x,y)

Output : predicted label ypred for x

1. Compute d(x,xj) for each (xj,yj) ∈ Dtrain 2. Sort the distances in an ascending order, choose the ﬁrst k samples (x(1),y(1)),...,(x(k),y(k))

3. Make majority vote ypred = Mode(y(1),...,y(k))

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Distance Metrics

(cid:115) • Minkowski distance : dh(x1,x2) = h

d (cid:80) i=1

(x1i − x2i)h ; h = 2,

Euclidean distance; h = 1, Manhattan distance

Mahalanobis distance : (cid:113)

(x1 − x2)T ˆΣ−1(x1 − x2), where ˆΣ is the

d(x1,x2) = covariance matrix of sample set; introduce correlations, could be applied to the non-scaling data

Hamming distance : Hamming(x1,x2) = d −

d (cid:80) i=1

I(x1i = x2i);

used to compare two strings, e.g., Hamming((cid:48)toned(cid:48),(cid:48) roses(cid:48)) = 3, Hamming((cid:48)101110(cid:48),(cid:48) 101101(cid:48)) = 2

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Distance Metrics - Similarity and Divergence

Cosine similarity : cos(x1,x2) = xT 1 x2 |x1||x2| =

d (cid:80) i=1

x1ix2i (cid:115) d (cid:80) i=1

(cid:115) d (cid:80) i=1

x2 1i

x2 2i

; its

range is [−1,1]; the greater the cosine similarity, the more similar (closer) the two samples; insensitive to absolute value, popular in measuring user rankings; it is related to Pearson correlation coeﬃcient

Jaccard similarity for sets A and B : Jaccard(A,B) = |A(cid:84)B| |A(cid:83)B|, used in comparing texts

(cid:2)log P(x) Q(x) measures the distance between two probability distributions P

Kullback-Leibler (KL) divergence : dKL(P(cid:107)Q) = EP

and Q ; in discrete case, dKL(p(cid:107)q) =

m (cid:80) i=1

pi log pi qi

References

(cid:3)

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Tuning k

Diﬀerent values of k = 3 and k = 5 leads to diﬀerent classiﬁcation results

M-fold Cross-validation

(CV) to tune k : partition the dataset into M parts (M = 5 or 10), let κ : {1,...,N} → {1,...,M} be randomized partition index map, The CV estimate of prediction error is CV(ˆf ,k) =

1 N

N (cid:80) i=1

L(yi,ˆf −κ(i)(xi,k))

Model Assessment

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Bayes Classiﬁer (Oracle Classiﬁer)

Assume Y ∈ Y = {1,2,...,C}, the classiﬁer f : X → Y is a piecewise constant function

For 0-1 loss L(y,f ), the learning problem is to minimize

E(f ) = EP(X,Y)L(Y,f (X)) = 1 − P(Y = f (X))

(cid:90)

= 1 −

P(Y = f (X)|X = x)pX(x)dx

X

Bayes rule : f ∗(x) = argmaxc P(Y = c|X = x), “the most probable label under the conditional probability on x” • Bayes error rate : inff E(f ) = E(f ∗) = 1 − P(Y = f ∗(X)) • Bayes decision boundary : the boundary separating the K partition domains in X on each of which f ∗(x) ∈ Y is constant. For binary classiﬁcation, it is the level set on which P(Y = 1|X = x) = P(Y = 0|X = x) = 0.5.

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Decision Boundary

The decision boundary of 15NN is smoother than that of 1NN

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Analysis

Time complexity : O(mndK) where n is the number of training samples, m is the number of test samples, d is the dimension, and K is the number of nearest neighbors • KD tree for indexing : K-dimensional binary search tree • 1NN error rate is twice the Bayes error rate :

Bayes error = 1 − pc∗(x) where c∗ = argmaxc pc(x) • Assume the samples are i.i.d., for any test sample x and small δ, there is always a training sample z ∈ B(x,δ) (the label of x is the same as that of z), then 1NN error is C (cid:88)

Bayes error = 1 − pc∗(x) where c∗ = argmaxc pc(x) • Assume the samples are i.i.d., for any test sample x and small δ, there is always a training sample z ∈ B(x,δ) (the label of x is the same as that of z), then 1NN error is C (cid:88)

Bayes error = 1 − pc∗(x) where c∗ = argmaxc pc(x) • Assume the samples are i.i.d., for any test sample x and small δ, there is always a training sample z ∈ B(x,δ) (the label of x is the same as that of z), then 1NN error is C (cid:88)

p2 c(x)

c=1

c=1 (cid:54)1 − p2 c∗(x) (cid:54)2(1 − pc∗(x))

(Remark : In fact, (cid:15) (cid:54) 2(1 − pc∗(x)) − C

C−1(1 − pc∗(x))2)

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Case Study

Use kNN to diagnose breast cancer (cookdata)

Data scaling : 0-1 scaling or z-score scaling

from sklearn.neighbors

import KNeighborsClassiﬁer



KNeighborsClassiﬁer(n neighbors = 10, metric = ’minkowski’, p=2)

Model Assessment

References

Introduction

k-Nearest Neighbor

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

References

Decision Trees

Naive Bayes

Outlines

Model Assessment

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Decision Tree as Medical Diagnosis

Diagnose whether it is ﬂu or cold • Rules :

If headache = severe, then ﬂu

If headache = mild and sore = yes, then ﬂu • If headache = mild and sore = no, then cold • If headache=no, cold

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Decision Tree Algorithm

Tree structure : internal nodes indicate features, while leaf nodes represent classes

Start from root, choose a suitable feature xi and its split point ci at each internal node, split the node to two child nodes depending on whether xi (cid:54) ci, until the child nodes are pure

Equivalent to rectangular partition of the region

Model Assessment

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

How to choose features and split points

Impurity : choose the feature and split point so that after each slit the impurity should decrease the most

Impurity(M0)-Impurity(M12) > Impurity(M0)-Impurity(M34), choose A as split node; otherwise choose B

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Impurity Measures - GINI Index

Gini index of node t : Gini(t) = 1 − (cid:80)C p(c|t) is the proportion of class-c data in node t

Gini index of node t : Gini(t) = 1 − (cid:80)C p(c|t) is the proportion of class-c data in node t

Maximum at 1 − 1 • Minimum at 0, when p(c|t) = 1 for some c • Gini index of a split : Ginisplit = (cid:80)K k=1

nk n Gini(k) where nk is

the number of samples in the child node k, n = (cid:80)K k=1 nk • Choose the split so that Gini(t) − Ginisplit is maximized

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

References

Impurity Measures - Information Gain

Entropy at t : H(t) = −(cid:80)C • Maximum at log2 C, when p(c|t) = 1 C • Minimum at 0, when p(c|t) = 1 for some c • Information gain : InfoGainsplit = H(t) − (cid:80)K k=1

nk n H(k) where k=1 nk

nk is the number of samples in the child node k, n = (cid:80)K

Choose the split so that InfoGainsplit is maximized (ID3 algorithm)

Drawback : easy to generate too many child nodes and overﬁt

Introduce information gain ratio : SplitINFO = −(cid:80)K (C4.5 algorithm)

Introduce information gain ratio : nk n log2

n , InfoGainRatio = InfoGainsplit nk

SplitINFO

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Impurity Measures - Misclassiﬁcation Error

Misclassiﬁcation error at t : Error(t) = 1 − maxc p(c|t); use majority vote

Maximum at 1 − 1 • Minimum at 0, when p(c|t) = 1 for some c • For two-class classiﬁcation, Gini(p) = 2p(1 − p), 2p log2 p − (1 − p)log2(1 − p) (up to a factor 1

2),

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Comparing Three Impurity Measures

Information gain and Gini index are more sensitive to changes in the node probabilities than the misclassiﬁcation error • Consider a two-class problem with 400 observations in each

class, (400, 400); two possible splits, A : (300, 100) + (100, 300), and B : (200, 400) + (200, 0); B should be preferred 4) = 3 8,

2Gini(A2) = 2 × 1 4Gini(A2) = 3 4 − 1 4 log2 2 3 − 2 3)) = 0.69 3 log2 2(1 − max(3 4)) = 1 4, 3, 2 • Gini index and information gain should be used when growing

Gini(A) = 1 Gini(B) = 3 • H(A) = 2 × 1 4(−1 H(B) = 3

2Gini(A1) + 1 4Gini(A1) + 1 2(−3 4 log2 1 3 log2 • Error(A) = 2 × 1 Error(B) = 3

4 × 1 3)) = 1

2(2 × 3 3 × 2

4(2 × 1 1 4)) = 0.81,

3

4, 1 3)) = 1

4(1 − max(1

4

the tree

In pruning, all three can be used (typically misclassiﬁcation error)

3

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Algorithms

Iterative Dichotomiser 3 (ID3) : by Ross Quinlan (1986), based on Occam’s Razor rule (be simple); information gain, choose feature values by enumeration

C4.5 and C5.0 : by R. Quinlan (1993), use information gain ratio instead, choose split thresholds for continuous features • Classiﬁcation and Regression Tree (CART) : by Leo Breiman etc. (1984); for classiﬁcation, use Gini index; for regression, use mean square error; binary split

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

ID3 Algorithm

Input : training set D = {(x1,y1),...,(xn,yn)},

Y = {y1 ...,yn}, set of features F = {column variables of X = (x1 ...xn)T}

Output : decision tree T

1. Create a root node

2. Check Y : if all are positive, then return a single node tree T with label “+”; if all are negative, then return a single node tree T with label “-”

3. Check F : if empty, then return a single node tree T with label as majority vote of Y

4. For each feature in F, compute information gain, choose the feature A ∈ F which maximizes information gain as root

5. For A = i, let D(i) = {(xj,yj) ∈ D|xjA = i} :

5.1 If D(i) = ∅, then create a leaf node and make majority vote of D as the

label

5.2 Else, let D = D(i), go back to step 1 iteratively

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Tree Pruning

Too complex tree structure easily leads to overﬁtting • Prepruning : set threshold δ for impurity decrease in splitting a node; if ∆Impuritysplit > δ, do slitting, otherwise stop

Postpruning : based on cost function |T| (cid:88)

Postpruning : based on cost function |T| (cid:88)

ntImpurity(t)

+α

t=1 (cid:124)

(cid:123)(cid:122) data ﬁdelity

(cid:125)

|T| (cid:124)(cid:123)(cid:122)(cid:125) model complexity

Input : a complete tree T, α • Output : postpruning tree Tα

1. Compute Impurity(t) for ∀t Iteratively merge child nodes 2. bottom-up : TA and TB are the trees before and after merging, do merging if Costα(TA) (cid:62) Costα(TB)

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Pros and Cons

Advantages

Easy to interpret and visualize : widely used in ﬁnance, medical health, biology, etc.

Easy to deal with missing values (treat as new data type) • Could be extended to regression : decision tree is a rectangular partition of the domain, the predictor can be written as

f (x) =

M (cid:80) m=1

cmI(x ∈ Rm); for regression problems

cm = ¯ym = 1 nm

n (cid:80) i=1

yiI(xi ∈ Rm) where nm =

n (cid:80) i=1

I(xi ∈ Rm)

Drawbacks :

Easy to be trapped at local minimum because of greedy algorithm

Simple decision boundary : parallel lines to the axes

References

Introduction

k-Nearest Neighbor

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

References

Decision Trees

Naive Bayes

Outlines

Model Assessment

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Introduction

Based on Bayes Theorem and conditional independency assumption on features

Widely used in text analysis, spam ﬁltering, recommender systems, and medical diagnosis

Bayes Theorem : let X and Y be a pair of random variables having joint probability P(X = x,Y = y); by deﬁnition, the condition probability of Y given X is P(Y|X) = P(X,Y) P(X) by symmetry, P(X|Y) = P(X,Y) P(Y) ; then

Bayes Theorem : let X and Y be a pair of random variables having joint probability P(X = x,Y = y); by deﬁnition, the condition probability of Y given X is P(Y|X) = P(X,Y) P(X) by symmetry, P(X|Y) = P(X,Y) P(Y) P(X|Y)P(Y) P(X)

P(Y) is prior prob. distribution, P(X|Y) is likelihood function, P(X) is evidence, P(Y|X) is posterior prob. distribution

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Naive Bayes

The core problem of machine learning is to estimate P(Y|X) (or its moments E[Y|X] = argmin

The core problem of machine learning is to estimate P(Y|X) E[(cid:107)Y − f (X)(cid:107)2])

Let X = {X1,...,Xd}, for ﬁxed sample X = x, P(X = x) is independent of Y, by Bayes Theorem

P(Y|X = x) ∝ P(X = x|Y)P(Y)

Assume conditional independency of X1,...,Xd given Y = c :

P(X = x|Y = c) =

d (cid:89)

P(Xi = xi|Y = c)

i=1

Naive Bayes model :

ˆy = argmax

c

P(Y = c)

d (cid:89)

i=1

P(Xi = xi|Y = c)

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Maximum Likelihood Estimate (MLE)

Estimate P(Y = c) and P(Xi = xi|Y = c) from the dataset D = {(x1,y1),...,(xn,yn)}

n (cid:80) i=1

I(yi=c)

MLE for P(Y = c) : P(Y = c) = • When Xi is discrete variable with range {v1,...,vK}, MLE for I(xi=vk,yi=c) I(yi=c)

When Xi is continuous variable

1. Do discretization, and go back to the above formula 2. Assume Xi follows some distribution (e.g., N(µ,σ2)) :

P(Xi = x|Y = c) =

√

1 2πσ

e− (x−µ)2

2σ2

Then use MLE to estimate µ and σ2

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Pros and Cons

Where it is good

Spam ﬁlter : compute the posterior prob. distribution of frequently used words (convert to vector by word2vec)

Stable : for outliers and miss values • Robust : for uncorrelated features; P(Xi|Y) is independent of Y and thus has no eﬀect on posterior probability

May outperform far more sophisticated alternatives even if conditional independency assumption is not satisﬁed

Disadvantage

However, when conditional independency assumption is violated, performance of Naive Bayes can be poorer

Depends heavily on how well the parameter estimates are

References

Introduction

k-Nearest Neighbor

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

References

Decision Trees

Naive Bayes

Outlines

Model Assessment

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

References

Confusion Matrix

For two-class classiﬁcation : • True Positive (TP) : both true label and predicted label are positive

Accuracy =

TP+TN TN+FN+FP+TP ;

not a good index when samples are imbalanced

True Negative (TN) : both true label and predicted label are negative

False Positive (FP) : true label is negative, but predicted label is positive

Precision = TP • Recall = TP in medical diagnosis (sensitivity)

Precision = TP • Recall = TP TP+FP

False Negative (FN) : true

label is positive, but predicted

label is negative

F score :

Fβ = (1+β2)Precision×Recall β2×Precision+Recall β = 1, F1 score • Speciﬁty = TN

TN+FP ; recall

;

for negative samples

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Receiver Operating Characteristic (ROC) and AUC

Aim to solve class distribution imbalance problem • Set diﬀerent threshold t for continuous predicted values (probability), e.g., if P(Y = 1|X = xi) > t, then ˆyi = 1

Compute TPR (= TP

TP+FN, or recall) vs. FPR(= FP

FP+TN) for

diﬀerent t and plot ROC curve

The higher the ROC, the better the performance • AUC : area under ROC, the larger the better, the more robust of the method for the change of t ; very good if > 0.75

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

References

Cohen’s Kappa Coeﬃcient

κ = po−pe 1−pe raters

= 1 − 1−p0 1−pe

measures the agreement between two

po is the accuracy (or the relative observed agreement) • pe is the hypothetical probability of chance agreement, ntrue N , where npred c pe = (cid:80)C predicted in class c, ntrue class c, N is the total number of samples 50 + 25 50 = 0.7, pe = 25

npred c N

is the number of samples is the true number of samples in

c

c=1

c

Eg : po = 20+15

50 × 20

50 × 30

50 = 0.5, κ = 0.4

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

The Values of Kappa Coeﬃcient

κ ∈ [−1,1]

κ = 1 : perfect agreement between two raters

κ = −1 : completely disagreement

κ = 0 : no agreement among the raters other than what would be expected by chance

κ < 0 : worse than random

κ > 0 : the result is meaningful, agree more as κ gets larger • κ (cid:62) 0.75 : good performance • κ < 0.4 : bad performance

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

Multiple Class Problem

ROC and AUC are not well-deﬁned

Confusion matrix : C × C, each entry means the number of samples in the intersection of the predicted class i and the true class j

Positive sample is the sample belonging to the class i, negative sample is the sample not belonging to the class i, so every sample could be positive or negative

Convert to multiple 0-1 classiﬁcation problems

Precision and recall are the averages of that in the each 0-1 classiﬁcation problem

F1 score is still deﬁned as the harmonic average of precision and recall

References

Introduction

k-Nearest Neighbor

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

References

Decision Trees

Naive Bayes

Outlines

Model Assessment

References

Introduction

k-Nearest Neighbor

Decision Trees

Naive Bayes

Model Assessment

References

Œ(cid:226)'(cid:219)(cid:19)(cid:216)§˘(cid:228)(cid:140)Œ(cid:226)˘(cid:19) • –(cid:147)u§¯(cid:236)˘S§2016 • T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning : Data mining, Inference, and Prediction, 2nd Edition, 2009

References

Introduction

Linear Regression

Regularizations

Model Assessment

Introduction to Big Data Analysis Regression

Zhen Zhang

Southern University of Science and Technology

References

Introduction

Linear Regression

Introduction

Linear Regression

Regularizations

Model Assessment

References

Regularizations

Outlines

Model Assessment

References

Introduction

Linear Regression

Regularizations

Model Assessment

Regression

Proposed by Francis Galton (left) and Karl Pearson (right), in the publication “Regression towards mediocrity in hereditary ”

The characteristics (e.g., height) in the oﬀspring regress towards a mediocre point (mean) of that of their parents • Generalization : predict the dependent variables y from the independent variables x : y = f (x) or y = E[y|x]

References

Introduction

Linear Regression

Regularizations

Model Assessment

Applications

Predict medical expenses from the individual proﬁles of the patients

Predict the scores on Douban from the quality of the movies

Predict the tips from the total expenses

References

Introduction

Linear Regression

Introduction

Linear Regression

Regularizations

Model Assessment

References

Regularizations

Outlines

Model Assessment

References

Introduction

Linear Regression

Regularizations

Model Assessment

Univariate Linear Model

Linear model : y = w0 + w1x + (cid:15), where w0 and w1 are regression coeﬃcients, (cid:15) is the error or noise

Assume (cid:15) ∼ N(0,σ2), where σ2 is a ﬁxed but unknown variance; then y|x ∼ N(w0 + w1x,σ2)

Assume the samples {xi,yi}n

i=1 are generated from this conditional distribution, i.e., yi|xi ∼ N(w0 + w1xi,σ2)

Intuitively, ﬁnd the best straight line (w0 and w1) such that the sample points ﬁt it well, i.e., the residuals are minimized, n (cid:88)

Intuitively, ﬁnd the best straight line (w0 and w1) such that the sample points ﬁt it well, i.e., the residuals are minimized, n (cid:88)

(yi − w0 − w1xi)2

i=1

References

Introduction

Linear Regression

Regularizations

Model Assessment

Multivariate Linear Model

Linear model : y = f (x) + (cid:15) = w0 + w1x1 + ··· + wpxp + (cid:15), where w0,w1,...,wp are regression coeﬃcients, x = (x1,...,xp)T is the input vector whose components are independent variables or attribute values, (cid:15) ∼ N(0,σ2) is the noise

For the size n samples {(xi,yi)}n

i=1, let y = (y1,...,yn)T be the response

or dependent variables, w = (w0,w1,...,wp)T, X = [1n,(x1,...,xn)T] ∈ Rn×(p+1), and ε = ((cid:15)1,...,(cid:15)n)T ∼ N(0,σ2In).

y = Xw + ε 

1 x11 1 x21 ... ... 1 xn1

   

X =

··· ··· ... ···

x1p x2p ... xnp



   

References

Introduction

Linear Regression

Regularizations

Model Assessment

Least Square (LS)

Minimize the total residual sum-of-squares : RSS(w) = (cid:80)n

i=1(yi − w0 − w1x1 − ··· − wpxp)2 = (cid:107)y − Xw(cid:107)2 2

When XTX is invertible, the minimizer ˆw satisﬁes

∇wRSS(ˆw) = 0 ⇒ ˆw = (XTX)−1XTy

The prediction ˆy = X(XTX)−1XTy = Py is a projection of y onto the linear space spanned by the column vectors of X; P = X(XTX)−1XT is the projection matrix satisfying P2 = P

References

Introduction

Linear Regression

Regularizations

Model Assessment

Maximal Likelihood Estimate (MLE)

A probabilistic viewpoint :

y|x ∼ N(w0 + w1x1 + ··· + wpxp,σ2)

Likelihood function :

L(w;X,y) = P(y|X,w) = Πn

i=1P(yi|xi,w) with

P(yi|xi,w) = 1√

2πσ

e−

(yi−w0−w1xi1−···−wpxip)2 2σ2

Maximal likelihood estimate : given the samples from some unknown parametric distribution, ﬁnd the parameters such that the samples the most probably seem to be drawn from that distribution, i.e., ˆw = argmaxw L(w;X,y) • Equivalent to maximize the log-likelihood function i=1(yi − w0 − w1xi1 − ··· − wpxip)2

The same minimizer as LS : ˆw = (XTX)−1XTy

References

Introduction

Linear Regression

Regularizations

Model Assessment

Projection by Orthogonalization

Another useful formulation : let ¯y = 1 n (cid:80)n

Another useful formulation : let ¯y = 1 n ¯x = 1 n centralized data {˜xi, ˜yi}n RSS(˜w) = (cid:80)n with ˆw0 = ¯y − ˆ˜wT¯x

i=1 xi, then OLS can be formulated by using the

i=1 = {xi − ¯x,yi − ¯y}n i=1(˜yi − w1˜xi1 − ··· − wp˜xip)2 = (cid:107)˜y − ˜X˜w(cid:107)2 2,

i=1,

Ordinary least square (OLS) prediction ˆy = Py is the projection of y on the linear space spanned by the columns of X, i.e., X = Span{x·,0,x·,1,...,x·,p}, recall that x·,0 = 1n • If {x·,0,x·,1,...,x·,p} forms a set of orthonormal basis, then

ˆy = (cid:80)p

i=0 < y,x·,i > x·,i

If not, we can ﬁrst do orthogonalization by Gram-Schmidt procedure for the set {x·,0,x·,1,...,x·,p}

Similar orthogonalization procedures can be done by QR decomposition or SVD of the matrix XTX (classic topics in numerical linear algebra)

References

Introduction

Linear Regression

Regularizations

Model Assessment

Regression by Successive Orthogonalization

The expansion of y on the standard orthonormal basis after Gram-Schmidt procedure can be summarised in the following algorithm :

1. Initialize z0 = x0 = 1n 2. For j = 1,...,p :

Regress xj on {z0,...,zj−1} to produce coeﬃcients ˆγlj =< zl,xj > / < zl,zl > with l = 0,...,j − 1 and residual vectors zj = xj − (cid:80)j−1

k=0 ˆγkjzk 3. Regress y on the residual zp to give the estimate ˆwp • If xp is highly correlated with some of the other xk’s, the

residual vector zp will be close to zero; in such situation, the coeﬃcient ˆwp with small Z-score ˆwp ˆσp is an estimate of Var(ˆwp) = σ2 where ˆσ2 (cid:107)zp(cid:107)2 2

could be thrown out,

p = ˆσ2 (cid:107)zp(cid:107)2 2

References

Introduction

Linear Regression

Regularizations

Model Assessment

Shortcomings of Fitting Nonlinear Data

Evaluating the model by Coeﬃcient of Determination R2 : R2 := 1 − SSres for linear regression), where SStot SStot = (cid:80)n i=1(yi − ¯y)2 is the total sum of squares, SSreg = (cid:80)n i=1(ˆyi − ¯y)2 is the regression sum of squares, and SSres = (cid:80)n i=1(yi − ˆyi)2 is the residual sum of squares.

The larger the R2, the better the model

References

Introduction

Linear Regression

Regularizations

Model Assessment

Multicolinearity

If the columns of X are almost linearly dependent, i.e., multicolinearity, then det(XTX) ≈ 0, the diagonal entries in (XTX)−1 is quite large. This implies the variances of ˆw get large, and the estimate is not accurate

Eg : 10 samples are drawn from the true model

y = 10 + 2x1 + 3x2 + (cid:15); the LS estimator is ˆw0 = 11.292, ˆw1 = 11.307, ˆw2 = −6.591, far from the true coeﬃcients; correlation coeﬃcient is r12 = 0.986

Remedies : ridge regression, principal component regression, partial least squares regression, etc.

References

Introduction

Linear Regression

Regularizations

Model Assessment

Overﬁtting

Easily to be overﬁtted when introducing more variables, e.g., regress housing price with housing size

The high degree model also ﬁts the noises in the training data, so generalizes poorly to new data

Remedy : regularization

References

Introduction

Linear Regression

Regularizations

Model Assessment

Bias-Variance Decomposition

Bias-variance decomposition of generalization error in L2 loss :

EtrainRexp(ˆf (x)) = EtrainEP[(y−ˆf (x))2|x] = Var(ˆf (x)) (cid:125) (cid:123)(cid:122) (cid:124) variance

+Bias2(ˆf (x)) (cid:123)(cid:122) (cid:125) bias

(cid:124)

where P = P(y|x) is the conditional probability of y given x

Bias : Bias(ˆf (x)) = Etrainˆf (x) − f (x) is the average accuracy of prediction for the model (deviation from the truth)

Variance : Var(ˆf (x)) = Etrain(ˆf (x) − Etrainˆf (x))2 is the variability of the model prediction due to diﬀerent data set (stability)

References

+ σ2 (cid:124)(cid:123)(cid:122)(cid:125) noise

Introduction

Linear Regression

Regularizations

Model Assessment

Bias-Variance Decomposition (Derivation)

Model y = f (x) + (cid:15), with E((cid:15)) = 0 and Var((cid:15)) = σ2 (system error)

EtrainRexp(ˆf (x)) =EP[(y − f (x))2|x] + Etrain[(f (x) − ˆf (x))2]

+ 2EtrainEP[(y − f (x))(f (x) − ˆf (x))|x] (cid:125)

(cid:124)

(cid:123)(cid:122) vanishes since EP(y−f (x)|x)=0

=σ2 + Etrain[(f (x) − Etrainˆf (x))2] + Etrain[(Etrainˆf (x) − ˆf (x))2]

+ 2Etrain[(f (x) − Etrainˆf (x))(Etrainˆf (x) − ˆf (x))] (cid:125)

(cid:124)

(cid:123)(cid:122)

vanishes since Etrain[Etrain

ˆf (x)−ˆf (x)])=0

=σ2 + Bias2(ˆf (x)) + Var(ˆf (x))

The more complicated the model, the lower the bias, but the higher the

variance.

References

Introduction

Linear Regression

Regularizations

Model Assessment

Bias-Variance Decomposition : kNN Regression

kNN can be used to do regression if the mode (majority vote) x(i)∈Nk(x) y(i)

Generalization error of kNN regression is

EtrainRexp(ˆf (x)) =σ2 + (f (x) −

1 k

(cid:88)

x(i)∈Nk(x)

f (x(i)))2

+ Etrain

(cid:104)1 k

(cid:88)

(y(i) − f (x(i)))

(cid:124)

x(i)∈Nk(x) (cid:123)(cid:122) 1 k σ2

where we have used the fact that Etrainyi = f (xi) and Var(yi) = σ2.

For small k, overﬁtting, bias (cid:38), variance (cid:37) • For large k, underﬁtting, bias (cid:37), variance (cid:38)

(cid:105)2

(cid:125)

References

Introduction

Linear Regression

Introduction

Linear Regression

Regularizations

Model Assessment

References

Regularizations

Outlines

Model Assessment

References

Introduction

Linear Regression

Regularizations

Model Assessment

Regularization by Subset Selection

In high dimensions, the more the input attributes, the larger the variance

Shrinking some coeﬃcients or setting them to zero can reduce the overﬁtting

Using less input variables also help interpretation with the most important variables

Subset selection(cid:181)retaining only a subset of the variables, while eliminating the rest variables from the model

Best-subset selection : ﬁnd for each k ∈ {0,1,...,p} the subset Sk ⊂ {1,...,p} of size k that gives the smallest RSS(w) = (cid:80)n j∈Sk

References

Introduction

Linear Regression

Regularizations

Model Assessment

Best-Subset Selection

The best subset of size k + 1 may not include the the variables in the best subset of size k

The RSS of the best subset of size k is not necessarily decreasing with k

Choose k based on bias-variance tradeoﬀ, usually by AIC and BIC, or practically by cross-validation

References

Introduction

Linear Regression

Regularizations

Model Assessment

Forward (Backward) Stepwise Selection

Forward-stepwise selection : start with the intercept ¯y, then sequentially add into the model the variables that improve the ﬁt most (reduce RSS most)

QR factorization helps search the candidate variables to add

Greedy algorithm : the solution could be sub-optimal

Computationally more eﬃcient than best-subset selection; statistically the constrained search enjoys lower variance than best-subset selection

Backward-stepwise selection : start with the full model, then sequentially delete from the model the variables that has the least impact on the ﬁt most (the candidate for dropping is the variable with the smallest Z-score); can only be used when n > p in order to ﬁt the full model by OLS

References

Introduction

Linear Regression

Regularizations

Model Assessment

Forward-Stagewise (FS) Selection

Starts with the intercept and centered variables with 0 coeﬃcients • At each step, identify the variables (among all variables) most correlated with the current residual, then regress the residual on this chosen variable and increment the current coeﬃcient with the new regression coeﬃcient • Ends when no variables are correlated with the residual (arrive at the OLS ﬁt when n > p)

Slower than forward-stepwise : the other variables and their coeﬃcients are not changed at each step except the chosen variable

References

Introduction

Linear Regression

Regularizations

Model Assessment

Regularization by Penalties

Add a penalty term, in general lq-norm

n (cid:88)

(yi − w0 − w1x1 − ··· − wpxp)2 + λ(cid:107)w(cid:107)q q

i=1

=(cid:107)y − Xw(cid:107)2

2 + λ(cid:107)w(cid:107)q q

q = 2 : ridge regression • q = 1 : LASSO regression

References

Introduction

Linear Regression

Regularizations

Model Assessment

Ridge Regression

The optimization problem turns to be

ˆw =argmin

w

=argmin

w

n (cid:88)

(yi − w0 − w1x1 − ··· − wpxp)2 + λ(cid:107)w(cid:107)2 2

i=1 (cid:107)y − Xw(cid:107)2

2 + λ(cid:107)w(cid:107)2 2

λ (cid:62) 0 is a ﬁxed parameter which has to be tuned by cross-validation

Equivalent to the constraint minimization problem :

ˆw = argmin

w

(cid:107)y − Xw(cid:107)2 2,

subject to

(cid:107)w(cid:107)2 (cid:54) µ,

where µ (cid:62) 0 is a prescribed threshold (tuning parameter)

The large λ corresponds to the small µ.

References

Introduction

Linear Regression

Regularizations

Model Assessment

Solving Ridge Regression

Easy to show that ˆwridge = (XTX + λIp+1)−1XTy • The estimator is also a projection of y : ˆyridge = X(XTX + λIp+1)−1XTy

X can be diagonalized by SVD : X = PDQ with

D = diag(ν1,...,νp+1), and P ∈ Rn×(p+1), Q ∈ R(p+1)×(p+1) being orthogonal matrices (PTP = Ip+1)

ν2 p+1 ν2 p+1+λ • In the spectral space, the ridge regression estimator is a

ˆyridge = Pdiag( ν2 1 ν2 1+λ

)PTy, while ˆyOLS = PPTy

,...,

shrinkage of the OLS estimator (λ = 0)

References

Introduction

Linear Regression

Regularizations

Model Assessment

Bayesian Viewpoint of Ridge Regression

Given X and w, the conditional distribution of y is 2σ2 (y − Xw)T(y − Xw)(cid:1)

In addition, assume w has a prior distribution 2(w − µ0)TΛ−1 P(w) = N(µ0,Λ0) ∝ exp(cid:0) − 1

0 (w − µ0)(cid:1)

By Bayes theorem, the posterior distribution of w given the data X and y is

P(w|X,y) ∝ P(y|X,w)P(w)

1 2σ2 (wTXTXw − 2yTXw) (wTΛ−1

∝ exp(cid:0) −

1 2 ∝ exp(cid:0) −

0 w)(cid:1)

0 w − 2µT

0 Λ−1

−

1 2 σ2 XTX + Λ−1

m (w − µm)(cid:1)

(w − µm)TΛ−1

0 )−1 and µm = Λm( 1

where Λm = ( 1

σ2 XTy + Λ−1

0 µ0)

If µ0 = 0 and Λ0 = σ2

λ Ip+1, then ˆw = µm = (XTX + λIp+1)−1XTy

maximizes the posterior probability P(w|X,y)

References

Introduction

Linear Regression

Regularizations

Ridge Trace

The functional plot of ˆwridge(λ) with λ is called ridge trace

The large variations in ridge trace indicate the multicolinearity in variables

When λ ∈ (0,0.5), the ridge traces have large variations, it suggests to choose λ = 1

Model Assessment

References

Introduction

Linear Regression

Regularizations

Model Assessment

Reading from Ridge Trace

Before plot ridge trace, do scaling for the variables

The coeﬃcients with stable trace and small absolute values should have little inﬂuence on y, as in (a) • The coeﬃcients with large stable absolute values should have great impact on y, as in (b) and (c)

The ridge traces of the

coeﬃcients of two variables are not stable, but the sum of the coeﬃcients is stable. This implies the multicolinearity as in (d) • The stable ridge traces of all

variables suggest good performance using OLS as in (f)

References

Introduction

Linear Regression

Regularizations

Model Assessment

LASSO Regression

Proposed by R. Tibshirani, short for “Least Absolute Shrinkage and Selection Operator”

Can be used to estimate the coeﬃcients and select the important variables simultaneously

Reduce the model complexity, avoid overﬁtting, and improve the generalization ability

Also improve the model interpretability

References

Introduction

Linear Regression

Regularizations

LASSO Formulation

The optimization problem

ˆw = arg min

E(w)

w E(w) = (cid:107)y − Xw(cid:107)2

2 + λ(cid:107)w(cid:107)1

Equivalent to the constraint minimization problem :

ˆw = arg min

w subject to

(cid:107)y − Xw(cid:107)2 2,

(cid:107)w(cid:107)1 (cid:54) µ,

The large λ corresponds to the small µ.

The optimal solution is sparse with ˆw2 = 0

Model Assessment

References

Introduction

Linear Regression

Regularizations

Model Assessment

Solving LASSO Regression

Assume XTX = Ip+1, then ˆwOLS = XTy • ∂wE(w) = w − XTy + λ(∂|w0| × ··· × ∂|wp|) • 0 ∈ ∂wE(ˆwlasso) implies 0 ∈ ˆwlasso • If ˆwlasso > 0, ∂|ˆwlasso i ˆwOLS i • If ˆwlasso i ˆwOLS i • If ˆwlasso i i • In summary, ˆwlasso )

Assume XTX = Ip+1, then ˆwOLS = XTy • ∂wE(w) = w − XTy + λ(∂|w0| × ··· × ∂|wp|) • 0 ∈ ∂wE(ˆwlasso) implies 0 ∈ ˆwlasso • If ˆwlasso > 0, ∂|ˆwlasso i ˆwOLS i • If ˆwlasso i ˆwOLS i • If ˆwlasso i i • In summary, ˆwlasso |

Assume XTX = Ip+1, then ˆwOLS = XTy • ∂wE(w) = w − XTy + λ(∂|w0| × ··· × ∂|wp|) • 0 ∈ ∂wE(ˆwlasso) implies 0 ∈ ˆwlasso • If ˆwlasso > 0, ∂|ˆwlasso i ˆwOLS i • If ˆwlasso i ˆwOLS i • If ˆwlasso i i • In summary, ˆwlasso = ˆwOLS i

− ˆwOLS i i | = {1}, and ˆwlasso

References

Introduction

ˆwlasso i ˆwOLS i

Linear Regression

Regularizations

Model Assessment

Shrinkage and Selection Property of LASSO

= (|ˆwOLS i i , where (a)+ = max(a,0) is the positive part of a

| − λ)+sign(ˆwOLS

) is called soft thresholding of

References

Introduction

Linear Regression

Regularizations

Model Assessment

Maximum A Posteriori (MAP) Estimation

Given θ, the conditional distribution of y is P(y|θ) • In addition, assume the parameter θ has a prior distribution P(θ) • The posterior distribution of θ given the data y is P(θ|y) ∝ P(y|θ)P(θ) • MAP choose the point of maximal posterior probability : (cid:0)log P(y|θ) + log P(θ)(cid:1)

Given θ, the conditional distribution of y is P(y|θ) • In addition, assume the parameter θ has a prior distribution P(θ) • The posterior distribution of θ given the data y is P(θ|y) ∝ P(y|θ)P(θ) • MAP choose the point of maximal posterior probability : ˆθMAP = arg max

Given θ, the conditional distribution of y is P(y|θ) • In addition, assume the parameter θ has a prior distribution P(θ) • The posterior distribution of θ given the data y is P(θ|y) ∝ P(y|θ)P(θ) • MAP choose the point of maximal posterior probability : P(θ|y) = arg max

If θ = w, and we choose the log-prior proportional to λ(cid:107)w(cid:107)2 normal prior N(0, σ2

λ I)), we recover the ridge regression

If the log-prior is proportional to λ(cid:107)w(cid:107)1, i.e., the prior is the tensor

product of Laplace (or double exponential) distribution Laplace(0, 2σ2 λ ) • Diﬀerent log-prior lead to diﬀerent penalties (regularization), but this is not the case in general : some penalties may not be the logarithms of probability distributions, some other penalties depend on the data (prior is independent of the data)

References

Introduction

Linear Regression

Regularizations

LASSO Path

When λ varies, the values of the coeﬃcients form paths (regularization paths)

The paths are piecewise linear with the same change points, may cross the x-axis many times

In practice, choose λ by cross-validation

Model Assessment

References

Introduction

Linear Regression

Regularizations

Model Assessment

Solving LASSO by LARS (Hastie and Efron)

1. Start with all coeﬃcients wi equal to zero

2. Find the predictor xi most correlated with y

3.

Increase the coeﬃcient wi in the direction of the sign of its correlation with y. Take residuals r = y − ˆy along the way. Stop when some other predictor xk has as much correlation with r as xi has

4.

Increase (wi,wk) in their joint least squares direction, until some other predictor xm has as much correlation with the residual r

5. Continue until all predictors are in the model

References

Introduction

Linear Regression

Regularizations

Model Assessment

Other Solvers

“glmnet” by Friedman, Hastie and Tibshirani, implemented by coordinate descent, can be used in linear regression, logistic regression, etc., with LASSO (l1), ridge (l2) and elastic net (l1 + l2) regularization terms

Why LASSO seeks the sparse solution in comparison with ridge?

References

Introduction

Linear Regression

Regularizations

Model Assessment

References

Related Regularization Models

Elastic net : ˆw = argminw (cid:107)y − Xw(cid:107)2 • Group LASSO : ˆw = argminw (cid:107)y − Xw(cid:107)2 2 + λ1(cid:107)w(cid:107)2 2 + (cid:80)G

Elastic net : ˆw = argminw (cid:107)y − Xw(cid:107)2 • Group LASSO : ˆw = argminw (cid:107)y − Xw(cid:107)2 w = (w1,...,wG) is the group partition of w

Dantzig Selector : minw (cid:107)w(cid:107)1, subject to (cid:107)XT(y − Xw)(cid:107)∞ (cid:54) µ • Smoothly clipped absolute deviation (SCAD) penalty by Fan and Li i=0 |wi| by (cid:80)p

Dantzig Selector : minw (cid:107)w(cid:107)1, subject to (cid:107)XT(y − Xw)(cid:107)∞ (cid:54) µ • Smoothly clipped absolute deviation (SCAD) penalty by Fan and Li i=0 Ja(wi,λ), where Ja(x,λ) (a−1)λ I(|x| > λ)

Dantzig Selector : minw (cid:107)w(cid:107)1, subject to (cid:107)XT(y − Xw)(cid:107)∞ (cid:54) µ • Smoothly clipped absolute deviation (SCAD) penalty by Fan and Li (2005) : replace the penalty λ(cid:80)p satisﬁes (for a (cid:62) 2) : dJa

(cid:16)

(cid:17)

Adaptive LASSO : weighted penalty (cid:80)p

i=0 µi|wi| where µi =

1 |ˆwOLS i

|ν with

ν > 0, as an approximation to |wi|1−ν, non-convex penalty

Introduction

Linear Regression

Introduction

Linear Regression

Regularizations

Model Assessment

References

Regularizations

Outlines

Model Assessment

References

Introduction

Linear Regression

Regularizations

Model Assessment

Errors and R2

(cid:80)n

Mean absolute error (MAE) : MAE = 1 n (cid:80)n • Mean square error (MSE) : MSE = 1 n • Root mean square error (RMSE) : (cid:80)n i=1(yi − ˆyi)2 • Coeﬃcient of Determination R2 : R2 := 1 − SSres SStot

i=1 |yi − ˆyi| i=1(yi − ˆyi)2

, where SStot = (cid:80)n i=1(yi − ¯y)2 is the total sum of squares, and SSres = (cid:80)n i=1(yi − ˆyi)2 is the residual sum of squares; R2 ∈ [0,1] (might be negative); the larger the R2, the smaller the ratio of SSres to SStot, thus the better the model

References

Introduction

Linear Regression

Regularizations

Model Assessment

Adjusted Coeﬃcient of Determination

Adjusted coeﬃcient of determination : R2 n−p−1 • n is the number of samples, p is the dimensionality (or the number of attributes)

The larger the R2 • When adding important variables into the model, R2 larger and SSres is reduced

When adding unimportant variables into the model, R2 gets smaller and SSres may increase

adj = ˆσ2 S2, where (cid:80)n i=1(yi − ˆyi)2 and S2 = 1 n−1 n−p−1 and (n − 1)S2

In fact, one can show that 1 − R2 ˆσ2 = 1 (n − p − 1) ˆσ2

In fact, one can show that 1 − R2 (cid:80)n

i=1(yi − ¯y)2 with n−1 if w = 0.

σ2 ∼ χ2

σ2 ∼ χ2

References

Introduction

Linear Regression

Introduction

Linear Regression

Regularizations

Model Assessment

References

Regularizations

Outlines

Model Assessment

References

Introduction

Linear Regression

Regularizations

Model Assessment

References

Œ(cid:226)'(cid:219)(cid:19)(cid:216)§˘(cid:228)(cid:140)Œ(cid:226)˘(cid:19) • –(cid:147)u§¯(cid:236)˘S§2016 • T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning : Data mining, Inference, and Prediction, 2nd Edition, 2009

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Introduction to Big Data Analysis Classiﬁcation : Part 2

Zhen Zhang

Southern University of Science and Technology

References

Logistic Regression

Linear Discriminant Analysis

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

References

Neural Network

Outlines

Support Vector Machine

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Logistic Regression

Not regression, but a classiﬁcation method

Connection with linear regression : y = w0 + w1x + (cid:15), y is binary (0 or 1); then E(y|x) = P(y = 1|x) = w0 + w1x ; but w0 + w1x may not be a probability

Find a function to map it back to [0,1] : Sigmoid function g(z) = 1 1+e−z with z = w0 + w1x1 + ... + wdxd

Equivalently, log P(y=1|x) w0 + w1x1 + ... + wdxd, logit transform logit(z) = log z 1−z

1−P(y=1|x) =

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

MLE for Logistic Regression

The prob. distribution for two-class logistic regression model is

Pr(y = 1|X = x) =

Pr(y = 0|X = x) =

exp(wTx) 1 + exp(wTx) 1 1 + exp(wTx)

,

.

Let P(y = k|X = x) = pk(x;w), k = 0 or 1. The likelihood n (cid:81) i=1 • MLE estimate of w : ˆw = argmax

Let P(y = k|X = x) = pk(x;w), k = 0 or 1. The likelihood n (cid:81) i=1 • MLE estimate of w : ˆw = argmax

pyi(xi;w)

L(w)

w

Solve ∇w logL(w) = 0 by Newton-Raphson method

References

Logistic Regression

Linear Discriminant Analysis

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

References

Neural Network

Outlines

Support Vector Machine

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Linear Discriminant Analysis (LDA)

Bayes Classiﬁer amounts to know the class posteriors P(Y|X) for optimal classiﬁcation : k∗ = argmaxk P(Y = k|X)

Let πk = P(Y = k) be the prior probability, fk(x) = P(X = x|Y = k) be the density function of samples in each class Y = k

By Bayes theorem, P(Y|X = x) ∝ fk(x)πk (Recall naive Bayes) • Assume fk(x) is multivariate Gaussian : (x−µk), with a common covariance

log

P(Y = k|X = x) P(Y = l|X = x)

1 2 + xTΣ−1(µk − µl)

πk πl

(µk + µl)TΣ−1(µk − µl)

=log

−

for the decision boundary between class k and l

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Discriminant Rule

Linear discriminant functions : δk(x) = xTΣ−1µk − 1

2µT P(Y=l|X=x) = δk(x) − δl(x)

k Σ−1µk + logπk

Then log P(Y=k|X=x) • Decision rule : k∗ = argmaxk δk(x) • Sample estimate of unknowns : ˆπk = Nk/N, where k=1 Nk, ˆµk = 1 Nk

Then log P(Y=k|X=x) • Decision rule : k∗ = argmaxk δk(x) • Sample estimate of unknowns : ˆπk = Nk/N, where yi=k xi, yi=k(xi − ˆµk)(xi − ˆµk)T

Then log P(Y=k|X=x) • Decision rule : k∗ = argmaxk δk(x) • Sample estimate of unknowns : ˆπk = Nk/N, where N = (cid:80)K ˆΣ = 1

(cid:80)K

(cid:80)

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Two-class LDA

LDA rule classiﬁes to class 2 if

(x −

ˆµ1 + ˆµ2 2

)TΣ−1(ˆµ2 − ˆµ1) + log

ˆπ2 ˆπ1

> 0

Discriminant direction : β = Σ−1(ˆµ2 − ˆµ1) • Bayes misclassﬁcation rate = 1 − Φ(βT(µ2 − µ1)/(βTΣβ) where Φ(x) is the Gaussian distribution function

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Other Variants

Quadratic discriminant analysis (QDA) : 2 log |Σk| − 1

2(x − µk)TΣ−1 • Regularized discriminant analysis : ˆΣk(α) = αˆΣk + (1 − α)ˆΣ • Computations for LDA :

k (x − µk) + log πk

1. Sphere the data with respect to ˆΣ = UDUT : X∗ = D− 1 Then the common covariance estimate of X∗ is Ip

2. Classsify to the closest class centroid in the transformed space, taking into account of the class prior probabilities πk’s

Reduced-Rank LDA : see dimensionality reduction

References

Logistic Regression

Linear Discriminant Analysis

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

References

Neural Network

Outlines

Support Vector Machine

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

What is Deep Learning?

Support Vector Machine

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Deep Learning

Deep learning is a sub ﬁeld of

Machine Learning that very closely tries to mimic human brain’s working using neurons.

These techniques focus on building Artiﬁcial Neural Networks (ANN) using several hidden layers.

There are a variety of deep learning networks such as Multilayer Perceptron (MLP), Autoencoders (AE), Convolution Neural Network (CNN), Recurrent Neural Network (RNN).

Support Vector Machine

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

ML vs DL

In Machine Learning, the features need to be identiﬁed by an domain expert.

In Deep Learning, the features are learned by the neural network.

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Why Deep Learning is Growing?

Processing power needed for Deep learning is readily becoming available using GPUs, Distributed Computing and powerful CPUs.

Moreover, as the data amount grows, Deep Learning models seem to outperform Machine Learning models.

Explosion of features and datasets.

Focus on customization and real time decisioning.

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Why Now?

Support Vector Machine

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

History

Support Vector Machine

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Big Guys

Support Vector Machine

References

Logistic Regression

Linear Discriminant Analysis

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

References

Neural Network

Outlines

Support Vector Machine

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

The Perceptron : Forward Propagation

ˆy = g

(cid:32)

w0 +

m (cid:88)

xiwi

i=1

ˆy is the Output,

g is a Non-linear activation function,

w0 is the Bias.

(cid:33)

References

,

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

The Perceptron : Forward Propagation

ˆy = g

(cid:16)

w0 + XTW

(cid:17)

,where

X =



 

x1 ... xm



  and W =



 

w1 ... wm



 .

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Common Activation Functions

Note all activation functions are nonlinear.

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Single Layer Neural Network

0,i + (cid:80)m

zi = w(1)

j=1 xjw(1) j,i ,

ˆyi = g

(cid:16)

0,i + (cid:80)d1 w(2)

j=1 zjw(2)

j,i

(cid:17)

.

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Deep Neural Network

0,i + (cid:80)dk−1 Theorem (Universal approximation theorem (Cybenko 1980, 1989))

zk,i = w(k)

j=1 g (zk−1,j)w(k) j,i .

1. Any function can be approximated by a three-layer neural network within suﬃciently high accuracy.

2. Any bounded continuous function can be approximated by a two-layer neural network within suﬃciently high accuracy.

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Loss Optimization

We want to ﬁnd the network weights that achieve the lowest loss

W∗ = argmin

W

1 n

n (cid:88)

i=1

L

(cid:16)

f

(cid:16)

x(i);W

(cid:17)

,y(i)(cid:17)

,

where L(cid:0)f (cid:0)x(i);W(cid:1),y(i)(cid:1) is the loss function we deﬁned according to the speciﬁc problem to measure the diﬀerences between output state f (cid:0)x(i);W(cid:1) and reference state y(i). It also can be written as

W∗ = argmin

C(W).

W

Remember

W =

(cid:110)

W(0),W(1),···

(cid:111)

.

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Gradient Decent

We can use Gradient Decent algorithm to ﬁnd the optimal parameter W.

Note that we should calculate ∂C

∂W to update W.

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Notations

wl

jk is the weight for the connection from the kth neuron in the (l − 1)th

layer to the jth neuron in the lth layer.

for brevity bl • al

j = wl

j0 is the bias of the jth neuron in the lth layer.

j for the activation of the jth neuron in the lth layer zl j . (cid:33)

(cid:32)

(cid:88)

j = g(zl al

wl

k + bl

jkal−1

j ) = g

j

k

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Four fundamental equations

We ﬁrst deﬁne the error δl

j of neuron j in layer by

δl j ≡

∂C ∂zl j

,

and we give the four fundamental equations of back propagation :

δL = ∇aC (cid:12) σ(cid:48) (cid:16)

zL(cid:17) (cid:19)

(cid:18)(cid:16)

wl+1(cid:17)T

δl =

δl+1

(cid:12) σ(cid:48) (cid:16)

zl(cid:17)

(BP1)

(BP2)

∂C ∂bL j ∂C ∂wl jk

= δl j

= al−1 k

δl j

(BP3)

(BP4)

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

An equation for the error in the output layer (BP1)

The components of δL are given by

δL = ∇aC (cid:12) σ(cid:48) (cid:16)

zL(cid:17)

(BP1)

D´emonstration. δL j =

∂C ∂zL j

=

=

=

(cid:88)

k

∂C ∂aL j ∂C ∂aL j

∂C ∂aL k

∂aL j ∂zL j σ(cid:48) (cid:16)

∂aL k ∂zL j

=

zL j

(cid:17)

∂C ∂aL j

∂σ

(cid:16)

zL j ∂zL j

(cid:17)

.

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

An equation for the error in the hidden layer (BP2)

δl =

(cid:18)(cid:16)

wl+1(cid:17)T

δl+1

(cid:19)

(cid:12) σ(cid:48) (cid:16)

zl(cid:17)

(BP2)

D´emonstration.  = (cid:80) j = ∂C ∂C δl  k ∂zl+1 ∂zl j k (cid:16)(cid:80) (cid:17) zl+1 i wl+1 ki al k = i  ∂zl+1 j = (cid:80) k δl+1 δl  k k ∂zl j kj σ(cid:48) (cid:16) ∂zl+1 = wl+1 k ∂zl j



⇒



∂zl+1 k ∂zl j + bl+1

= (cid:80)

k =

∂zl+1 k ∂zl j i wl+1

k δl+1 k (cid:16)(cid:80)

ki σ (cid:0)zl

i

(cid:1)(cid:17)

+ bl+1 k

zl j

(cid:17) ⇒ δl

j = (cid:80)

kj σ(cid:48) (cid:16)

k δl+1

k wl+1

zl j

(cid:17)

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

The change of the cost with respect to any bias (BP3)

∂C ∂bL j

= δl j

(BP3)

D´emonstration.  ∂zl = (cid:80) ∂C  k k ∂zl ∂bl j k (cid:16)(cid:80) jkdl−1 k wl k

∂C ∂bl j zl j =



∂zl j ∂bl j

= ∂C ∂zl j (cid:17) + bl

j ⇒ ∂zl

k ∂bl j

= 1

⇒ ∂C ∂bl j

= ∂C ∂zl j

1 = δl j.

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

The change of the cost with respect to any weight (BP4)

∂C ∂wl jk

= al−1 k

δl j

(BP4)

D´emonstration.

 



∂C ∂wl jk

zl j =

∂zl j ∂wl jk

∂zl ∂C i ∂zl ∂wl i jk jmal−1 m wl m

= (cid:80) i (cid:16)(cid:80)

= ∂C ∂zl j (cid:17)

∂zl j ∂wl jk ∂zl j ∂wl jk

+ bl

j ⇒

∂C ∂zl j

∂C ∂wl jk

⇒

=

= al−1 k

= δl

jal−1 k

.

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Back Propagation procedure

1. Input x : Set the corresponding activation a1 for the input layer.

2. Feedforward : For each l = 2,3,...,L compute zl = wlal−1 + bl and al = σ (cid:0)zl(cid:1).

3. Output error δL : Compute the vector δL = ∇aC (cid:12) σ(cid:48) (cid:0)zL(cid:1). • 4. Backpropagate the error : For each l = L − 1,L − 2,...,2 (cid:12) σ(cid:48) (cid:0)zl(cid:1).

5. Output : The gradient of the cost function is given by = δl j. ∂C ∂wl jk

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Gradient Descent

Support Vector Machine

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Stochastic Gradient Descent

Mini-batches lead to fast training!

Can parallelize computation + achieve signiﬁcant speed increases on GPUs.

References

Logistic Regression

Linear Discriminant Analysis

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

References

Neural Network

Outlines

Support Vector Machine

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Support Vector Machine (SVM)

Use hyperplane to separate data : maximize margin • Can deal with low-dimensional data that are not linearly separated by using kernel functions

Decision boundary only depends on some samples (support vectors)

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Linear SVM

Training data : {(x1,y1),...,(xn,yn)}, yi ∈ {−1,+1} • Hyperplane : S = wTx + b ; decision function : f (x) = sign(wTx + b)

f (xi) > 0 ⇔ yi = 1 f (xi) < 0 ⇔ yi = −1

(cid:41)

⇒ yif (xi) > 0

Geometric margin between a point and hyperplane : ri = yi(wTxi+b)

(cid:107)w(cid:107)2

Margin between dataset and hyperplane : min ri

Maximize margin : max w,b min i

yi(wTxi+b) (cid:107)w(cid:107)2

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Formulation as Constrained Optimization

Without loss of generality, let min i

Without loss of generality, let min i

yi(wTxi + b) = 1 (multiply

Maximize margin is equivalent to

max w,b

1 (cid:107)w(cid:107)2

,

s.t. yi(wTxi + b) (cid:62) 1,i = 1,...,n

Further reduce to

min w,b

1 2

(cid:107)w(cid:107)2 2,

s.t. yi(wTxi + b) (cid:62) 1,i = 1,...,n

This is primal problem : quadratical programming with linear constraints, computational complexity is O(p3) where p is dimension

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Method of Lagrange Multipliers

Introduce αi (cid:62) 0 as Lagrange multiplier of constraint

yi(wTxi + b) (cid:62) 1 • Lagrange function : 2(cid:107)w(cid:107)2

L(w,b,α) = 1

2 −

n (cid:80) i=1

αi[yi(wTxi + b) − 1]

Since

max α

L(w,b,α) =

 



1 2 + ∞,

(cid:107)w(cid:107)2 2,

yi(wTxi + b) − 1 (cid:62) 0,∀i

yi(wTxi + b) − 1 < 0,∃i

Primal problem is equivalent to the minimax problem

min w,b

max α

L(w,b,α)

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Dual problem

When slater condition is satisﬁed, minmax ⇔ maxmin • Dual problem : max min w,b

∇wL = 0 =⇒ w∗ =

(cid:88)

αiyixi

i

∂L ∂b

= 0 =⇒

(cid:88)

i

αiyi = 0

Plug into L : L(w∗,b∗,α) = (cid:80) i

αi − 1 2

(cid:80) i

(cid:80) j

αiαiyiyj(xT

i xj)

Dual optimization :

min α

1 2

(cid:88)

(cid:88)

αiαjyiyj(xT

i xj) −

(cid:88)

αi,

j s.t. αi (cid:62) 0,i = 1,...,n,

i

(cid:88)

i

αiyi = 0

i

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

KKT conditions

Three more conditions from the equivalence of primal and minimax problems

 



(cid:62) 0,

α∗ i yi((w∗)Txi + b∗) − 1 (cid:62) 0, i [yi((w∗)Txi + b∗) − 1] = 0. α∗

These together with two zero derivative conditions form KKT conditions

αi > 0 ⇒ yi(wTxi + b∗) = 1 • Index set of support vectors S = {i|αi > 0} • b = ys − wTxs = ys − (cid:80) i∈S i xs

More stable solution : b = 1 |S| (cid:80) s∈S

More stable solution : b = 1 |S| (cid:80) s∈S

ys − (cid:80) i∈S

αiyixT

i xs

(cid:17)

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Sequential Minimal Optimization (SMO) Algorithm

Invented by John C. Platt (1998) • Coordinately optimize dual problem, select two variables and ﬁx others, then dual problem reduces to one variable quadratic programming with positivity constraint

1. Initially, choose αi and αj 2. Fix other variables, solve for αi and αj 3. Update αi and αj, redo step 1 iteratively 4. Stop until convergence

How to choose αi and αj ? choose the pair far from KKT conditions the most

Computational complexity O(n3) • Easy to generalize to high dimensional problem with kernel functions

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Soft Margin

When data are not linear separable, introduce slack variables (tolerance control of fault) ξi (cid:62) 0

Relax constraint to yi(wTxi + b) (cid:62) 1 − ξi • Primal problem : n (cid:88)

Relax constraint to yi(wTxi + b) (cid:62) 1 − ξi • Primal problem : n (cid:88)

1 2

(cid:107)w(cid:107)2

min w,b

ξi

i=1

s.t. yi(wTxi + b) (cid:62) 1 − ξi,ξi (cid:62) 0,i = 1,...,n

Similar derivation to dual problem :

min α

1 2

(cid:88)

(cid:88)

αiαjyiyj(xT

i xj) −

(cid:88)

αi,

i

j

i

s.t. 0 (cid:54) αi (cid:54) C,i = 1,...,n,

(cid:88)

αiyi = 0

i

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Nonlinear SVM

Nonlinear decision boundary could be mapped to linear boundary in high-dimensional space

Modify objective function in dual problem : αiαjyiyj(φ(xi)Tφ(xj)) − (cid:80) i 1 2

Kernel function as inner product : K(xi,xj) = φ(xi)Tφ(xj)

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Kernel Methods

Reduce eﬀect of curse of dimensionality

Diﬀerent kernels lead to diﬀerent decision boundaries

Popular kernels :

Kernel Polynomial

Gaussian

Laplacian Fisher

Deﬁnition 1 x2 + 1)d (xT e− (cid:107)x1−x2(cid:107)2 e− (cid:107)x1−x2(cid:107)

2δ2

δ2 tanh(βxT 1 x2 + θ)

Parameters d is positive integer

δ > 0

δ > 0 β > 0,θ < 0

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

Pros and Cons

Where it is good

Applications in pattern recognition : text classiﬁcation, face recognition

Easy to deal with high-dimensional data with kernels • Robust (only depends on support vectors), and easy to generalize to new dataset

Disadvantage

Poor for ultra high dimensional data • Low computational eﬃciency for nonlinear SVM when sample size is large

Poor interpretability without probability

References

Logistic Regression

Linear Discriminant Analysis

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

References

Neural Network

Outlines

Support Vector Machine

References

Logistic Regression

Linear Discriminant Analysis

Neural Network

Support Vector Machine

References

Œ(cid:226)'(cid:219)(cid:19)(cid:216)§˘(cid:228)(cid:140)Œ(cid:226)˘(cid:19) • –(cid:147)u§¯(cid:236)˘S§2016 • T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning : Data mining, Inference, and Prediction, 2nd Edition, 2009

References

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Introduction to Big Data Analysis Ensemble Methods

Zhen Zhang

Southern University of Science and Technology

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Outlines

Introduction

Bagging and Random Forest

Boosting and AdaBoost

Gradient Boosting Decision Tree

XGBoost

Conclusion and Python Examples

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Ensemble Methods

Wisdom of Crowds (“n(cid:135)ˇ(cid:153)œ§”(cid:135)ˆ(cid:129)(cid:0)”) • Multiple weak learners (base learners, may be heterogenous) can improve learning performance

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Why it can improve the performance

More expressive, can approximate larger functional space • Single linear classiﬁer (perceptron) does not work • Try multiple classiﬁers

Reduce misclassﬁcation rate

Misclassﬁcation rate of single classiﬁer is p • Choose N classiﬁers, same but independent, voting • Error rate of majority vote = (cid:80) k )pk(1 − p)N−k (N k>N/2

When N = 5,p = 0.1, Error rate < 0.01

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Two commonly used ensemble methods

Bagging

Random sampling :

generating independent models, and averaging for regressions (making majority vote for classiﬁcations) • Reducing variances • Example : Random forests

Boosting

Sequential training :

training the subsequent models based on the errors of previous models

Reducing bias • Examples : AdaBoost and GBDT

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Outlines

Introduction

Bagging and Random Forest

Boosting and AdaBoost

Gradient Boosting Decision Tree

XGBoost

Conclusion and Python Examples

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Bagging

Bagging is short for bootstrap aggregation • Bagging generates a committee of predictors and combine them in a certain manner to the ﬁnal model

Single predictor suﬀers from instability, while bagging could improve the stability by majority vote (classiﬁcation) or averaging (regression) over all single predictors

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Sampling

Given a dataset D of n samples, at the iteration

m = 1,...,M, the training set Dm is obtained by sampling from D with replacement. Then Dm is used to construct classiﬁer ˆfm(x).

Sampling with replacement : some samples in D may be missing in Dm, while some other samples may occur more than once

On average, 63.2% of the samples in D could be selected into Dm. In fact, for each sample, the probability that it is not selected in one round is 1 − 1 rounds with probability lim n→∞ n)n = 0.368.

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Algorithm

Input : training set D = {(x1,y1),...,(xN,yN)} • Output : additive model ˆfbag(x)

1. For m = 1 to M :

1.1 Sample from D with replacement to obtain Dm 1.2 Train a model ˆfm(x) from the dataset Dm : for classiﬁcation,

ˆfm(x) returns a K-class 0-1 vector ek ; for regression, it is just a value

2. Compute bagging estimate ˆfbag(x) = 1 M

M (cid:80) m=1

ˆfm(x) : for

classiﬁcation, make majority vote ˆGbag(x) = argmaxk ˆfk(x); for regression, just return the average value

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Variance Reduction

In bagging, we use the same model to train diﬀerent sample set in each iteration; assume the models {ˆfm(x)}M m=1 have the same variance σ2(x), while the correlation of each pair is ρ(x)

Then the variance of the ﬁnal model is :

Var(ˆfbag(x)) =

1 M2

(cid:16) M (cid:88)

m=1

Var(ˆfm(x)) +

(cid:88)

t(cid:54)=m

(cid:17) Cov(ˆft(x)ˆfm(x))

= ρ(x)σ2(x) +

1 − ρ(x) M

σ2(x)

As M → ∞, Var(ˆfbag(x)) → ρ(x)σ2(x). This usually reduces the variance.

If ρ(x) = 0, the variance could approach zero • The random sampling in bagging is to reduce the correlation ρ(x), i.e., make the sub-predictors as independent as possible

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Limitations of Decision Tree

Stuck at local optimum : The greedy algorithm makes it stop at the local optimum, as it seeks the maximal information gain in each tree split

Decision boundary : Use one feature in each split, the decision boundary is parallel to the coordinate axes

Bad representability and instability

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Random Forest

Random Forest further reduces the variance by adding independency to the committee of decision trees

This is achieved by introducing more randomness. • More randomness :

Sampling on the training data with replacement • Select features at random

No pruning is needed.

Example : RF consisting of 3 independent trees, each with an error rate of 40%. Then the probability that more than one tree misclassify the samples is 0.43 + 3 ∗ 0.42 ∗ (1 − 0.4) = 0.352

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Random Forest Algorithm

Input : training set D = {(x1,y1),...,(xN,yN)} • Output : additive model ˆfrf (x)

1. For m = 1 to M :

1.1 Sample from D with replacement to obtain Dm 1.2 Grow a random-forest tree Tm to the dataset Dm : by

recursively repeating the following steps for each terminal node of the tree, until the minimum node size nmin is reached 1.2.1 Select q features at random from the p features 1.2.2 Pick the best feature/split-point among the q 1.2.3 Split the node into two daughter nodes

2. Output the ensemble of trees {Tm}M M (cid:80) m=1

2. Output the ensemble of trees {Tm}M M (cid:80) m=1

m=1 : for regression,

Tm(x) : for classiﬁcation, make majority vote

Small value of q increases the independency of trees; empirically, q = log2 p + 1

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Model Evaluation

Margins : The diﬀerence between the percentage of decision trees that correctly classify the samples and the percentage of trees misclassifying the samples

Out-of-bag (OOB) erros : The observation is called out-of-bag sample to some trees if it is not sampled for those trees. Denote the training set in the m-th sampling by Dm. OOB error is computed as :

1. For each observation (xi,yi), ﬁnd the trees which treat it as OOB sample : {ˆTm(x) : (xi,yi) /∈ Dm}

2. Use those trees to classify this observation and make majority vote as the label of this observation : ˆfoob(xi) = argmax y∈Y

M (cid:80) m=1

I(ˆfm(xi) = y)I(xi /∈ Dm)

3. Compute the number of misclassiﬁed samples, and take the ratio of this number to the total number of samples as OOB i=1 I(ˆfoob(xi) (cid:54)= yi) error : Erroob = 1 N (cid:80)N

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Feature Importance

Using split criteria

The improvement in the split-criterion as feature importance

It is accumulated over all the

trees for each variable

Using OOB randomization

Randomly permute the values of each feature in the OOB samples, and compute the prediction accuracy

The decrease in accuracy as a result of this permutation is averaged over all trees as feature importance

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Pros and Cons

Where it is good

Bagging or random forest (RF) work for models with high variance but low bias

Better for nonlinear estimators • RF works for very high-dimensional data, and no need to do feature selection as RF gives the feature importance

Easy to do parallel computing

Disadvantage

Overﬁtting when the samples are large-sized with great noise, or when the dimension of data is low

Slow computing performance comparing to single tree • Hard to interpret

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Outlines

Introduction

Bagging and Random Forest

Boosting and AdaBoost

Gradient Boosting Decision Tree

XGBoost

Conclusion and Python Examples

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Boosting

Boosting : combines the outputs of many “weak” classiﬁers to produce a powerful “committee”

Weak classiﬁer : error rate < 0.5 (random guessing)

Sequentially apply the weak classiﬁers to the repeatedly modiﬁed data, emphasizing the misclassiﬁed samples

Combine weak classiﬁers through a weighted majority vote or averaging to produce the ﬁnal prediction

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Boosting Fits an Additive Model

Additive model : f (x) =

M (cid:80) m=1

βmb(x;γm)

Possible choices for basis function b(x;γ) :

Neural networks : σ(γ0 + γT • Wavelets • Cubic spline basis • Trees • Eigenfunctions in reproducing kernel Hilbert space (RKHS)

1 x), where σ(t) = 1/(1 + e−t)

N (cid:80) i=1 • Loss function : squared error L(y,f (x)) = (y − f (x))2 or

Parameter ﬁtting : min {βm,γm}

Parameter ﬁtting : min {βm,γm}

L(yi,

βmb(xi;γm))

likelihood-based loss

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Forward Stagewise Additive Modeling

Input : training set D = {(x1,y1),...,(xN,yN)} • Output : additive model fM(x)

1. Initialize f0(x) = 0 2. For m = 1 to M :

2.1 Compute (βm,γm) = argmin β,γ

N (cid:80) i=1

L(yi,fm−1(xi) + βb(xi;γ))

2.2 Update fm(x) = fm−1(x) + βmb(xi;γm)

Squared error loss : in step 2.1,

L(yi,fm−1(xi) + βb(xi;γ)) = (yi − fm−1(xi) (cid:125)

(cid:124)

(cid:123)(cid:122) residual

−βb(xi;γ)2

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Exponential Loss and AdaBoost

Exponential loss : L(y,f (x)) = exp(−yf (x)) • Classiﬁer as basis function : b(x;γ) = G(x) ∈ {−1,1} • Let w(m) n (cid:88)

Exponential loss : L(y,f (x)) = exp(−yf (x)) • Classiﬁer as basis function : b(x;γ) = G(x) ∈ {−1,1} • Let w(m) i = exp(−yifm−1(xi)), then step 2.1 turns to be :

w(m) i

(βm,Gm) = argmin β,G

exp(−βyiG(xi))

i=1 (cid:104) (cid:88)

w(m) i

(eβ − e−β) + e−β

n (cid:88)

w(m) i

(cid:105)

yi(cid:54)=G(xi)

i=1

Gm = argmin G

βm = argmin β w(m) i

βm = argmin β w(m) i

(cid:15)m = (

n (cid:80) I(yi (cid:54)= G(xi)). i=1 (cid:15)m(eβ − e−β) + e−β(cid:105) (cid:104)

w(m) i

= 1

2 log 1−(cid:15)m (cid:15)m

where

I(yi

(cid:54)= G(xi)))/

n (cid:80) i=1

w(m) i

is weighted error rate

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

AdaBoost Algorithm

Input : training set D = {(x1,y1),...,(xN,yN)}, loss function L(y,f (x))

Output : Weighted classiﬁer G(x)

1. Initialize wi = 1/N, i = 1,...,N 2. For m = 1 to M :

2.1 Fit a classiﬁer Gm(x) to the training data D with weight {wi}

2.2 Compute the error (cid:15)m = (

2.3 Compute αm = log 1−(cid:15)m (cid:15)m 2.4 Update the weight w(m+1)

i

n (cid:80) i=1 (αm = 2βm > 1)

w(m) i

I(yi

(cid:54)= G(xi)))/

n (cid:80) i=1

w(m) i

= w(m) i

exp(αmI(yi

(cid:54)= Gm(xi)), for

i = 1,...,N

3. Output G(x) = sign

(cid:104) M (cid:80) m=1

(cid:105) αmGm(x)

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Illustration

Weights of weak classiﬁers : the better the classiﬁer is, the larger its weight is

Weights of samples :

Re-weighting after each step, increase the weights for misclassiﬁed samples

Simulation : 2-class

classiﬁcation, 1000 training samples from each class, 10,000 test samples; two-leaf classiﬁcation tree (stump) as base learner

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Loss Functions

For classiﬁcation, exponential loss and binomial negative log-likelihood (deviance) loss log(1 + exp(−2yf )) share the same population minimizer; thus it is equivalent to MLE rule

For classiﬁcation, squared error loss is not good (not monotonically decreasing); the exponential loss is good and binomial deviance is better (less penalty for large −yf )

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Pros and Cons

Where it is good

AdaBoost improve the classiﬁcation performance comparing to weak classiﬁers

Many choices for weak classiﬁers : trees, SVMs, kNNs, etc. • Only one tuning parameter M : # of weak classiﬁers • prevent overﬁtting suﬀered by single weak classiﬁers (e.g. complex decision tree)

Disadvantage

Weak interpretability • Overﬁtting when using very bad weak classiﬁers • Sensitive to outliers • Not easy for parallel computing

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Outlines

Introduction

Bagging and Random Forest

Boosting and AdaBoost

Gradient Boosting Decision Tree

XGBoost

Conclusion and Python Examples

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Boosting Tree

Using classiﬁcation trees or regression trees as base learners

fM(x) =

M (cid:80) m=1

T(x;Θm) where T(x;Θ) =

J (cid:80) j=1

γjI(x ∈ Rj)

Parameter set Θ = {Rj,γj}J • Parameter ﬁnding : minimizing the empirical risk j=1

ˆΘ = argmin Θ

J (cid:88)

j=1

(cid:88)

xi∈Rj

L(yi,γj)

(Combinatorial optimization)

Approximate suboptimal solutions : 1. Finding γj given Rj : γj = ¯yj = 1 |Rj| (cid:80) yi∈Rj

yi for L2 loss; and

γj = modal class in Rj for misclassiﬁcation loss

2. Finding Rj given γj : Diﬃcult, need to estimate γj as well; greedy, top-down recursive partitioning algorithm

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Boosting Tree as Forward Stagewise Algorithm

N (cid:80) i=1 1. ˆγjm = argmin γjm

ˆΘm = argmin Θm

L(yi,fm−1(xi) + T(xi;Θm))

(cid:80) xi∈Rjm

L(yi,fm−1(xi) + γjm)

2. Finding Rjm is more diﬃcult than for a single tree in general.

Squared-error loss : ﬁt a tree to the residual L(yi,fm−1(xi) + T(xi;Θm)) = (yi − fm−1(xi) (cid:125) (cid:123)(cid:122) residual

−T(xi;Θm))2

Two-class classiﬁcation and exponential loss : AdaBoost for

trees, ˆΘm = argmin Θm

(cid:80)

1. ˆγjm = log

(cid:80)

xi ∈Rjm

xi ∈Rjm

N (cid:80) i=1 w(m) i w(m) i

w(m) i

exp[−yiT(xi;Θm)]

I(yi=1)

I(yi=−1)

Absolute error or the Huber loss : robust but slow

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Gradient Descent for General Loss

Supervised learning is equivalent to the optimization problem

min f

L(f ) = min

f

N (cid:88)

i=1

L(yi,f (xi))

Numerical optimization : ˆf = argmin f

Numerical optimization : ˆf = argmin f

f = {f (x1),f (x2),...,f (xN)},

M (cid:80) m=0 • Gradient descent method : fm = fm−1 − ρmgm, where

Approximate ˆf by fM =

hm, where f0 = h0 is initial guess

gim =

(cid:104)∂L(yi,f (xi)) ∂f (xi)

(cid:105)

f (xi)=fm−1(xi)

, and hm = −ρmgm

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Gradient Boosting Decision Tree (GBDT)

Find a tree T(x;Θm) by minimization problem

˜Θm = argmin Θm

N (cid:88)

i=1

(−gim − T(xi;Θm))2

In general ˜Rjm (cid:54)= Rjm

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

GBDT Algorithm

Input : training set D = {(x1,y1),...,(xN,yN)}, loss function L(y,f (x))

Output : boosting tree ˆf (x)

1. Initialize f0(x) = argmin γ

N (cid:80) i=1

L(yi,γ)

2. For m = 1 to M :

2.1 For i = 1,2,...,N compute rim =

(cid:104)∂L(yi,f (xi)) ∂f (xi)

(cid:105)

f =fm−1

2.2 Fit a regression tree to the target (residual) rim, giving

terminal regions Rjm, j = 1,...,Jm

2.3 For j = 1,...,Jm, compute

(cid:80) xi∈Rjm

γjm = argmin

L(yi,fm−1(xi) + γ)

γ

2.4 Update fm(x) = fm−1(x) +

Jm(cid:80) j=1

γjmI(xi ∈ Rjm)

3. ˆf (x) = fM(x)

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Regularization Techniques

Shrinkage : the step 2.4 is modiﬁed as fm(x) = fm−1(x) +

ν

Jm(cid:80) j=1

γjmI(xi ∈ Rjm)

Subsampling : at each

iteration, sample a fraction η of the training set and grow the next tree using the subsample

Shrinkage + subsampling : best performance

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Feature importance and Partial Dependence Plots

Feature importance

When ﬁtting a single tree T, at each node t, one feature Xv(t) and one separate value Xv(t) = cv(t) are chosen to improve a certain quantity of criterion (e.g. GINI, entropy, squared error, etc.)

Sum all these improvements it brought by each feature Xk over J−1 (cid:80) t=1 • Average the improvements of all trees ⇒ importance of that Ik(Tm)

Partial Dependence Plots

Partial dependence of f (X) on XS : fS(XS) = EXCf (XS,XC) • Estimate by empirical mean : ¯fS(XS) = 1 N N (cid:80) i=1

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Pros and Cons

Where it is good

For all regression problems • Better for two-class classiﬁcation, possible for multi-class problems (not suggested)

Various nonlinearity, strong representability

Disadvantage

Sequential process, inconvenient for parallel computing • High computational complexity, not suitable for high-dimensional problems with sparse features

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Outlines

Introduction

Bagging and Random Forest

Boosting and AdaBoost

Gradient Boosting Decision Tree

XGBoost

Conclusion and Python Examples

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Introduction

Developed by Tianqi Chen

(http ://homes.cs.washington.edu/∼tqchen/)

Distributed gradient boosting : can be parallelized

Highly eﬃcient

Good performance

Out-of-Core Computing for big dataset

Cache Optimization of data structures and algorithms

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Cost Functions

Cost function : N (cid:80) i=1

F(Θm) =

L(yi,fm−1(xi) + T(xi;Θm)) + R(Θm), where

R(Θ) is regularization term (L0, L1 or L2 penalties)

Taylor expansion up to second order : L(yi,fm−1(xi)) + g(m) N (cid:80) i=1 ii T(xi;Θm)2(cid:105) 2h(m) (cid:104)∂L(yi,f (xi)) g(m) i = ∂f (xi) function, and h(m)

Taylor expansion up to second order : L(yi,fm−1(xi)) + g(m) F(Θm) ≈

i T(xi;Θm) +

+ R(Θm), where (cid:105)

is the gradient of loss (cid:105)

f (xi)=fm−1(xi) (cid:104)∂2L(yi,f (xi)) ∂f (xi)2 of the Hessian of loss function (oﬀ-diagonals are zeros).

ii =

is the diagonal

f (xi)=fm−1(xi)

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Penalties

Take regression trees as examples : Let Jm be the number of leaf nodes (number of rectangles in the partition), γjm is the approximate constant (weight w) in the leaf node (region) Rjm

The complexity of tree is the sum of L0 and L2 norm of {γjm} : R(Θm) = 2λ(cid:80)Jm 1 j=1 γ2 jm + µJm

R =

1 2

λ(4 + 0.01 + 1) + 3µ

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Optimal solutions

Reformulation of approximated cost function : N (cid:80) i=1

Reformulation of approximated cost function : (cid:17)

Reformulation of approximated cost function : Jm(cid:80) j=1 + µJm = (cid:105)

g(m) i

L(yi,fm−1(xi)) +

F(Θm) ≈ (cid:16)(cid:80)

γjm +

(cid:17) h(m) ii + λ

(cid:105)

1 2 Jm(cid:80) j=1 j = (cid:80) G(m)

xi∈Rjm

(cid:104) G(m) j

2(H(m) g(m) i

γjm + 1

j + λ)γ2 jm

+ µJm + constant, where

and H(m)

j = (cid:80)

h(m) ii

xi∈Rjm

xi∈Rjm

By diﬀerentiation w.r.t. γjm, we have the optimal solution : G(m) j H(m) j +λ • Simpliﬁed cost function :

By diﬀerentiation w.r.t. γjm, we have the optimal solution : G(m) j H(m) j +λ • Simpliﬁed cost function :

F(Θm) = −1 2

Jm(cid:80) j=1

(G(m) j H(m) j +λ

)2

+ µJm + constant

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Structure Score

Neglecting the constant term, we obtain the structure score : (G(m) j H(m) j +λ

Neglecting the constant term, we obtain the structure score : (G(m) j H(m) j +λ

+ µJm

It is similar to information gain : minimizing the structure score leads to the best tree

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Node Splitting - Greedy Algorithm

When splitting a node into left (L) and right (R) child nodes, (cid:104) G2 HL+λ + G2 L R

When splitting a node into left (L) and right (R) child nodes, (cid:104) G2 HL+λ + G2 L R

left to right

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Greedy Algorithm for split ﬁnding

Input : training set D = {(x1,y1),...,(xN,yN)}, loss function L(y,f (x)), the index set I = {i|xi ∈ Rjm} of current node Rjm, feature dimension d

Output : best split

1. Initialize gain = 0, G = (cid:80) 2. For k = 1 to K : 2.1 GL = 0, HL = 0 2.2 For j in sorted(I, by xjk), do

1. Initialize gain = 0, G = (cid:80) 2. For k = 1 to K : 2.1 GL = 0, HL = 0 2.2 For j in sorted(I, by xjk), do

i∈I gi, H = (cid:80)

2.2.1 GL = GL + gj, HL = HL + hjj, GR = G − GL, HR = H − HL 2.2.2 score = max(score, G2 HL+λ + G2 3. Output split with max score

HR+λ − G2

L

R

H+λ)

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Loss Functions

Square loss L(y,f ) = (y − f )2 : i = 2(fi − yi) = 2 × residue, h(m) g(m)

ii = 2

Logistic loss L(y,f ) = y ln(1 + e−f ) + (1 − y)ln(1 + ef ) : 1+e−fm−1(xi) + (1 − yi) ii = e−fm−1(xi) 1

(cid:16)

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Outlines

Introduction

Bagging and Random Forest

Boosting and AdaBoost

Gradient Boosting Decision Tree

XGBoost

Conclusion and Python Examples

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Conclusions

Ensemble methods have integrable abilities of single models, achieving better performance

Easy to generalize to new data

When there are strong noises, easy to overﬁt

Computationally intensive

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

Python Examples

Random forest :

from sklearn . ensemble import RandomForestClassifier r f = RandomForestClassifier ( n estimators =100) # RandomForestClassifier ( bootstrap=True ,

class weight=None , p c r i t e r i o n=’ g i n i ’ , max depth=None , max features=’ auto ’ , max leaf nodes=None , m i n i m p u r i t y s p l i t=1e−07, min samples leaf =1, min samples split =2, m i n w e ig h t f ra c t io n l e a f =0.0 , n estimators =100, n jobs =1, oob score=False , verbose =0, warm start=False )

random state=None ,

# Feature importance in random f o r e s t feature imp = pd . Series ( r f . feature importances r f . f i t ( X train , Y train ) Y p r e d i c t r f = r f . predict ( X test ) oob error = 1 − rf2 . oob score

)

AdaBoost :

from sklearn . ensemble import AdaBoostClassifier adaboost = AdaBoostClassifier ( n estimators = 50) adaboost . f i t ( X train , Y train ) adaboost . staged predict ( X train ) Y predict ada = adaboost . predict ( X test )

Introduction Bagging and Random Forest Boosting and AdaBoost Gradient Boosting Decision Tree XGBoost Conclusion and Python Examples

References

Œ(cid:226)'(cid:219)(cid:19)(cid:216)§˘(cid:228)(cid:140)Œ(cid:226)˘(cid:19) • –(cid:147)u§¯(cid:236)˘S§2016 • T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning : Data mining, Inference, and Prediction, 2nd Edition, 2009

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Introduction to Big Data Analysis Clustering Analysis

Zhen Zhang

Southern University of Science and Technology

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Outlines

Introduction

K-Means Clustering

Hierarchical Clustering

DBSCAN

Expectation-Maximization Algorithm

Spectral Clustering

Model Assessment

Case Study

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Clustering

Also called data

segmentation, group a collection of objects into subsets or /clusters0 • Results : objects in each

cluster are more similar to one another than objects in diﬀerent clusters.

Example : applications in consumption analysis

Can be used in data preprocessing

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Concepts in Clustering

Diﬀerent from classiﬁcation : it is unsupervised learning; no outputs or labels

Central goal : Optimize the similarity (or dissimilarity) between the individual objects being clustered : • Obtain great similarity of samples within cluster • Obtain small similarity of samples between clusters

Cost functions : not related to the outputs, but related to the similarity

Two kinds of input data :

n × n similarity (dissimilarity) matrix D : only depends on the distances between pairs of samples; may lose some information on data

Original data with features X ∈ Rn×d

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Clustering Methods

Clustering process :

data preprocessing,

especially standadization

Similarity matrix • Clustering Methods • Determine the best number of clusters • Clustering methods :

Partitional clustering :

K-means • K-Medoids • Spectral clustering • DBSCAN

Hierarchical clustering

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Outlines

Introduction

K-Means Clustering

Hierarchical Clustering

DBSCAN

Expectation-Maximization Algorithm

Spectral Clustering

Model Assessment

Case Study

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Introduction

K-means clustering originates from signal processing, it is quite popular in image processing (segmentation)

Group n samples to k clusters, making each sample belong to the nearest cluster

In an image, each pixel is a sample

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Idea

Data set {xi}n • Representatives : Mass center of kth-cluster Ck is ck, k = 1,...,K

Sample xi belongs to cluster k if d(xi,ck) < d(xi,cm) for m (cid:54)= k, where d(xi,xj) is dissimilarity function

Make the mass centers well-located so that the average distance between each sample to its cluster center is as small as possible

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Optimization Problem

Let C : {1,...,n} → {1,...,k} be the assignment from the data indices to the cluster indices. C(i) = k means xi ∈ Ck n (cid:80) i=1 dij + (cid:80) C(j)(cid:54)=k 1 2

Total point scatter : T = 1 2

Loss function : within-cluster point scatter K (cid:80) k=1 K (cid:80) k=1

Loss function : within-cluster point scatter K (cid:80) k=1 K (cid:80) k=1

(cid:80)

(cid:80)

W(C) = 1 2

dij ; between-cluster point scatter

C(i)=k

C(j)=k

(cid:80)

(cid:80)

dij

C(j)(cid:54)=k • Minimize W(C) is equivalent to maximize B(C)

C(i)=k

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Dissimilarities

Proximity matrices : n × n symmetric matrix D with nonnegative entries and zero diagonal elements provides information about dissimilarity between a pair of samples, this is not distance in general

Dissimilarities based on attributes :

d(xi,xj) = (cid:80)p dk(xik,xjk) = (xik − xjk)2, absolute distance dk(xik,xjk) = |xik − xjk|

k=1 dk(xik,xjk); dk can be squared distance

Weighted average : d(xi,xj) = (cid:80)p (cid:80)p ¯dk = 1 i=1 n2 inﬂuence to all features

k=1 wkdk(xik,xjk) where

(cid:80)n

(cid:80)n

j=1 dk(xik,xjk) = 2(cid:100)Var(Xk) will assign equal

Dissimilarities based on correlation : d(xi,xj) ∝ 1 − ρ(xi,xj)

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

K-Means (as Central Voronoi Tessellation)

Minimizing W(C) is in general infeasible since this is a greedy algorithm that only works for small data sets

Taking squared dissimilarity, W(C) = nk

Taking squared dissimilarity, W(C) = nk

(cid:80)

(cid:107)xi − ¯xk(cid:107)2,

C(i)=k

n (cid:80) i=1

where nk =

I(C(i) = k) is the number of samples in

(cid:80)

(cid:80)

cluster k, ¯xk = 1 nk

(cid:107)xj − mk(cid:107)2

xj = argmin mk

C(j)=k

C(j)=k

min C

W(C) ⇐⇒ min C,mk

K (cid:80) k=1

nk

(cid:80)

C(i)=k

(cid:107)xi − mk(cid:107)2

Alternating minimization :

1. Given C, solve for mk =⇒ m∗ 2. Given mk, solve for C =⇒ C(i) = arg min 1(cid:54)k(cid:54)K

k = ¯xk (choose representatives)

(cid:107)xi − mk(cid:107)2

(partitioning, equivalent to Voronoi tessellation for given center mk)

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

K-Means Iterations

The alternating iterations can stop when the mass centers {¯xk}K k=1 do not change

Initial guess :

Random guess, try the best one with smallest W(C)

Base on other clustering methods (e.g., hierarchical clustering), choose the cluster centers as initial guess

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

How to choose K

Minimizing Bayesian Information Criterion (BIC) :

BIC(M|X) = −2logPr(X|ˆΘ,M) + p log(n), where M indicates the model, ˆΘ is the MLE of the model parameters in M, Pr(X|M) is the likelihood function, and p is the number of parameters in model M; trade-oﬀ between log-likelihood and model complexity

Based on Minimum Description Length (MDL) : starting from large K, decreases K until the description length −logPr(X|ˆΘ,M) − logPr(Θ|M) achieves its minimum (similar to MAP)

Based on Gaussian distribution assumption : starting from K = 1, increases K until the points in every cluster follow Gaussian distribution

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Pros and Cons

Where it is good

Intuitive, easy to implement • Low computational complexity, O(tnpK), where t is the number of iterations

Disadvantage

Need to specify K ﬁrst (K is tuning parameter) • Strong dependence on the initial guess of cluster center • Easy to stuck at local minimum • Naturally assume ball-shaped data, hard to deal with data which are not ball-shaped

Sensitive to outliers

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Variant : Bisecting K-means

Invented to deal with initial guess of center selection • Idea : sequentially divide the poorest cluster into two sub-clusters

1. Initially gather all data into one cluster 2. Repeat :

2.1 Select the cluster k that maximizes the within-cluster point

scatter (cid:80)

(cid:80)

(cid:107)xi − xj(cid:107)2

C(i)=k

C(j)=k

2.2 Use 2-means to divide cluster k into two sub-clusters, with

random initial guess of two centers

2.3 Repeat step 2.2 p times, choose the best pair of clusters that

minimizes the within-cluster point scatter

3. Stop when there are K clusters (Or you can stop any time you like to have a satisfactory clustering result)

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Variant : K-medoids

Invented to overcome the inﬂuence of outliers

Can deal with data of general type, assuming general dissimilarity d(xi,xj)

Idea : centers for each cluster are restricted to be one of the observations assigned to that cluster

Alternating minimization :

1. Given C, solve for mk = xi∗ k

1. Given C, solve for mk = xi∗ k

1. Given C, solve for mk = xi∗ k = arg min

(cid:80)

d(xi,xj) (choose the

C(j)=k

2. Given mk, solve for C =⇒ C(i) = arg min 1(cid:54)k(cid:54)K

d(xi,mk)

More robust than K-means

More computational eﬀort when solving for the center in step k) comparing to O(nk) in K-means

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Other Variants

K-medians : use Manhattan distance (L1-distance) instead in K-means; then the centers are not means, but medians

K-means++ : designed to select good initial centers that are far away from each other

Rough-set-based K-means : each sample could be assigned to more than one cluster

Figure: K-medoids

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Outlines

Introduction

K-Means Clustering

Hierarchical Clustering

DBSCAN

Expectation-Maximization Algorithm

Spectral Clustering

Model Assessment

Case Study

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Hierarchical Clustering

Clustering in diﬀerent

hierarchies, generating tree structure

Two approaches :

Agglomerate clustering : bottom-up

Figure: Agglomerate clustering

Divisive clustering : top-down

Limitation : once merged or divided, the operation cannot be modiﬁed

Figure: Divisive clustering

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Agglomerate Clustering

Given n samples and proximity matrix, do the following steps :

1. Let every observation represent a singleton cluster 2. Merge the two closest clusters into one single cluster 3. Calculate the new proximity matrix (dissimilarity between two clusters)

4. Repeat step 2 and 3, until all samples are merged into one cluster

Three methods for computing intergroup dissimilarity :

Single linkage (SL) • Complete linkage (CL) • Average linkage (AL)

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Intergroup Dissimilarity

Single linkage : Greatest similarity or least dissimilarity dSL(Ci,Cj) = d(x,y)

min x∈Ci,y∈Cj

Complete linkage : Least similarity or greatest dissimilarity dSL(Ci,Cj) = d(x,y) max x∈Ci,y∈Cj

Average linkage : Average similarity or dissimilarity dAL(Ci,Cj) = d(x,y)

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Generalized Agglomerative Scheme

Input : training set D = {(x1),...,(xn)}, dissimilarity function d(Ci,Cj)

Output : A dendrogram containing {Rt}n−1 clustering result at time t

Output : A dendrogram containing {Rt}n−1 clustering result at time t

1. Initialize the clustering result R0 = {{x1},{x2},...,{xn}}, t = 0

2. Do iterations : 2.1 t = t + 1 2.2 Choose (Ci,Cj) from Rt−1 so that d(Ci,Cj) = min (r,s) d(Cr,Cs)

3. Stop at t = n − 1 when |Rn−1| = 1, return {Rt}n−1 t=0

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Generalized Divisive Scheme

Input : training set D = {(x1),...,(xn)}, dissimilarity function d(Ci,Cj)

Output : A dendrogram containing {Rt}n−1 i=1 is the clustering result at time t

1. Initialize R0 = {X}, t = 0 2. Do iterations : 2.1 t = t + 1 2.2 For i = 1 to t, do : 2.2.1 Choose (C1 t−1,i,C2 d(C1 t−1,i,C2 t−1,i) =

t−1,i) from Ct−1,i so that

max G (cid:83) H=Ct−1,i

d(G,H)

2.3 Choose it−1 so that it−1 = argmax 2.4 Rt = (Rt−1 \ {Ct−1,it−1})(cid:83){C1

d(C1 i t−1,i,C2

t−1,i,C2 t−1,i})

t−1,i)

3. Stop at t = n − 1 when |Rn−1| = n, return {Rt}n−1 t=0

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Pros and Cons

Where it is good

Hierarchical clustering computes tree structure of the whole clustering process in one stroke

SL and CL are sensitive to outliers, while AL gives a compromise

As n → ∞, dAL(Ci,Cj) → (cid:82) (cid:82) d(x,y)pi(x)pj(y)dxdy, the

expected dissimilarity w.r.t. the two densities pi(x) and pj(x) • In contrast, dSL(Ci,Cj) → 0 and dCL(Ci,Cj) → ∞ independent

of pi(x) and pj(x)

Disadvantage

Computationally intensive • Once a sample is incorrectly grouped into a branch, it will stay in the clusters corresponding to that branch no matter how you threshold the tree

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Outlines

Introduction

K-Means Clustering

Hierarchical Clustering

DBSCAN

Expectation-Maximization Algorithm

Spectral Clustering

Model Assessment

Case Study

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Density-based Clustering

Limitations of hierarchical clustering and K-means clustering : tend to discover convex clusters

Density-based Clustering : looks for high-density regions separated by low-density regions, could discover clusters of any shape

Density-Based Spatial Clustering of Applications with Noise (DBSCAN)

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Concepts

Three types of points :

Core point : # of samples in its (cid:15)-neighborhood (cid:62) MinPts • Boundary point : it lies in the (cid:15)-neighborhood of some core point, # of samples in its (cid:15)-neighborhood < MinPts

Noise point : neither core point nor boundary point, it lies in the sparse region

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Concepts

(cid:15)-neighborhood : for each sample xi ∈ D, N(cid:15)(xi) = {xj ∈ D|d(xi,xj) (cid:54) (cid:15)}

Directly density-reachable : if the sample xj ∈ N(cid:15)(xi), and xi is core point, then xj is directly density-reachable from xi

Density-reachable : for xi and xj, if there exist p1,...,pm, s.t. p1 = xi,pm = xj, and pk+1 is directly density-reachable from pk, then xj is density-reachable from xj

Density-connected : if there exists p, s.t. both xi and xj are density-reachable from p, then xi and xj are density-connected

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

DBSCAN Algorithm

Input : training set D = {(x1),...,(xn)}, dissimilarity function

d(Ci,Cj), parameters MinPts,(cid:15) • Output : a set of clusters {Ct}

1. Mark all samples in D as non-processed 2. For each sample p ∈ D, do :

2.1 If p has been grouped into some cluster or marked as noise

point, go to check next sample

2.2 Else, if |N(cid:15)(p)| < MinPts, then mark p as boundary point or

noise point

2.3 Else, mark p as core point, construct cluster C = N(cid:15)(p). For

each q ∈ N(cid:15)(p), do : 2.3.1 If |N(cid:15)(q)| (cid:62) MinPts, then put all un-clustered points in N(cid:15)(q)

into C

3. Stop when all samples in D have been clustered

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Examples ((cid:15) = 0.11, MinPts = 5)

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

DBSCAN vs. K-means

DBSCAN

K-Means

The clustering result is not a complete partition of original dataset (noise points are excluded)

Could deal with clusters with any shape and size

The clustering result is a complete partition of original dataset

The clusters are nearly ball-shaped

Sensitive to outliers

Could deal with noise points and outliers

The deﬁnition of cluster centers must be meaningful

The deﬁnition of density must be meaningful

Eﬃcient to deal with high-dimensional data

Not eﬃcient when dealing with high-dimensional data • No implicit assumptions on the sample distribution

The samples implicitly follow the Gaussian distribution assumption

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Pros and Cons

Computational complexity O(n × T), where t is the time for searching (cid:15)-neighborhood); in the worst case, O(n2)

In low-dimensional space, could be improved as O(nlogn) by KD-tree

Where it is good

Fast for clustering • Better to deal with noise points • Eﬀective for clusters of any shape

Disadvantage

Need large memory • Bad performance when the density is not well-distributed and the between-cluster distances are large

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Outlines

Introduction

K-Means Clustering

Hierarchical Clustering

DBSCAN

Expectation-Maximization Algorithm

Spectral Clustering

Model Assessment

Case Study

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Gaussian Mixture Models

We want to estimate the density of given data set. This is an unsupervised learning problem.

Commonly used approach is the parametric estimation, such as maximum likelihood estimate (MLE). • Consider the following set of data points :

0.39 0.06

0.12 0.48

0.94 1.01

1.67 1.68

1.76 1.80

2.44 3.25

3.72 4.12

4.28 4.60

4.92 5.28

5.53 6.22

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Latent Variables

A single Gaussian family would not be appropriate. A mixture of two Gaussian distributions seems good.

Z1 ∼ N(µ1,σ2

1), Z2 ∼ N(µ2,σ2

2), Z = (1 − Y)Z1 + YZ2,

where Y ∈ {0,1} with P(Y = 1) = c.

In general, for mixture of K Gaussian distributions, we assume there is a latent variable Y indicating which distribution the data x is sampled from, i.e., P(Y = y) = cy with y ∈ {1,...,K}. Given Y = y, the random variable X follows the conditional distribution : (cid:17) .

The density of X is then

P(X = x) =

K (cid:88)

P(Y = y)P(X = x|Y = y)

y=1

=

K (cid:88)

y=1

cy

1

(2π)d/2(Σy)1/2 exp

(cid:16)

−

1 2

(x − µy)TΣ−1

y (x − µy)

(cid:17)

.

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

MLE of Gaussian Mixture

Let θ = (cy,µy,Σy)K i=1 is

Let θ = (cy,µy,Σy)K i=1 is

L(θ) =

n (cid:88)

log Pθ(X = xi) =

n (cid:88)

log

(cid:16) K (cid:88)

Pθ(X = xi,Y = y)

(cid:17)

.

i=1

i=1

y=1

MLE : θ = arg max θ

L(θ) is hard due to the summation inside the log.

Make a simple assumption : we have known the value of the latent variable for each sample xi, i.e., Yi ∈ {1,...,K} is known, then the choice from the Yi-th Gaussian becomes deterministic, and the log-likelihood is replaced by n (cid:88)

Make a simple assumption : we have known the value of the latent variable for each sample xi, i.e., Yi ∈ {1,...,K} is known, then the choice from the Yi-th Gaussian becomes deterministic, and the log-likelihood is replaced by (cid:17)

Make a simple assumption : we have known the value of the latent variable for each sample xi, i.e., Yi ∈ {1,...,K} is known, then the choice from the Yi-th Gaussian becomes deterministic, and the log-likelihood is replaced by (cid:17)

Make a simple assumption : we have known the value of the latent variable for each sample xi, i.e., Yi ∈ {1,...,K} is known, then the choice from the Yi-th Gaussian becomes deterministic, and the log-likelihood is replaced by (cid:16) K (cid:88)

i=1

y=1

=

n (cid:88)

K (cid:88)

I(Yi=y) log Pθ(X = xi,Y = y).

i=1

y=1

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Expectation-Maximization (EM) Algorithm (Dempster, Laird, and Rubin, 1977’)

But Yi is indeed random, so that the event Yi = y happens with a y=1 Qi,y = 1.

Consider the modiﬁed objective function deﬁned over Q = (Qi,y)i=1,...,n;y=1,...,K and θ

F(Q,θ) =

n (cid:88)

K (cid:88)

Qi,y log

(cid:16)

Pθ(X = xi,Y = y)

(cid:17)

.

i=1

y=1

The optimization problem (Q,θ) = arg max Q,θ alternatingly :

The optimization problem (Q,θ) = arg max Q,θ alternatingly :

1. E-Step : Given θ(m), solve for

Q(m+1) = Eθ(m)(I(Y=y)|X = xi) = Pθ(m)(Y = y|X = xi);

2. M-Step : Given Q(m+1), solve for θ(m+1) = argmax θ

2. M-Step : Given Q(m+1), solve for θ(m+1) = argmax θ

(assume this is tractable).

Initial values of Q(0) and θ(0) are chosen randomly. • Terminate until satisfactory (not always converge to the maximum, but guaranteed convergent).

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Illustrative Example (Algorithm)

EM Algorithm for two-component Gaussian Mixture :

1. Take initial guesses for the parameters ˆθ1 = (ˆµ1, ˆσ2 ˆcφ ˆθ2

2. E-Step : ˆqi = P(Yi = 1|Z = zi, ˆθ1, ˆθ2) = (1−ˆc)φ ˆθ1

2. E-Step : ˆqi = P(Yi = 1|Z = zi, ˆθ1, ˆθ2) = (1−ˆc)φ ˆθ1

1), ˆθ2 = (ˆµ2, ˆσ2 (zi) (zi)+ˆcφ ˆθ2

2), ˆc ;

i = 1,2,...,n ;

3. M-Step : Compute the weighted means and variances :

ˆµ1 =

ˆµ2 =

(cid:80)n (cid:80)n

i=1(1 − ˆqi)zi i=1(1 − ˆqi) i=1 ˆqizi i=1 ˆqi

(cid:80)n (cid:80)n

ˆσ2 1 =

ˆσ2 2 =

(cid:80)n

i=1(1 − ˆqi)(zi − ˆµ1)2 i=1(1 − ˆqi) i=1 ˆqi(zi − ˆµ2)2 i=1 ˆqi

(cid:80)n

(cid:80)n

(cid:80)n

and the mixing probability ˆc = 1 n

(cid:80)n

i=1 ˆqi ;

4.

Iterate between E-Step and M-Step until convergence.

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Illustrative Example (Result)

Iteration ˆc

1 0.485

5 0.493

10 0.523

15 0.544

20 0.546

The ﬁnal MLEs are ˆµ1 = 4.62, ˆσ2 ˆc = 0.546.

1 = 0.87, ˆµ2 = 1.06, ˆσ2

2 = 0.77,

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

EM as Maximization-Maximization

Introduce entropies as penalty to the modiﬁed objective function :

G(Q,θ) = F(Q,θ) −

n (cid:88)

K (cid:88)

Qi,y log Qi,y,

i=1

y=1

where Q ∈ Q = {Q ∈ [0,1]n,K : (cid:80)K

y=1 Qi,y = 1,∀i}

M-Step is equivalent to : θ(m+1) = argmax θ

G(Q(m+1),θ)

E-Step is equivalent to : Q(m+1) = argmax Q∈Q conditional maximization) : by Jensen’s inequality,

E-Step is equivalent to : Q(m+1) = argmax Q∈Q conditional maximization) : by Jensen’s inequality,

G(Q,θ(m)) =

(cid:54)

n (cid:88)

i=1

n (cid:88)

i=1

(cid:16) K (cid:88)

Qi,y log

y=1 (cid:16) K (cid:88)

log

Qi,y

y=1

Pθ(m)(X = xi,Y = y) Qi,y

Pθ(m)(X = xi,Y = y) Qi,y

(cid:17)

(cid:17)

= L(θ(m))

where “=” iﬀ

P

θ(m)(X=xi,Y=y) Qi,y

= C, ∀i,y ⇔ Qi,y = Pθ(m)(Y = y|X = xi).

• Monotonicity : L(θ(m+1)) (cid:62) L(θ(m))

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

EM for Gaussian Mixture as Soft K-Means

For simplicity, assume Σy = I for any y. • E-Step (Partition-Step in K-Means) : Pθ(m)(Y = y|X = xi) = 1 Zi where Zi is a normalization factor. (cid:80)n

(cid:16)

y (cid:107)2(cid:17)

c(m) y

2(cid:107)xi − µ(m)

− 1

exp

(cid:16)

(cid:80)K

log cy − 1

M-Step : max cy,µy leads to

M-Step : max cy,µy leads to

y=1 Pθ(m)(Y = y|X = xi)

i=1

,

µy =

n (cid:88)

Pθ(m)(Y = y|X = xi)xi

(Mean-Step in K-Means)

i=1

cy =

(cid:80)n

i=1 Pθ(m)(Y = y|X = xi) (cid:80)n

(cid:80)K

i=1 Pθ(m)(Y = y(cid:48)|X = xi)

y(cid:48)=1

(for partition in next step)

“Soft” because the partition is done in probabilistic sense instead of deterministic sense and the average is weighted according to the probability.

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Summary of EM Algorithm

EM is unsupervised learning, an approach to perform MLE for mixture models with latent variables

EM is an alternating optimization • EM can be viewed as soft K-Means • EM can deal with problems including missing data (treat missing data as latent variables and use Bayes formula, see Section 8.5.2 in the book “Elements of Statistical Learning” for general EM algorithm).

EM can be used in the framework of Bayesian reasoning (MAP), e.g., Variational Bayesian EM algorithm

EM is related to generative model, e.g., EM for Gaussian mixture is a population approach to learning the sample distributions, analogous to Gibbs sampling which is sampling approach to learning the distribution. • EM is a general methodology, even used in natural language processing (e.g., latent dirichlet allocation), deep learning (e.g., restricted Boltzmann machine, deep belief network)

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Outlines

Introduction

K-Means Clustering

Hierarchical Clustering

DBSCAN

Expectation-Maximization Algorithm

Spectral Clustering

Model Assessment

Case Study

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Graphs

A set of data points {x1,...,xn}, similarity sij or distance dij • Graph G = (V,E), where V = {vi}n representing a sample xi

vi and vj are connected (wij > 0) if sij > (cid:15) where (cid:15) (cid:62) 0 is a threshold; then the edge is weighted by wij = sij

Undirected graph wij = wji, adjacency matrix W = {wij} • Degree of vi : di = (cid:80)n j=1 wij ; D = diag(d1,...,dn)

W =



       

0 1 1 0 0 0 0

1 0 1 0 0 0 0

1 1 0 1 1 0 0

0 0 1 0 0 1 1

0 0 1 0 0 0 0

0 0 0 1 0 0 0

0 0 0 1 0 0 0



       

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Similarity Graphs

(cid:15)-neighborhood graph : vi and vj are connected if

d(xi,xj) < (cid:15); unweighted graph; (cid:15) ∼ (logn/n)p ; diﬃcult to choose (cid:15) for data on diﬀerent scales

k-nearest neighbor graph : connect vi to vj if vj is among the k-nearest neighbors of vi, directed graph; connect vi and vj if vi and vj are among the k-nearest neighbors of each other, mutual k-nearest neighbor graph, undirected; k ∼ logn

Fully connected graph : connect all points with positive similarity with each other; model local neighborhood relationships; Gaussian similarity function s(xi,xj) = exp(−(cid:107)xi − xj(cid:107)2/(2σ2)), where σ controls the width of neighborhoods; adjacency matrix is not sparse; σ ∼ (cid:15)

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Graph Laplacian

Unnormalized graph Laplacian : L = D − W

Has 1 as an eigenvector corresponding to the eigenvalue 0 • Symmetric and positive deﬁnite : fTLf = 1 2 • Non-negative, real-valued eigenvalues 0 = λ1 (cid:54) λ2 (cid:54) ··· (cid:54) λn • The eigenspace of eigenvalue 0 is spanned by the indicator vectors 1A1,...,1Ak, where A1,...,Ak are k connected components in the graph (cid:80)

Normalized graph Laplacians :

Symmetric Laplacian : Lsym = D−1/2LD−1/2 • Random walk Laplacian : Lrw = D−1L • Both have similar properties as L

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Spectral Clustering

Graph cut : segment G into K clusters A1,...,AK, where Ai ⊂ V, this is equivalent to minimize the graph cut function

cut(A1,...,AK) =

1 2

K (cid:88)

k=1

W(Ak, ¯Ak)

where W(A,B) = (cid:80) singleton and its complement

i∈A,j∈B wij. Trivial solution consists of a

RatioCut : RatioCut(A1,...,AK) = 1 2

K (cid:80) k=1

W(Ak,¯Ak) |Ak|

, where |A|

is the number of vertices in A

Normalized cut : Ncut(A1,...,AK) = 1 2 vol(A) = (cid:80)

i∈A di ; it is NP-hard

K (cid:80) k=1

W(Ak,¯Ak) vol(Ak) , where

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Relaxation of RatioCut to Eigenvalue Problems with K = 2

min A⊂V

RatioCut(A, ¯A)

Binary vector f = (f1,...,fn)T as indicator function : if vi ∈ A

Binary vector f = (f1,...,fn)T as indicator function : |¯A|/|A|, (cid:112)

Binary vector f = (f1,...,fn)T as indicator function : if vi ∈ A

||A|/¯A|,

if vi ∈ ¯A

f TLf = |V| · RatioCut(A, ¯A),

n (cid:80) i=1

fi = 0, and (cid:107)f (cid:107)2

2 = n

Relax f to be real-valued : min f ∈Rn n

f TLf , subject to f ⊥ 1 and

By Rayleigh-Ritz theorem, the solution f is the eigenvector corresponding to the second smallest eigenvalue of L

Cluster {fi}n else vi ∈ ¯A

i=1 to two groups C and ¯C : vi ∈ A if fi ∈ C, and

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Relaxation of RatioCut and Ncut with general K

RatioCut

Binary vector hj = (h1j,...,hnj)T, j = 1,...,K, as indicator if vi ∈ Aj

Binary vector hj = (h1j,...,hnj)T, j = 1,...,K, as indicator if vi ∈ Aj

j Lhj = Cut(Aj, ¯Aj)/|Aj|, H = (h1,...,hK) ∈ Rn×K, RatioCut(A1,...,AK) = Tr(HTLH), HTH = I

hT

Relax H : min H∈Rn×K

Tr(HTLH), subject to HTH = I

Solution : the ﬁrst K eigenvectors of L as columns • Cluster the rows of H to K groups

Ncut

Replacing |Aj| by vol(Aj), the same argument for the Tr(HTLH), subject to HTDH = I

Replacing |Aj| by vol(Aj), the same argument for the relaxation of Ncut : min

Solution : the ﬁrst K eigenvectors of Lrw as columns

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Spectral Clustering Algorithm

Input : Similarity matrix S ∈ Rn×n, number k of clusters • Output : Clusters A1,...,AK of indices of vertices • Algorithm :

1. Construct a similarity graph G = (V,E) with weighted adjacency matrix W

2. Compute the unnormalized graph Laplacian L or normalized graph Laplacian Lsym or Lrw

3. Compute the ﬁrst K eigenvectors U = [u1,...,uK] ∈ Rn×K 4. In the case of Lsym, normalize the rows of U to norm 1; for the other two cases, skip this step

5. Let yi ∈ RK be the i-th row of U, use K-means to cluster the i=1 into clusters C1,...,CK

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Mixture of 4 Gaussians on R :

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Interpretations

Usually better than K-means

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Outlines

Introduction

K-Means Clustering

Hierarchical Clustering

DBSCAN

Expectation-Maximization Algorithm

Spectral Clustering

Model Assessment

Case Study

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Two Types of Indices

External indices : validate against ground truth (labels), or compare two clusters (how similar)

Purity • Jaccard coeﬃcient and Rand index

Mutual information • Internal indices : validate without external info, based on the within-cluster similarity and between-cluster distance • Davies-Bouldin index

(DBI)

Silhouette coeﬃcient (SI)

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Purity

Let nij be the number of samples that belong to label j but were assigned to cluster i

Then ni = (cid:80)C • pij = nij/ni is the probability distribution in cluster i • Purity of cluster i : pi (cid:44) max • Total purity (cid:44) (cid:80) i j

j=1 is the total number of samples in cluster i

100%

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Confusion Matrix

SS (True Positive or TP) :

# of pairs of samples belonging to the same cluster in both models

DD (True Negative or TN) : # of pairs of samples belonging to diﬀerent clusters in both models

DS (False Negative or FN) : # of pairs of samples belonging to diﬀerent clusters in clustering model, but the same cluster in reference model

SD (False Positive or FP) : # of pairs of samples belonging to the same cluster in clustering model, but diﬀerent clusters in reference model

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Jaccard Coeﬃcient and Rand Index

Rand index (RI) : RI =

SS+DD

SS+SD+DS+DD ∈ [0,1], similar to the

accuracy in classiﬁcation problems

Jaccard coeﬃcient (JC) : JC =

SS

SS+SD+DS ∈ [0,1], compare

the similarity and diversity of the samples

Example : # of pairs in the same cluster in clustering model 6 + C2 +C2 (cid:124)

Example : # of pairs in the same cluster in clustering model 5 = 40, and

= 20, so SD = 20; # of

pairs in the same cluster in clustering model =DS + DD = 6 × 6 + 6 × 5 + 6 × 5 = 96, and +1 × 5 + 1 × 2 + 5 × 2 DS = 4 × 1 (cid:124) (cid:123)(cid:122) (cid:125) (cid:123)(cid:122) (cid:125) A B DD = 72.

+1 × 3 (cid:124) (cid:123)(cid:122) (cid:125) C

(cid:124)

= 24, so

RI =

20 + 72 20 + 20 + 24 + 72

= 0.68,

JC =

20 20 + 20 + 24

= 0.31

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Mutual Information (Wikipedia)

Mutual information (MI) measures the uncertainty decrement of one random variable given another random variable

Entropy : H(X) = −(cid:80) x

Conditional entropy :

p(x)log p(x)

Probability that a sample belongs to both cluster ui and vj : pUV(i,j) =

(cid:84) vj| n

|ui

H(X|Y) =

(cid:88)

=

(cid:88)

y p(y)(cid:0) −

p(y)H(X|Y = y)

(cid:88)

p(x|y)log p(x|y))(cid:1)

Its marginal probabilities are : vj n

Mutual information : I(U,V) = C (cid:80) j=1

pUV(i,j)log pUV (i,j) pU(i)pV (j)

y

x

MI : I(X;Y) = H(X) − H(X|Y)

MI attains its maximum

min{H(U),H(V)} only when we have many small clusters

Normalized MI : NMI(U,V) =

I(U,V) (H(U)+H(V))/2

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Davies-Bouldin Index and Silhouette Coeﬃcient

Davies-Bouldin index (DBI) measures both the within-cluster divergence and between-clusters distance

DBI = 1 k

k (cid:80) i=1

max j(cid:54)=i

(cid:16)div(ci)+div(cj) d(µi,µj)

(cid:17)

where div(ci) represents the

average distance of samples within cluster ci, µi is the center of cluster ci

Silhouette Coeﬃcient (SC) : SC = bi−ai

max(ai,bi), where ai is average distance between the i-th sample and every other sample in the same cluster, bi is the minimal distance from the i-th sample to the other clusters; range is [−1,1]

The smaller the DBI, or the larger the SC, the better the clustering results

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Outlines

Introduction

K-Means Clustering

Hierarchical Clustering

DBSCAN

Expectation-Maximization Algorithm

Spectral Clustering

Model Assessment

Case Study

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Case Study

Use clustering to group the cars with similar performance based on parameters of the cars

Dataset comes from “Auto” in the R package ISLR.

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Hierarchical Clustering

Scaling of the feature values : Auto Scaled

from scipy.cluster.hierarchy import dendrogram, linkage

Construct linkage matrix : Z = linkage(Auto Scaled, method = ’complete’, metric = ’euclidean’), possible choice for metric could be ’euclidean’, ’cityblock’, ’minkowski’, ’cosine’, ’correlation’, ’hamming’, ’jaccard’, etc.

Data structure of linkage matrix : in the t-th iteration, clusters Ci with index “Z[i, 0]” and Cj with index “Z[i, 1]” are combined to form cluster Cq with index “n + i”; “Z[i, 2]” is the distance between Ci and Cj ; “Z[i, 3]” is the number of samples in Cq

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

Dendrogram

dendrogram(Z, no labels = True)

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

K-Means Clustering

from sklearn.cluster import KMeans

clf = KMeans(n clusters=3, n init=1, verbose=1)

clf.ﬁt(Auto Scaled)

Cluster 1 : (economy or compact vehicles) high mpg, low horsepower, low weight; cluster 2 : (luxury vehicles) low mpg, high horsepower, high weight; cluster 0 : intermediate performance

Introduction K-Means Clustering Hierarchical Clustering DBSCAN Expectation-Maximization Algorithm Spectral Clustering Model Assessment Case Study

References

Œ(cid:226)'(cid:219)(cid:19)(cid:216)§˘(cid:228)(cid:140)Œ(cid:226)˘(cid:19) • –(cid:147)u§¯(cid:236)˘S§2016 • T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning : Data mining, Inference, and Prediction, 2nd Edition, 2009

Arthur, D., Vassilvitskii, S. “k-means++ : the advantages of careful seeding”. Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, Society for Industrial and Applied Mathematics Philadelphia, PA, USA. pp. 1027ı1035, 2007

Lingras P, West C, Interval Set Clustering of Web Users with Rough Kmeans, Journal of Intelligent Information Systems 23(1) :5ı16, 2004

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Introduction to Big Data Analysis Dimensionality Reduction

Zhen Zhang

Southern University of Science and Technology

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Outlines

Introduction

Principal Component Analysis

Linear Discriminant Analysis

Nonlinear Dimensionality Reduction

Feature Selection

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

What is Dimensionality Reduction

The process of reducing the number of random variables under consideration, via obtaining a set of /uncorrelated0principal variable

By mapping from high-dimensional space to low-dimensional space

Learning f : X → Y, where dimX = n and dimY = r with n > r.

Including both unsupervised learning (mostly common) and supervised learning

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Why Need Dimensionality Reduction?

Curse of dimensionality

Eg : classify cats and dogs using features, if we want to cover 20% of the feature space, how many data do we need?

However, the number of samples is limited in practice

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Why Need Dimensionality Reduction? (Cont’)

Due to the sparsity of data in high dimensions, it is easy to overﬁt • Hard to train a good model to classify the corner data (getting more in high dimensions)

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Curse of Dimensionality

The volume of hypersphere decays to zero with the increase of dimension

The performance gets worse with the increase of dimension

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Roles of Dimensionality Reduction

Data compression

Denoising

Feature extraction by mapping and feature selection (eg. Lasso)

Reduce both spatial and time complexity, so that fewer parameters are needed and smaller computational power is required

Data visualization

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Methods in Dimensionality Reduction

Linear dimensionality reduction :

Principal component analysis (PCA) • Linear discriminant analysis (LDA) • Independent component analysis (ICA)

Nonlinear dimensionality reduction :

Kernel based methods (Kernel PCA) • Manifold learning (ISOMAP, Locally Linear Embedding (LLE), Multidimensional scaling (MDS), t-SNE)

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Outlines

Introduction

Principal Component Analysis

Linear Discriminant Analysis

Nonlinear Dimensionality Reduction

Feature Selection

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Variance and Covariance Matrix

Variance measures the variability or divergence of single variable : Var(X) = E(X − EX)2, sample version (cid:80)n S2 = 1 n−1 Std(X) = (cid:112)Var(X)

i=1(xi − ¯x)2 ; standard deviation :

For more variables, Cov(X,Y) = E(X − EX)(Y − EY), (cid:80)n i=1(xi − ¯x)(yi − ¯y)

If X = (x1,...,xn)T ∈ Rn×p is the sample matrix, then n−1(X − 1n¯xT)T(X − 1n¯xT) = n X)T(X − 1 n is a projection matrix with rank n − 1.

If X = (x1,...,xn)T ∈ Rn×p is the sample matrix, then n−1(X − 1n¯xT)T(X − 1n¯xT) = n X)T(X − 1 n is a projection matrix with rank n − 1.

n X) = 1

n1n1T n1n1T

n1n1T

n−1XTJX, where

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Principal Component Analysis (PCA)

PCA transforms a set of strongly correlated variables to another set (typically much smaller) of weakly correlated variables by using orthogonal transformation

The new variables are called principal components

The new set of variables are linear combinations of the original variables whose variance information is inherited as much as possible

Unsupervised learning

Proposed by Karl Pearson, successfully used in economics by Stone (1947) : keep 97.4% information, 17 variables about income and expenditure are ﬁnally reduced to 3 variables (F1 : total income, F2 : rate of change in total income, F3 : economic development or recession)

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Geometric Interpretation

Assume a set of 2D data follows Gaussian distribution (but not limited to Gaussian distribution!), the reduction to 1D is successfully achieved by taking a direction with larger variance (larger variability of data)

The direction in the major axis contains more information than the other direction, since smaller variance indicates the variables are almost the same

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Linear Algebra

Let {ei}p

i=1 be the canonical basis in Euclidean space, want to i=1 such that the random

ﬁnd another orthonormal basis {˜ei}p vector v = (cid:80)p v = (cid:80)p Cov(˜xi, ˜xj) ≈ 0 for i (cid:54)= j

i=1 xiei can be expressed in the new basis by

i=1 ˜xi˜ei, where Var(˜x1) (cid:62) ··· (cid:62) Var(˜xp) and

By linear algebra, the coordinate transformation is given by the linear transformation : (˜e1,··· ,˜ep) = (e1,··· ,ep)W, where W ∈ Rp×p is an invertible matrix

The component coeﬃcients is transformed accordingly : x = W˜x

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Eigendecomposition of Sample Covariance Matrix

Assume we have n centralized samples {xi}n i=1 xi = 0p

Then XT = (x1,··· ,xn) = W(˜x1,··· ,˜xn) = W ˜XT • The sample covariance matrix of X is Cov(X) = 1 • The sample covariance matrix of ˜X is n−1W TXTXW = W TCov(X)W

Its diagonals are the sample versions of Var(˜x1),...,Var(˜xp), while its oﬀ-diagonals are the covariances between ˜xi and ˜xj • Need that Cov(˜X) is nearly diagonal with decreasing diagonal entries for some W.

Equivalent to do eigendecomposition :

Cov(X) = Odiag(λ1,··· ,λp)OT with some orthogonal matrix O ∈ Rp×p and λ1 (cid:62) ··· (cid:62) λp (cid:62) 0, then let W = O completes the job

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Interpretations

Variances in the transformed variables : Var(˜xi) = λi, eigenvalues of Cov(X)

The new basis consists of the columns of W = O, i.e., the eigenvectors of Cov(X)

The percentage

λi j=1 λj

(cid:80)p

explains the importance of the new

variable ˜xi

Given a thereshold t, we can choose the number of variables r such that the total contribution to the variance of the new r variables (cid:80)r exceeds the threshold t. Thus these r directions w1,...,wr are enough to represent the original n variables (cid:80)p

For any random vector x ∈ Rp, the corresponding r principal 1 x,...,wT r x components are thus wT

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Another Viewpoint - Best Reconstruction

Note that the new basis {˜ej}p • After the projection (if we keep the ﬁrst r components), the projected point of each sample xi is ˜xi,1w1 + ··· + ˜xi,rwr, where the coordinate is given by ˜xi,j = wT j xi ;

The reconstruction error is the sum of all squared L2 errors of all samples :

RE(W) =

n (cid:88)

(cid:107)

r (cid:88)

˜xi,jwj − xi(cid:107)2

2 =

n (cid:88)

(cid:107)(WrW T

r − I)xi(cid:107)2 2

i=1

j=1

i=1

=

n (cid:88)

i (I − WrW T xT

r )xi = Tr(

n (cid:88)

xixT

i (I − WrW T

r ))

i=1

i=1

=Tr(X TX(I − WrW T

r )) = Tr(X TX) − Tr(W T

r X TXWr)

Resulting in an optimization problem :

min Wr

−Tr(W T

r X TXWr),

subject toW T

r Wr = I

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

PCA Algorithm

Given the data matrix X = (x1,...,xn)T ∈ Rn×p and a threshold t (in some other cases, the number of principal components r) :

1. Centralize the data by their mean ¯x = 1 n1T

1. Centralize the data by their mean ¯x = 1 sample covariance matrix C = 1

2. Compute the eigenvalues {λi}p eigenvectors {wi}p

2. Compute the eigenvalues {λi}p eigenvectors {wi}p

i=1

3. Order the eigenvalues as λ(1) (cid:62) ··· (cid:62) λ(p), and compose an orthogonal matrix W by the eigenvectors columnwise in the same order : W = (w1,...,wp)

4. Compute the variance contribution of the ﬁrst r eigenvalues : (cid:80)r

4. Compute the variance contribution of the ﬁrst r eigenvalues : , ﬁnd a suitable r such that this variance

i=1

contribution is greater than the threshold t

5. Pick the ﬁrst r columns in W and form a matrix Wr = (w1,...,wr) ∈ Rp×r

6. Output ˜Xr = XWr ∈ Rn×r as the projected data matrix, whose rows consist of data points in r dimensional subspace

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

An Example

The data : the monthly prices of three brands of vehicles (Jeep : x1, Toyota : x2, Benz : x3) • The the covariance matrix is given by

C =



 

1 2√ 10 − 2√ 10

2√

10 1 −4 5

− 2√ 10 −4 5 1



 

Compute the characteristic polynomial :

det(λI − C) =

(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)

λ − 1 − 2√ 10 − 2√ λ − 1 10 4 2√ 5

10

2√ 10 4 5 λ − 1

(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)

Solve for the eigenvalues : λ1 = 2.38, λ2 = 0.42, λ3 = 0.2

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

An Example (Cont’)

Plug in each eigenvalues and solve for the corresponding eigenvectors, e.g., (λ1I − C)w1 = 0, or equivalently, w12 + 2√ 10 

10

One can ﬁnd three eigenvectors as w1 = (0.54,0.59,−0.59)T, w2 = (0.84,−0.39,0.39)T, w3 = (0,0.71,0.71)T

The three components are

˜x1 = wT ˜x2 = wT ˜x3 = wT

1 x = 0.54x1 + 0.59x2 − 0.59x3, 2 x = 0.84x1 − 0.39x2 + 0.39x3, 3 x = 0.71x2 + 0.71x3.

As λ1 (cid:29) λ2,λ3, the ﬁrst principal component ˜x1 reﬂects the change of prices in all three brands of vehicles

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Outlines

Introduction

Principal Component Analysis

Linear Discriminant Analysis

Nonlinear Dimensionality Reduction

Feature Selection

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Linear Discriminant Analysis (LDA)

Supervised learning : based on the labels, do linear projection in order to maximize the between-class point scatter (variability) in low dimensions

Initially proposed by R. Fisher for two-class classiﬁcation (1936)

Generalized by C. R. Rao (1948) to K classes {C1,...,CK}

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Basic Concepts

The number of samples in each class is nk = (cid:80) i:xi∈Ck k=1 nk

The number of samples in each class is nk = (cid:80) i:xi∈Ck k=1 nk

(cid:80) i:xi∈Ck

xi, whereas

the mean of all samples is µ = (cid:80)K

k=1

nk n µk

Before projection, the between-class point scatter is nk n (µk − µ)(µk − µ)T ; after projection Sb = (cid:80)K Wr ∈ Rp×r, the between-class point scatter is ˜Sb = W T r SbWr • Before projection, the within-class point scatter (variance) for (xi − µk)(xi − µk)T, thus the

k=1

(cid:80) i:xi∈Ck

each class Ck is Sk = 1 nk total within-class point scatter is Sw = (cid:80)K projection, the within-class point scatter for each class Ck is ˜Sk = W T ˜Sw = W T

nk n Sk ; after

k=1

r SkWr, and the total within-class point scatter is r SwWr

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Optimization Problem

Need to ﬁnd the optimal directions (columns of Wr) such that the between-class point scatter ˜Sb is maximized and within-class point scatter ˜Sw is minimized, i.e.,

max w

J(w) =

wTSbw wTSww

This is equivalent to solve

max w

Jb(w) = wTSbw,

subject to wTSww = 1

By introducing a Lagrange multiplier λ, we deﬁne Lagrangian as L(w,λ) = wTSbw − λ(wTSww − 1)

The optima is obtained as the solution to the equation

∇wL = 2Sb − 2λSww = 0 ⇒ S−1

w Sbw = λw

• The optimal directions are the eigenvectors of S−1

w Sb

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

An Example

Given two sets of data : class 1 is

{(4,1)T,(2,4)T,(2,3)T,(3,6)T,(4,4)T}, and class 2 is {(9,10)T,(6,8)T,(9,3)T,(8,7)T,(10,8)T}

Class means : µ1 = (3,3.6)T, µ2 = (8.4,7.6)T, the point scatter metrics are

S1 =

Sb =

(cid:16) 0.8 −0.4 2.6 (cid:16) 7.29 4.86 4.86 3.24

−0.4

(cid:17)

(cid:17)

,

,

S2 =

Sw =

(cid:16) 1.84 −0.28 5.36 (cid:16) 1.32 −0.34

−0.28

−0.34

4

(cid:17)

(cid:17)

.

,

The eigenvalue of S−1

w Sb is solved from

0 = det(λI −S−1

w Sb) =

(cid:12) (cid:12) (cid:12)

λ − 5.97 −3.98

−1.72

λ − 1.15

(cid:12) (cid:12) (cid:12) ⇒ λ = 7.11

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

An Example (Cont’)

The optimal directions is w∗ = (0.96,0.28)T • After projection, the data become 1D : • Class 1 : {4.12,3.03,2.75,4.55,4.95} • Class 2 : {11.42,7.98,9.48,9.63,11.83}

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

PCA vs. LDA

PCA

Start from sample covariance matrix and ﬁnd directions with maximal variances

Unsupervised learning, used as pre-training step, must be coupled with other learning methods

LDA

Make use of labels and ﬁnd projections after which the classiﬁcation becomes more obvious

Supervised learning, can be used as classiﬁcation or coupled with other learning methods

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Outlines

Introduction

Principal Component Analysis

Linear Discriminant Analysis

Nonlinear Dimensionality Reduction

Feature Selection

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Kernel PCA

PCA works well for Gaussian distribution

If the data do not follow Gaussian, we can ﬁnd a map φ : Rp → Rq so that φ(x) (almost) follows Gaussian

We can do PCA for the transformed data {φ(xi)}n • Similar to nonlinear SVM, kernel trick can be used to avoid explicit computation of φ

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Covariance Matrix in Transformed Space

Assume the transformed data are centralized : i=1 φ(xi) = 0

Covariance Matrix ˜C = 1 n−1 • Do PCA for transformed data is equivalent to ﬁnd the eigenvalues and eigenvectors of ˜C

Let λ be an eigenvalue of ˜C and v ∈ Rq be the corresponding eigenvector, i.e., ˜Cv = λv. • It can be shown that v = (cid:80)n λ(n−1)φ(xi)Tv

i=1 αiφ(xi) where

αi = 1

(cid:80)n

Furthermore, αi = 1

i=1 K(xi,xj)αj, where

λ(n−1) K(xi,xj) = φ(xi)Tφ(xj) is kernel function

It is suﬃcient to solve the eigenvalue problem :

Kα = λ(n − 1)α where K = (K(xi,xj))i,j is kernel matrix and α = (αi) is the coeﬃcient vector of v

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Kernel PCA Algorithm

1. Choose a kernel function K(x,y) satisfying the necessary properties

2. Compute the kernel matrix K = (K(xi,xj))i,j 3. Compute the eigenvalues λ1 (cid:62) ··· (cid:62) λq and eigenvectors α(1),...,α(q) of K

4. For any new sample x, the j component after projection is

zj = vT

j φ(x) =

n (cid:88)

α(j) i K(xi,x)

i=1

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Kernel PCA : An Example

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Manifolding Learning

A manifold is a topological space that locally resembles Euclidean space near each point. It generalizes the concepts of curves and surfaces in Euclidean space.

The dimension of a manifold is the minimal number of coordinates to represent a point on the manifold

Some dimensionality reduction methods are based on the concept of manifold : ISOMAP, LLE, MDS, t-SNE

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Locally Linear Embedding (LLE)

Reduce the number of free coordinates while keeping the local geometric structure of the data, e.g., if xA and xB are neighbor in high dimension, after the dimension reduction (transformation), they must be close to each other in low dimension

The clustering eﬀect should also be inherited

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

LLE Reconstruction

Assume each data point is locally linearly dependent of its neighbors : it can be written as the linear combination of its K nearest neighbors {xkij}K the KNN indices {kij}K

j=1, with

j=1

The weight is determined by the optimization for each xi :

min w

(cid:107)xi −

K (cid:88)

wikijxkij(cid:107)2

2

j=1

subject to

K (cid:88)

wikij = 1, wij (cid:62) 0

j=1

where wij = 0 if j /∈ {kij}K

j=1

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Low Dimensional Representation

In r (r < p) dimensional space, ﬁnd n points such that the local structure (e.g., clustering eﬀect) is preserved

min y1,...,yn

n (cid:88)

i=1

(cid:107)yi −

n (cid:88)

j=1

wijyj(cid:107)2 2

This is equivalent to the matrix minimization problem

min Y

Tr(YTMY),

s.t. YYT = I,

where Y = (y1,...,yn)T ∈ Rn×r and M = (I − W)T(I − W) with W = (wij)n i,j=1 being the weight matrix (not necessarily symmetric)

This is solved by eigen-decomposition : The columns of Y consist of the r eigenvectors corresponding to the r smallest eigenvalues of M

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Summary of LLE

Only one tuning parameter K

Linear algebra computation

Only local information, no global information • No explicit mapping as in PCA (˜Xr = XWr)

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

The Motivation of ISOMAP

The distance between two points may be diﬀerent in diﬀerent metrics (manifold metric vs. Euclidean metric)

Geodesic distance could be a good metric instead of Euclidean distance

Computation of geodesic distance, minimal path in graph

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

ISOMAP Algorithm

Construct KNN graph G = (V,E) :

For each xi, ﬁnd its K nearest neighbors {xj}j∈N(i)

The weight of the edge

< i,j > between xi and xj is the Euclidean distance for each j ∈ N(i)

Use Floyd algorithm to compute the minimal path between each pair of vertices (i,j) as the geodesic distance dG(i,j)

Find the low dimensional representation (e.g. by MDS) :

min y1,...,yn

(cid:88)

i(cid:54)=j

(dG(i,j) − (cid:107)yi − yj(cid:107))2

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Floyd Algorithm (Complexity O(n3))

1. Initialization :

dG(i,j) =

(cid:40)

dx(i,j), ∞,

if < i,j >∈ E

otherwise

2. For each pair (i,j), update the distance as follows : for each k = 1,...,n, dG(i,j) = min{dG(i,j),dG(i,k) + dG(k,j)} 3. The ﬁnal output dG(i,j) is the geodesic distance between i and j

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Summary of ISOMAP

Only one tuning parameter K

High computational power

Preserve the global information

Sensitive to noise

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Multidimensional Scaling (MDS)

For data points in high dimensional space, x1,...,xn ∈ Rp, i,j, e.g.,

Find {yi}n

i=1 ⊂ Rr (r < p), such that the distance information

is preserved :

min y1,...,yn

SM(y1,...,yn)

where SM(y1,...,yn) = (cid:80) function. This is called least square or Kruskal-Shephard scaling

i(cid:54)=j(dij − (cid:107)yi − yj(cid:107))2 is the stress

Alternative objective function (Sammon mapping) : (dij−(cid:107)yi−yj(cid:107))2 dij • This is nonconvex minimization

takes care of small dij

i(cid:54)=j

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

t-distributed Stochastic Neighbor Embedding (t-SNE)

Developed by Laurens van der Maaten and Geoﬀrey Hinton

Eﬀective for data visualization in 2D and 3D, applications in computer security research, music analysis, cancer research, especially for bioinformatic data

Often display clusters in low dimensional space (may be false ﬁndings)

With special parameter choices, approximates a simple form of spectral clustering

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Similarity in High Dimensional Space

For data points in high dimensional space, x1,...,xn ∈ Rp, ﬁnd the similarity of xi and xj in the form of probability pij

The similarity of data point xj to data point xi is the conditional probability, pj|i, that xi would pick xj as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at xi :

pj|i =

(cid:80)

exp(−(cid:107)xi − xj(cid:107)2/2σ2 i ) k(cid:54)=i exp(−(cid:107)xi − xk(cid:107)2/2σ2 i )

pij = (pj|i + pi|j)/2n, pii = 0 • The bandwidth is adapted to the density of the data : smaller values of σi are used in denser parts of the data space

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Similarity in Low Dimensional Space

t-SNE aims to learn a set of low dimensional data y1,...,yn ∈ Rr that reﬂects the similarity pij as well as possible

The similarity between the data point yi and yj follows t-distribution : (assume qii = 0)

qij =

(cid:80)

(1 + (cid:107)yi − yj(cid:107)2)−1 k(cid:54)=l(1 + (cid:107)yk − yl(cid:107)2)−1

t-distribution is heavy tailed so that large pij (dissimilar data pair) leads to even larger qij (falls apart)

The closedness between the two similarity measures pij and qij is given by the Kullback-Leibler divergence :

DKL(P(cid:107)Q) =

(cid:88)

i(cid:54)=j

pij log

pij qij

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Comparison (Optical Character Recognition)

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Outlines

Introduction

Principal Component Analysis

Linear Discriminant Analysis

Nonlinear Dimensionality Reduction

Feature Selection

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

What is Feature Selection

Low computational cost, better accuracy (avoid overﬁtting), and better interpretation,

Feature engineering : feature extraction and selection. Feature extraction is according to the knowledge of the professions, usually done by expertise in the professional areas

Three types : Filter, Wrapper, and Embedded

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Subset Selection

Subset search :

Forward search (forward stepwise, forward stagewise) : ∅ ⇒ {x1} ⇒ {x1,x4} ⇒ ···

Backward search (backward stepwise) :

{x1,x2,...,xp} ⇒ {x1,x2,...,xp} \ {x4} ⇒ ···

Bidirectional search

Evaluation metrics :

Distances : Euclidean, Manhattan, point scatter matrices, Kullback-Leibler divergence, etc.

Information : mutual information, information gain (IG), etc. • Correlations : Pearson correlation, Maximal information coeﬃcients (MIC)

Stopping rules : number of features, number of iterations, non-incremental metrics, attaining optimality, etc.

Validation and comparison

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

Three Types of Feature Selection

Filter : ﬁlter the features by their correlations (or MIC, IG) with response variables

Wrapper : use accuracy, precision, recall, AUC, etc.

Akaike Information Criteria (AIC) : AIC = −2ln(L) + 2k • Bayes Information Criteria (BIC) : BIC = −2ln(L) + k ln(n) • Minimize AIC or BIC, where L is likelihood function, k is the number of features (parameters), n is the number of samples

Embedded :

Random forest : feature importance • Regularization : Ridge and LASSO • Recursive feature elimination (RFE) : select the best (worst) feature according to the coeﬃcients (e.g. linear regression), then do this recursively to ﬁnd the feature importance

Introduction Principal Component Analysis Linear Discriminant Analysis Nonlinear Dimensionality Reduction Feature Selection

References

Œ(cid:226)'(cid:219)(cid:19)(cid:216)§˘(cid:228)(cid:140)Œ(cid:226)˘(cid:19) • –(cid:147)u§¯(cid:236)˘S§2016 • T. Hastie, R. Tibshirani, and J. Friedman, The Elements of Statistical Learning : Data mining, Inference, and Prediction, 2nd Edition, 2009

Arthur, D., Vassilvitskii, S. “k-means++ : the advantages of careful seeding”. Proceedings of the eighteenth annual ACM-SIAM symposium on Discrete algorithms, Society for Industrial and Applied Mathematics Philadelphia, PA, USA. pp. 1027ı1035, 2007

Lingras P, West C, Interval Set Clustering of Web Users with Rough Kmeans, Journal of Intelligent Information Systems 23(1) :5ı16, 2004

Introduction

The structural building block of Deep Learning

MA333 Introduction to Big Data Science Deep Learning

Zhen Zhang

Southern University of Science and Technology

Architectures

Introduction

The structural building block of Deep Learning

Outlines

Introduction

What is Deep Learning? Why Deep Learning is Growing? History

The structural building block of Deep Learning

Forward Propagation Back Propagation

Architectures

Recurrent Neural Network (RNN) Convolutional Neural Network (CNN) Generative model

Architectures

Introduction

The structural building block of Deep Learning

What is Deep Learning?

Architectures

Introduction

The structural building block of Deep Learning

Deep Learning

Deep learning is a sub ﬁeld of

Machine Learning that very closely tries to mimic human brain’s working using neurons.

These techniques focus on building Artiﬁcial Neural Networks (ANN) using several hidden layers.

There are a variety of deep learning networks such as Multilayer Perceptron (MLP), Autoencoders (AE), Convolution Neural Network (CNN), Recurrent Neural Network (RNN).

Architectures

Introduction

The structural building block of Deep Learning

ML vs DL

In Machine Learning, the features need to be identiﬁed by an domain expert.

In Deep Learning, the features are learned by the neural network.

Architectures

Introduction

The structural building block of Deep Learning

Why Deep Learning is Growing?

Processing power needed for Deep learning is readily becoming available using GPUs, Distributed Computing and powerful CPUs.

Moreover, as the data amount grows, Deep Learning models seem to outperform Machine Learning models.

Explosion of features and datasets.

Focus on customization and real time decisioning.

Architectures

Introduction

The structural building block of Deep Learning

Why Now?

Architectures

Introduction

The structural building block of Deep Learning

History

Architectures

Introduction

The structural building block of Deep Learning

Big Guys

Architectures

Introduction

The structural building block of Deep Learning

Outlines

Introduction

What is Deep Learning? Why Deep Learning is Growing? History

The structural building block of Deep Learning

Forward Propagation Back Propagation

Architectures

Recurrent Neural Network (RNN) Convolutional Neural Network (CNN) Generative model

Architectures

Introduction

The structural building block of Deep Learning

The Perceptron : Forward Propagation

ˆy = g

(cid:32)

w0 +

m (cid:88)

xiwi

i=1

ˆy is the Output,

g is a Non-linear activation function,

w0 is the Bias.

Architectures

(cid:33)

,

Introduction

The structural building block of Deep Learning

The Perceptron : Forward Propagation

ˆy = g

(cid:16)

w0 + XTW

X =



 

x1 ... xm



  and W =

Architectures

(cid:17)

,where



 

w1 ... wm



 .

Introduction

The structural building block of Deep Learning

Common Activation Functions

Note all activation functions are nonlinear.

Architectures

Introduction

The structural building block of Deep Learning

Single Layer Neural Network

zi = w(1)

0,i + (cid:80)m

j=1 xjw(1) j,i ,

ˆyi = g

(cid:16)

0,i + (cid:80)d1 w(2)

j=1 zjw(2)

j,i

(cid:17)

.

Architectures

Introduction

The structural building block of Deep Learning

Deep Neural Network

0,i + (cid:80)dk−1 Theorem (Universal approximation theorem (Cybenko 1980, 1989))

zk,i = w(k)

j=1 g (zk−1,j)w(k) j,i .

1. Any function can be approximated by a three-layer neural network within suﬃciently high accuracy.

2. Any bounded continuous function can be approximated by a two-layer neural network within suﬃciently high accuracy.

Architectures

Introduction

The structural building block of Deep Learning

Loss Optimization

We want to ﬁnd the network weights that achieve the lowest loss

W∗ = argmin

W

1 n

n (cid:88)

i=1

L

(cid:16)

f

(cid:16)

x(i);W

(cid:17)

,y(i)(cid:17)

,

where L(cid:0)f (cid:0)x(i);W(cid:1),y(i)(cid:1) is the loss function we deﬁned according to the speciﬁc problem to measure the diﬀerences between output state f (cid:0)x(i);W(cid:1) and reference state y(i). It also can be written as

W∗ = argmin

C(W).

W

Remember

W =

(cid:110)

W(0),W(1),···

(cid:111)

.

Architectures

Introduction

The structural building block of Deep Learning

Gradient Decent

We can use Gradient Decent algorithm to ﬁnd the optimal parameter W.

Note that we should calculate ∂C

∂W to update W.

Architectures

Introduction

The structural building block of Deep Learning

Notations

wl

jk is the weight for the connection from the kth neuron in the (l − 1)th

layer to the jth neuron in the lth layer.

for brevity bl • al

j = wl

j0 is the bias of the jth neuron in the lth layer.

j for the activation of the jth neuron in the lth layer zl j . (cid:33)

(cid:32)

(cid:88)

j = g(zl al

wl

k + bl

jkal−1

j ) = g

j

k

Architectures

Introduction

The structural building block of Deep Learning

Four fundamental equations

We ﬁrst deﬁne the error δl

j of neuron j in layer by

δl j ≡

∂C ∂zl j

,

and we give the four fundamental equations of back propagation :

δL = ∇aC (cid:12) σ(cid:48) (cid:16)

zL(cid:17) (cid:19)

(cid:18)(cid:16)

wl+1(cid:17)T

δl =

δl+1

(cid:12) σ(cid:48) (cid:16)

zl(cid:17)

(BP1)

(BP2)

∂C ∂bL j ∂C ∂wl jk

= δl j

= al−1 k

δl j

(BP3)

(BP4)

Architectures

Introduction

The structural building block of Deep Learning

An equation for the error in the output layer (BP1)

The components of δL are given by

δL = ∇aC (cid:12) σ(cid:48) (cid:16)

zL(cid:17)

(BP1)

D´emonstration. δL j =

∂C ∂zL j

=

=

=

(cid:88)

k

∂C ∂aL j ∂C ∂aL j

∂C ∂aL k

∂aL j ∂zL j σ(cid:48) (cid:16)

∂aL k ∂zL j

=

zL j

(cid:17)

∂C ∂aL j

∂σ

(cid:16)

zL j ∂zL j

(cid:17)

.

Architectures

Introduction

The structural building block of Deep Learning

An equation for the error in the hidden layer (BP2)

δl =

(cid:18)(cid:16)

wl+1(cid:17)T

δl+1

(cid:19)

(cid:12) σ(cid:48) (cid:16)

zl(cid:17)

(BP2)

D´emonstration.  = (cid:80) j = ∂C ∂C δl  k ∂zl ∂zl+1 j k (cid:16)(cid:80) (cid:17) zl+1 i wl+1 ki al k = i  ∂zl+1 j = (cid:80) k δl+1 δl  k k ∂zl j kj σ(cid:48) (cid:16) ∂zl+1 = wl+1 k ∂zl j



⇒



∂zl+1 k ∂zl j + bl+1

= (cid:80)

k =

∂zl+1 k ∂zl j i wl+1

k δl+1 k (cid:16)(cid:80)

ki σ (cid:0)zl

i

(cid:1)(cid:17)

+ bl+1 k

zl j

(cid:17) ⇒ δl

j = (cid:80)

kj σ(cid:48) (cid:16)

k δl+1

k wl+1

zl j

(cid:17)

Architectures

Introduction

The structural building block of Deep Learning

The change of the cost with respect to any bias (BP3)

∂C ∂bL j

= δl j

(BP3)

D´emonstration.  ∂zl = (cid:80) ∂C  k k ∂zl ∂bl j k (cid:16)(cid:80) jkdl−1 k wl k

∂C ∂bl j zl j =



∂zl j ∂bl j

= ∂C ∂zl j (cid:17) + bl

j ⇒ ∂zl

k ∂bl j

= 1

⇒ ∂C ∂bl j

= ∂C ∂zl j

1 = δl j.

Architectures

Introduction

The structural building block of Deep Learning

The change of the cost with respect to any weight (BP4)

∂C ∂wl jk

= al−1 k

δl j

(BP4)

D´emonstration.

 



∂C ∂wl jk

zl j =

∂zl j ∂wl jk

∂zl ∂C i ∂zl ∂wl i jk jmal−1 m wl m

= (cid:80) i (cid:16)(cid:80)

= ∂C ∂zl j (cid:17)

∂zl j ∂wl jk ∂zl j ∂wl jk

+ bl

j ⇒

∂C ∂zl j

∂C ∂wl jk

⇒

=

= al−1 k

= δl

jal−1 k

.

Architectures

Introduction

The structural building block of Deep Learning

Back Propagation procedure

1. Input x : Set the corresponding activation a1 for the input layer.

2. Feedforward : For each l = 2,3,...,L compute zl = wlal−1 + bl and al = σ (cid:0)zl(cid:1).

3. Output error δL : Compute the vector δL = ∇aC (cid:12) σ(cid:48) (cid:0)zL(cid:1). • 4. Backpropagate the error : For each l = L − 1,L − 2,...,2 (cid:12) σ(cid:48) (cid:0)zl(cid:1).

5. Output : The gradient of the cost function is given by = δl j. ∂C ∂wl jk

Architectures

Introduction

The structural building block of Deep Learning

Gradient Descent

Architectures

Introduction

The structural building block of Deep Learning

Stochastic Gradient Descent

Mini-batches lead to fast training!

Can parallelize computation + achieve signiﬁcant speed increases on GPUs.

Architectures

Introduction

The structural building block of Deep Learning

Outlines

Introduction

What is Deep Learning? Why Deep Learning is Growing? History

The structural building block of Deep Learning

Forward Propagation Back Propagation

Architectures

Recurrent Neural Network (RNN) Convolutional Neural Network (CNN) Generative model

Architectures

Introduction

The structural building block of Deep Learning

A sequence modeling problem : predict the next word

France is where I grew up, but I now live in Boston. I speak ﬂuent ???.

The food was good, not bad at all. vs. The food was bad, not good at all.

Architectures

Introduction

The structural building block of Deep Learning

Sequence modeling : design criteria

To model sequences, we need to :

1. Handle variable-length sequences

2. Track long-term dependencies

3. Maintain information about order

4. Share parameters across the sequence

Today : Recurrent Neural Networks (RNNs) as an approach to sequence modeling problems

Architectures

Introduction

The structural building block of Deep Learning

RNN state update and output

Architectures

Introduction

The structural building block of Deep Learning

RNNs : backpropagation through time

Note that it reuse the same weight matrices at every time step

Architectures

Introduction

The structural building block of Deep Learning

Standard RNN gradient ﬂow : exploding gradients

Architectures

Introduction

The structural building block of Deep Learning

Standard RNN gradient ﬂow : vanishing gradients

Architectures

Introduction

The structural building block of Deep Learning

The problem of long-term dependencies

Architectures

Introduction

The structural building block of Deep Learning

Trick 1 : activation functions

Architectures

Introduction

The structural building block of Deep Learning

Trick 2 : parameter initialization

Initialize weights to identity matrix .

Initialize biases to zero.

In =



     

1 0 0 ··· 0 1 0 ··· 0 0 1 ··· ... ... ... 0 0 0 ···

...

This helps prevent the weights from shrinking to zero.

Architectures

0 0 0 ... 1



     

Introduction

The structural building block of Deep Learning

Trick 3 : gated cells

Idea : use a more complex recurrent unit with gates to control what information is passed through

Long Short Term Memory (LSTMs) networks rely on a gated cell to track information throughout many time steps.

Architectures

Introduction

The structural building block of Deep Learning

Long Short Term Memory (LSTMs)

LSTM repeating modules contain interacting layers that control information ﬂow.

LSTM cells are able to track information throughout many time steps.

Architectures

Introduction

The structural building block of Deep Learning

Architectures

Long Short Term Memory (LSTMs)

What information to throw away?

What to store?

What to output?

Introduction

The structural building block of Deep Learning

Recurrent neural networks (RNNs)

1. RNNs are well suited for sequence modeling tasks.

2. Model sequences via a recurrence relation.

3. Training RNNs with backpropagation through time.

4. Gated cells like LSTMs let us model long-term dependencies.

5. Models for music generation, classiﬁcation, machine translation.

Architectures

Introduction

The structural building block of Deep Learning

Tasks in Computer Vision

Regression : output variable takes continuous value.

Classiﬁcation : output variable takes class label. Can produce probability of belonging to a particular class.

Architectures

Introduction

The structural building block of Deep Learning

Problems in Manual Feature Extraction

Architectures

Introduction

The structural building block of Deep Learning

Learning Feature Representations

Can we learn a hierarchy of features directly from the data instead of hand engineering?

Architectures

Introduction

The structural building block of Deep Learning

Architectures

Fully Connected Neural Network

Fully Connected :

Connect

Input :

2D image.

neuron in hidden layer to all neurons in input layer.

Vector of pixel values.

No spatial information!

And many, many parameters!

And many, many parameters!

Introduction

The structural building block of Deep Learning

Using Spatial Structure

Connect patch in input layer to a single neuron in subsequent layer.

• Use a sliding window to deﬁne connections. • How can we weight the patch to detect particular features?

Architectures

Introduction

The structural building block of Deep Learning

Applying Filters to Extract Features

1 Apply a set of weights (a ﬁlter) to extract local features.

2 Use multiple ﬁlters to extract diﬀerent features.

3 Spatially share parameters of each ﬁlter. (features that matter in one part of the input should matter elsewhere.)

Architectures

Introduction

The structural building block of Deep Learning

Features of X

Image is represented as matrix of pixel values and computers are literal!

We want to be able to classify an X as an X even if it is shifted, shrunk, rotated, deformed.

Architectures

Introduction

The structural building block of Deep Learning

Filters to Detect X Features

Architectures

Introduction

The structural building block of Deep Learning

The Convolution Operation

Suppose we want to compute the convolution of a 5x5 image and a 3x3 ﬁlter.

We slide the 3x3 ﬁlter over the input image, element-wise multiply, and add the outputs :

Architectures

Introduction

The structural building block of Deep Learning

The Convolution Operation

Suppose we want to compute the convolution of a 5x5 image and a 3x3 ﬁlter.

We slide the 3x3 ﬁlter over the input image, element-wise multiply, and add the outputs :

Architectures

Introduction

The structural building block of Deep Learning

Producing Feature Maps

Diﬀerent ﬁlters have diﬀerent eﬀects!

Architectures

Introduction

The structural building block of Deep Learning

Pooling

Architectures

Introduction

The structural building block of Deep Learning

CNNs for Classiﬁcation

1. Convolution : Apply ﬁlters with learned weights to generate feature maps.

2. Non-linearity : Often ReLU. • 3. Pooling : Downsampling operation on each feature map. Train model with image data.

2. Non-linearity : Often ReLU. • 3. Pooling : Downsampling operation on each feature map. Train model with image data.

Architectures

Introduction

The structural building block of Deep Learning

ImageNet Challenge : Classiﬁcation Task

Architectures

Introduction

The structural building block of Deep Learning

The problem when go deeper

Architectures

Introduction

The structural building block of Deep Learning

Architectures

Better network structure for learning : ResNet

Introduce shortcut connections (exists in prior literature in various forms).

Key invention is to skip 2 layers. Skipping single layer do not give much improvement for some reason.

Introduction

The structural building block of Deep Learning

ResNet vs ODENet

ResNet : ht+1 = ht + f (ht,θt),

ODENet :

dh(t) dt = f (h(t),t,θ).

Architectures

Introduction

The structural building block of Deep Learning

Beyond Classiﬁcation

Architectures

Introduction

The structural building block of Deep Learning

Architectures

Supervised vs unsupervised learning

Supervised Learning

Unsupervised Learning

Data : (x, y)

x is data, y is label.

Data : x

x is data, no labels!

Goal : Learn function to map x ⇒ y.

Goal : Learn some hidden or underlying structure of the data.

Examples : Classiﬁcation, regression, object detection, semantic segmentation, etc.

Examples : Clustering, feature or dimensionality reduction, etc.

Introduction

The structural building block of Deep Learning

Generative modeling

Goal : Take as input training samples from some distribution and learn a model that represents that distribution

How can we learn pmodel(x) similar to pdata(x)?

Architectures

Introduction

The structural building block of Deep Learning

Why generative models? Outlier detection

Architectures

Introduction

The structural building block of Deep Learning

What if we just want to sample

Idea : do not explicitly model density, and instead just sample to generate new instances.

Problem : want to sample from complex distribution can not do this directly!

Solution : sample from something simple (noise), learn a transformation to the training distribution.

Architectures

Introduction

The structural building block of Deep Learning

Generative Adversarial Networks (GANs)

Generative Adversarial Networks (GANs) are a way to make a generative model by having two neural networks compete with each other.

Architectures

Introduction

The structural building block of Deep Learning

Architectures

Training GANs

Neural Network Discriminator tries to identify real data from fakes created by the generator.

Neural Network Generator tries to create imitations of data to trick the discriminator.



min θg

max θd

(cid:2)Ex∼pdata logDθd(x) + Ez∼p(z) log(cid:0)1 − Dθd

(cid:0)Gθg(z)(cid:1)(cid:1)(cid:3)

Introduction

The structural building block of Deep Learning

What if we want the model density

Flow based model

f is bijective and diﬀerentiable functions. • We have the explicitly model density pzk(zk).

Architectures

Introduction

The structural building block of Deep Learning

CVF

Theorem (Change of Variable Theorem) Given any z0 ∼ pz0(z0), if the transformation f : Rd → Rd is diﬀerentiable and bijective, then the distribution of x = f (z0) is px(x) = pz0(z0) (cid:12) (cid:12)det ∂f (cid:12) ∂z0

.

(cid:12) (cid:12) (cid:12)

The density probability follows

logpzk(zk) = logpz0(z0) −

k (cid:88)

i=1

(cid:12) (cid:12) (cid:12) (cid:12)

det

∂fi ∂zi−1

(cid:12) (cid:12) (cid:12) (cid:12)

.

we want to ﬁnd parameter λ of neural network fi satisﬁes :

argmin λ

DKL(pdata||pλ zk

) ≈ argmax

λ

m (cid:88)

i=1

logpλ zk

(x(i)),

Architectures

Introduction

The structural building block of Deep Learning

FUTURE : LONG WAY TO GO

Theory : why deep learning works? How? Interpretability.

Capacity : how deep is enough?/Network structure selection.

Manipulate network.

Geometry of loss function.

Deep learning with less data?

Interdisciplinary ﬁelds : statistics, physics, geometry, game theory...

More exciting applications! Healthcare, industrial process, ﬁnance, AI game play, animation...

Architectures

Linear Algebra

MA233 Introduction to Big Data Science Mathematical Preliminary

Zhen Zhang

Southern University of Science and Technology

References

Linear Algebra

Probability and Information Theory

Outlines

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

References

Statistics and Machine Learning

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Inner Product and Euclidean Norm

For ∀x,y ∈ Rn, their inner product is deﬁned as

< x,y >=

n (cid:88)

xiyi = xTy.

i=1

It satisﬁes

1. (Commutativity) < x,y >=< y,x >;

2. (Scalar Multiplication) < λx,y >= λ < x,y >=< x,λy >;

3. (Bilinearity) < x + y,z >=< x,z > + < y,z >, < x,y + z >=< x,y > + < x,z >;

4. (Positivity) < x,x >(cid:62) 0, and < x,x >= 0 iﬀ x = 0. √

4. (Positivity) < x,x >(cid:62) 0, and < x,x >= 0 iﬀ x = 0. √

The Euclidean norm (l2-norm) is (cid:107)x(cid:107) =

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Linear Independency and Orthogonality

Linear Independency :

A set of vectors U = {bx1,...,xk} is linearly independent if for ∀i, xi does not lie in the space spanned by x1,...,xi−1,xi+1,xk. We say U spans a subspace V if V is the span of the vectors in U. U is a basis of V if it is both independent and spans V. The dimension of V is the size of a basis of V (i.e., the number of linearly independent vectors in U).

Orthogonality :

We say that U is an orthogonal set if for all i (cid:54)= j, < xi,xj >= 0. We say that U is an orthonormal set if it is orthogonal and if for every i, (cid:107)xi(cid:107) = 1.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Gram-Schmidt Orthogonalization

Given a set of linear independent vectors V = {v1,...,vk}, we can apply Gram-Schmidt orthogonalization to obtain an orthonormal set {u1,...,uk} which have the same span as spanV. The procedure is as follows : 1. Let u1 = v1/(cid:107)v1(cid:107); 2. For j = 2 to k, project vj onto span{u1,...,uj−1} and ﬁnd i=1 < ui,vj > ui, then

the perpendicular part ˜uj = vj − (cid:80)j−1 normalize it to be uj = ˜uj/(cid:107)˜uj(cid:107);

This procedure is summarized in the matrix form : Q = AP, where Q = (u1 ···uk) ∈ Rk×k is an orthogonal matrix whose columns are given by ui’s, A = (v1 ···vk) ∈ Rk×k is a nonsingular matrix whose columns are given by vi’s, and P ∈ Rk×k is an upper tridiagonal matrix whose upper tridiagonal (i,j)-entry is given by < ui,vj >. This is known as the QR factorization : A = QR where R = P−1.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Concepts in Matrix

Kernel and Range :

Given a matrix A ∈ Rn×d, the range of A (Range(A)) is the span of its columns and the kernel of A (Ker(A)) is the subspace of all vectors that satisfy Ax = 0. The rank of A is the dimension of its range and is denoted by rank(A) or r(A) for short.

Symmetric and Deﬁnite Matrix :

A is symmetric if A = AT. A symmetric matrix A ∈ Rd×d is positive deﬁnite if for all x ∈ Rd, < x,Ax >(cid:62) 0, and equality holds if and only if (“iﬀ”) x = 0. This deﬁnition can be relaxed to give semideﬁniteness : A symmetric matrix A ∈ Rd×d is positive semideﬁnite if for all x ∈ Rd, xTAx (cid:62) 0. In particular, all the eigenvalues of a positive deﬁnite (resp. semideﬁnite) matrix are positive (resp. nonnegative). And A = BBT for some matrix B. (See next slides for eigen-decomposition)

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Eigenvalues and Eigenvectors

Let A ∈ Rd×d be a squared matrix. A nonzero vector xRd is an eigenvector of A with a corresponding eigenvalue λ if Ax = λx.

Theorem (Eigen-decomposition or Spectral Decomposition) If A ∈ Rd×d is a symmetric matrix of rank k, then there exists an orthogonal basis of Rd, x1,...,xd, such that each xi is an eigenvector of A. Furthermore, A can be written as A = (cid:80)d is the eigenvalue corresponding to the eigenvector xi. In matrix form, this is A = UDUT, where the columns of U are the vectors x1,...,xd, and D = diag{λ1,...,λd} is a diagonal matrix. Finally, r(A) is the number of nonzero λi’s, and the corresponding eigenvectors span the range of A. The eigenvectors corresponding to the zero eigenvalues span the null space of A.

i=1 λixixT

i , where each λi

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Singular Values Decomposition (SVD)

Let A ∈ Rm×n be a matrix of rank r. Unit (nonzero) vector v ∈ Rn and u ∈ Rm are called right and left singular vectors of A with corresponding singular values σ if Av = σv and uTA = σuT.

Theorem (SVD) Let A ∈ Rm×n be a matrix of rank r. Then there exist orthonormal sets of right and left singular vectors of A, say {v1,...,vr} and {u1,...,ur} respectively, and the corresponding singular values σ1,...,σr, such that A = (cid:80)r i . In matrix form, this is A = UDV T, where the columns of U are the vectors u1,...,ur, the columns of V are the vectors v1,...,vr, and D = diag{σ1,...,σd} is a diagonal matrix.

i=1 σiuivT

Corollary The squared matrices ATA ∈ Rn×n and AAT ∈ Rm×m have (a subset of) the eigenvectors {v1,...,vr} and {u1,...,ur} respectively, corresponding the the same eigenvalues σ2

1,...,σ2 r .

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Reyleigh Quotient

Theorem Let A ∈ Rm×n be a matrix of rank r. Deﬁne v1 = argmax v∈Rn:(cid:107)v(cid:107)=1 (cid:107)Av(cid:107). Then v1,...,vr

v2 = argmax v∈Rn:(cid:107)v(cid:107)=1 <v,v1>=0

(cid:107)Av(cid:107),...,vr = argmax v∈Rn:(cid:107)v(cid:107)=1 ∀i<r,<v,vi>=0

is an orthonormal set of right singular vectors of A. Remark :(Reyleigh Quotient) If A ∈ Rn×n is a squared matrix, then its eigenvalues can be found as the solution to the following optimization problems :

λ1 = max

v∈Rn:(cid:107)v(cid:107)=1

vTAv,

λ2 = max

v∈Rn:(cid:107)v(cid:107)=1 <v,v1>=0

vTAv,

...,

λn =

max v∈Rn:(cid:107)v(cid:107)=1 ∀i<n,<v,vi>=0

vTAv.

References

(cid:107)Av(cid:107),

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Power Method - Dominant Eigenvalue

Assume the eigenvalues of A can be sorted according to their magnitudes : |λ1| > |λ2| (cid:62) |λ3| (cid:62) ··· (cid:62) |λn| (cid:62) 0. If The corresponding eigenvectors {v1,...,vn} form a basis of Rn, then any vector can be expressed as x = (cid:80)n i=1 βivi. Multiplying x by A on the left for n times, we have an idea

Akx =

n (cid:88)

i=1

βiλk

i vi = λk 1

n (cid:88)

i=1

βi

(cid:0)λi λ1

(cid:1)kvi ∼ λk

1β1v1,

k → ∞

1. For any nonzero vector x, let y(0) = x;

2. For k = 0,1,... : compute the smallest integer pk such that satisfying pk , y(k+1) = Ax(k),

2. For k = 0,1,... : compute the smallest integer pk such that satisfying pk = (cid:107)y(k)(cid:107)∞, then compute x(k) = y(k)/y(k) y(k) µ(k+1) = y(k+1)

.

x(k) = v1/(cid:107)v1(cid:107)∞. Other methods : QR factorization, Householder transformations

µ(k) = λ1 and lim k→∞

It can be shown that lim k→∞

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Linear Systems

A system of m linear algebraic equations in n unknown variables can be written in the matrix form : Ax = b, where A ∈ Rm×n, x ∈ Rn, and b ∈ Rm. This system has solutions iﬀ r([A,b]) = r(A). Theorem (Solvability Condition)

1.

If Ker(A) = {0}, then Ax = b either has a unique solution or has no solution. It has a solution iﬀ b⊥Ker(AT).

2.

If Ker(A) (cid:54)= {0}, then Ax = b either has inﬁnitely many solutions or has no solution. It has a solution iﬀ b⊥Ker(AT).

If A ∈ Rn×n is a square matrix, we have a simple rule : the system has a unique solution iﬀ detA (cid:54)= 0. If the solution exists, we can solve it by x = A−1b, where A−1 is the inverse of A satisfying A−1A = AA−1 = I. Moreover, we can ﬁnd it by Cramer’s rule : xi = detAi the matrix obtained from A by replacing its i-th column with b. The direct application of this formula requires O(n!) arithmetic operations to ﬁnd detA, which is unacceptable for large n.

detA , where Ai is

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Gaussian Elimination

Gaussian Elimination is an algorithm that can reduce the computational complexity of solving linear systems to O(n3). It is equivalent to perform an elementary row transformation for A to obtain an upper or lower triangular matrix.

Another way to view Gaussian elimination is the LU decomposition : The k-th row transformation can be represented by a left multiplication by M(k), where M(k) is a lower triangular matrix with its diagonal entries being all 1’s; after n operations, A is transformed to an upper triangular matrix U, i.e., M(n) ···M(2)M(1)A = U ; since the inverse of a lower triangular matrix is also a lower triangular matrix, we have A = LU, where L = (M(1))−1(M(2))−1 ···(M(n))−1 with its diagonal entries being all 1’s.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

LU Decomposition

Theorem An n × n nonsingular matrix A can be decomposed uniquely in the form A = LU, where

L =



    

1

l21 ... ln,1

0

1 ... ···

··· ... ... ln,n−1

0 ...

0 1



    

,U =



    

u11

0 ... 0

u12

u22 ... ···

··· ... ... 0

u1,n ...

un−1,n un,n

The computational complexity for LU decomposition is O(n3). If A is symmetric, we have Cholesky decomposition A = LLT, where L is a lower diagonal matrix.



    

.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Iterative Solver

We introduce two commonly used iterative methods : Jacobi iteration and Gauss-Seidel iteration. First we write A = D − L − U, where D = diag{a11,...,ann}, L = {lij} and U = {uij} are the lower and upper diagonal parts of −A respectively. That means lij = −aij for i > j and 0 for i (cid:54) j, uij = −aij for i < j and 0 for i (cid:62) j.

Jacobi iteration : Rewrite the linear system as

Dx = (L + U)x + b, if D−1 exists (aii the iteration x(k) = D−1(L + U)x(k−1) + D−1b, k = 1,2,...

(cid:54)= 0), then we can build

Gauss-Seidel iteration : Rewrite the linear system as (cid:54)= 0), then we can

Both are easy to implement in component form and can be written as x(k) = Tx(k−1) + c, with T = D−1(L + U) for Jacobi iteration and (D − L)−1U for Gauss-Seidel iteration. (Fixed point iteration!)

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Vector Norms

Vector Norm is a non-negative real-valued function on Rn, usually denoted by (cid:107) · (cid:107) : Rn → R, with the following properties :

1. (Positivity) (cid:107)x(cid:107) (cid:62) 0 for all x ∈ Rn ; (cid:107)x(cid:107) = 0 iﬀ x = 0; 2. (Homogeneity) (cid:107)αx(cid:107) = |α|(cid:107)x(cid:107) for ∀α ∈ R and x ∈ Rn ; 3. (Triangle Inequality) (cid:107)x + y(cid:107) (cid:54) (cid:107)x(cid:107) + (cid:107)y(cid:107) for ∀x,y ∈ Rn.

Examples :

l2-norm : (cid:107)x(cid:107)2 = (

l1-norm : (cid:107)x(cid:107)1 =

n (cid:80) i=1 n (cid:80) i=1

x2 i )

|xi|;

1 2 ;

l∞-norm : (cid:107)x(cid:107)∞ = max 1(cid:54)i(cid:54)n |xi|;

Theorem Deﬁne lp-norm as (cid:107)x(cid:107)p = (

n (cid:80) i=1

xp i )

1

p, it is really a norm for p (cid:54) 1.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Vector Norms (Cont’)

Remark : i) lp-norm is not a norm for 0 < p (cid:54) 1, since the triangular inequality is not satisﬁed. It is called semi-norm. ii) Useful to deﬁne l0-norm : (cid:107)x(cid:107)0 = #{1 (cid:54) i (cid:54) n : xi (cid:54)= 0}. Induced Distances : dist(x,y) = (cid:107)x − y(cid:107), e.g., l2-distance is

(cid:107)x − y(cid:107)2 = (

n (cid:80) i=1

(xi − yi)2)

1 2.

Theorem ∀x ∈ Rn, (cid:107)x(cid:107)∞ (cid:54) (cid:107)x(cid:107)2 (cid:54) (cid:107)x(cid:107)1.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Matrix Norms

Matrix Norm is a non-negative real-valued function on Rn×m, usually denoted by (cid:107) · (cid:107) : Rn×m → R, with the following properties :

1. (Positivity) (cid:107)A(cid:107) (cid:62) 0 for all A ∈ Rn×m ; (cid:107)A(cid:107) = 0 iﬀ A = 0; 2. (Homogeneity) (cid:107)αA(cid:107) = |α|(cid:107)A(cid:107) for ∀α ∈ R and A ∈ Rn×m ; 3. (Triangle Inequality) (cid:107)A + B(cid:107) (cid:54) (cid:107)A(cid:107) + (cid:107)B(cid:107) for ∀A,B ∈ Rnp×m ;

4. (cid:107)AB(cid:107) (cid:54) (cid:107)A(cid:107)(cid:107)B(cid:107) for ∀A,B ∈ Rn×m.

Theorem If (cid:107) · (cid:107) is a vector norm on Rn, then (cid:107)A(cid:107) = max (cid:107)x(cid:107)=1

(cid:107)Ax(cid:107) = max x(cid:54)=0

is a matrix norm (called natural norm).

Corollary (cid:107)Ax(cid:107) (cid:54) (cid:107)A(cid:107)(cid:107)x(cid:107) for ∀A ∈ Rn×m and x ∈ Rn.

References

(cid:107)Ax(cid:107) (cid:107)x(cid:107)

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Matrix Norm (Cont’)

Examples :

l1-norm : (cid:107)A(cid:107)1 = max 1(cid:54)j(cid:54)n • l∞-norm : (cid:107)A(cid:107)∞ = max 1(cid:54)i(cid:54)n (cid:80)n

l1-norm : (cid:107)A(cid:107)1 = max 1(cid:54)j(cid:54)n • l∞-norm : (cid:107)A(cid:107)∞ = max 1(cid:54)i(cid:54)n (cid:80)n

i=1 |aij|; (cid:80)n

l2-norm is not trivial. For a symmetric matrix A, deﬁne its spectral radius as ρ(A) = max1(cid:54)i(cid:54)n λi, where λi(i = 1,...,n) are the eigenvalues of A. Then Theorem 1. (cid:107)A(cid:107)2 = (cid:112)ρ(ATA); 2. ρ(A) (cid:54) (cid:107)A(cid:107) for any natural norm (cid:107) · (cid:107).

Theorem (Convergence of Jacobi and Gauss-Seidel Iterations) The Jacobi and Gauss-Seidel iterations converge to the unique solution of x = Tx + c iﬀ ρ(T) < 1. Moreover, we have the error estimate (cid:107)x − x(k)(cid:107) (cid:54) (cid:107)T(cid:107)k 1−(cid:107)T(cid:107)(cid:107)x(1) − x(0)(cid:107).

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Matrix Calculus

By convention, the lowercase letter a denotes a scalar, the bold letter x = (x1,...,xn)T denotes a column vector, and the uppercase letter A = (aij) denotes an m × n matrix. Assume x (or x) is independent variables, a, b, etc. are constant vectors, A, B, etc. are constant matrices, f (x), g(x), u(x), and v(x) are (scalar or vector valued) functions of x (or x)

Vector-by-vector formula : (resulting in matrix ∂y (cid:1))

∂x = 0, ∂(Ax)

1. Linear vector-valued functions : ∂a ∂(xTA)

1. Linear vector-valued functions : ∂a ∂(xTA)

∂x = AT,

∂x = (cid:0)∂ui

(cid:1) is Jacobian,

2. Nonlinear vector-valued functions : ∂u = a ∂(u(x)) = f (x)∂(u(x)) ∂x

∂xj

, , ∂(Au(x)) ,

∂x = A∂(u(x))

∂x

∂x ∂(u(x)) ∂x

3. Chain rule : ∂g(u(x))

∂x = ∂g(u)

∂u

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Matrix Calculus

Scalar-by-vector : (resulting in row vector ∂y

∂x = (∇xy)T)

Some of the formula can be obtained from the previous page by letting the numerator be of dimension one, the others are :

1. Inner product : ∂(aTx) ∂(u(x)TAv(x)) ∂x

1. Inner product : ∂(aTx) ∂(u(x)TAv(x)) ∂x

∂x = aT, ∂aTu(x)

∂x = ∂(xTa)

∂x = aT ∂(u(x))

,

∂x

∂x + vTAT ∂(u(x)) ∂x = xT(A + AT), ∂(xTAx)

= uTA∂(v(x)) 2. Quadratic forms : ∂(xTAx) is symmetric, ∂(aTxxTb) ∂(Ax+b)TC(Dx+e) ∂x ∂x = (x−a)T 3. l2 norm : ∂(cid:107)x−a(cid:107) 4. 2nd order derivative (resulting in a matrix) :

∂x = 2xTA if A

= xT(abT + baT) ∂x = (Dx + e)TCTA + (Ax + b)TCD

(cid:107)x−a(cid:107)

∂x∂xT = (A + AT), ∂2(xTAx) ∂2f (x) ∂x∂xT = H = ( ∂f ∂xi∂xj

∂2(xTAx)

∂x∂xT = 2A if A is symmetric,

) is the Hessian matrix.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Trace and Frobenius inner product

Trace is deﬁned as the sum of the diagonal entries in a matrix : tr(A) = (cid:80)n

i=1 aii

tr(A) = tr(AT) • tr(AB) = tr(BA), tr(ABC) = tr(CAB) = tr(BCA) ∂A = BT, ∂tr(ABATC) • ∂tr(AB) • a = tr(a) for scalar a, as a result, < x,y >= tr(xTy) = tr(yxT) (useful formula)

The Frobenius inner product is deﬁned for matrices : < A,B >F= tr(ABT) = (cid:80)n Frobenius norm : (cid:107)A(cid:107)F = (cid:112)tr(AAT) = A last useful formula : d

i,j=1 aijbij. The induced norm is called

(cid:113)(cid:80)n

i,j=1 a2 ij.

dt logdet(A(t)) = tr(A(t)−1A(cid:48)(t))

References

Linear Algebra

Probability and Information Theory

Outlines

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

References

Statistics and Machine Learning

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Basics

Three Sources of Uncertainty

1. Inherent randomness in the system being modeled. (e.g. quantum mechanics, particles are probabilitic.)

2. Incomplete observability

(e.g. Monty Hall problem. 3 dogs, one of which has a bonus behind it.)

3. Incomplete modeling (e.g. linear regression.)

Random Variables (r.V.) X : r.V., its values x X = x happens with a certain probability p(X = x). Range of X may be discrete or continuous.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Discrete variables & Probability mass functions (PMF)

P(X = x) = p(x) or X ∼ p(x), 0 ≤ p(x) ≤ 1. Joint probability distribution : p(X = x,Y = y) = p(x,y) Properties :

(1) Dom(P) = Range(X) = set of all possible states of X.

(2) ∀x ∈ Range(X), 0 ≤ p(x) ≤ 1. (3) (cid:80)

p(x) = 1 (Normalized)

x∈Range(X)

e.g. uniform distribution : P(X = xi) = 1 k,

i = 1,...,k.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Continuous variables & Probability density function (p.d.f)

Properties :

(1) Dom(P) = Range(X) = set of all possible states of X.

(2) ∀x ∈ Range(X),p(x) ≥ 0. (No need for p(x) ≤ 1) (3) (cid:82) p(x)dx = 1. p(x) does not give the probability of a speciﬁc state x, but the prob of X landing inside the interval [x,x + δx) with an inﬁnitesimal δx. (prob = p(x)δx) e.g. uniform distribution function on an interval [a,b] :

u(x;a,b) =

 



0

1 b − a

x /∈ [a,b]

x ∈ [a,b]

x ∼ U(a,b)

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Marginal Probability & Conditional Probability

Marginal Probability : prob over a subset of all variables. discrete : p(x) = P(X = x) = (cid:80) y continuous : p(x) = (cid:82) p(x,y)dy

P(X = x,Y = y) = (cid:80) y

Conditional Probability : prob of some event given some other events have happened. P(X = x,Y = y) P(X = x)

P(Y = y|X = x) =

(well-deﬁned if P(X = x) > 0)

Chain Rules :

P(X(1), · · · , X(n)) = P(X(1))P(X(2)|X(1))P(X(3)|X(1), X(2)) · · · P(X(n)|X(1), · · · , X(n−1))

p(x,y)

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Independence & Conditional Independence

X and Y are independent, if ∀x,y, P(X = x,Y = y) = P(X = x)P(Y = y)

X,Y are conditionally independent given Z if ∀x,y,z, P(X = x,Y = y|Z = z) = P(X = x|Z = z)P(Y = y|Z = z).

short-hand notation : X⊥Y, X⊥Y|Z.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Expectation Variance and Covariance

Ex∼p[f (x)] =

(cid:40) (cid:80) p(x)f (x) x (cid:82) p(x)f (x)dx

,

E[αf (x) + βg(x)] = αEf (x) + βEg(x) Var[f (x)] = E[(f (x) − Ef (x))2], std[f (x)] = (cid:112)Var[f (x)] Cov[f (X),g(Y)] = E[(f (x) − Ef (x))(g(Y) − Eg(Y))] measures how much two variable are linearly related to each other as well as their scales. Cov[f (X),g(Y)] = Cov[f (X),g(Y)] X⊥Y ⇒ Cov[X,Y] = 0 or Corv[X,Y] = 0 (cid:48)(cid:48) ⇐(cid:48)(cid:48) is only true when X,Y ∼ normal distribution X) = (Cov(Xi,Yj))1≤i,j≤n Covariance Matrix : Cov( Diagonal Cov(Xi,Xi) = Var(Xi)

std[f (X),g(Y)] ∈ [−1,1] normalized

(cid:42)

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Common Prob Distributions

1) Bernoulli : binary r.v. one parameter φ ∈ [0,1] p(X = 1) = φ, p(X = 0) = 1 − φ or p(X = x) = φx(1 − φ)x x ∈ {0,1}, EX[X] = φ VarX[X] = φ(1 − φ).

2) Muttinoulli (Categorical) Range(X) = {0,1,··· ,k}

p(X = i) = pi

k (cid:80) i=1

pi = 1 only k − 1 parameters.

3) Gaussian (Normal) (cid:113) 1 N(x;µ,σ2) = E(X) = µ, β = σ−2=precision.

2πσ2 exp(− 1 Var[X] = σ2 > 0

2σ2(x − µ)2)

4) ..., continue after the importance of Gaussian.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Why are Gaussian important

(1) Central Limit Theorem (many versions)

{Xi}n Sn = 1 ⇒ Sn → EX1 = µ as n → ∞) stronger result : distribution σ2 = Var[X1]. Pr[ lim n→∞ N(0,1).

i=1 i.i.d. (independent and identically distributed), n(X1 + ··· + Xn). (Law of Large numbers

√

n(Sn − µ) ⇒ N(0,σ2) as n → ∞),in

√

n(Sn − µ) ≤ z] = Φ( z

σ), where Φ(x) is c.d.f of

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Why are Gaussian important

(2) Information theory : Gaussian encodes the maximum amout of

uncertainty H[x] = Ex∼p[−logp(x)] = −(cid:82) p(x)logp(x)dx. max p

E[X] s.t.

(cid:113) 1

(cid:82) p(x)dx = 1 ⇒ p∗(x) = (cid:82) xp(x)dx = µ (cid:82) (x − µ)2p(x)dx = σ2,

2πσ2 exp(− 1

2σ2(x − µ)2)

λ1 λ2 λ3 p(x) = exp{λ1 − 1 + λ2(x − µ) + λ3(x − µ)3}, λ2 = 0, λ1 : nomalization constant, λ3 = −1/(2σ2).

Multivariate normal : (cid:113) 1 (cid:42) N((cid:42)x; (cid:42)µ, Σ) = Σ−1 = Ω precision matrix. E(cid:42)x = (cid:42)µ Cov((cid:42)x) = Σ,

(2π)n det(Σ) exp(−1

2((cid:42)x − (cid:42)µ)TΣ−1((cid:42)x − (cid:42)µ))

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Common Prob Distributions (Continued)

4) Exponential and Laplace (Doubly exponential) exponential : p(x;λ) = λ⊥x≥0 exp(−λx) Laplace (x;µ,r) = 1 5) Dirac and Empirical Dirac : p(x) = δ(x − µ) which puts prob 1 (samples), i.e. P(X = x(i)) = 1

m on each of x(i).

m, i = 1,··· ,m.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Common Prob Distributions (Continued)

6) Mixture :

P(x) = (cid:80)P(c = i)P(x|c = i), where P(c) is multimoulli. Empirical is mixture with Dirac. Latent variable c, P(X|c) relates the latent variable c to the visible variables X. e.g. Gaussian Mixtures P(x|c = i) is Gaussian for ∀i, ∼ N(x;µ(i),Σ(i)). P(c = i) is prior prob. P(c|x) is posterior prob. Gaussian mixture model is universal approximator of density in the sense that any smooth density can be approximated with any speciﬁc non-zero amount of error by a Gaussian mixture model with enough components.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Bayes’ Rule

P(x|y) = P(x)P(y|x) P(y) = (cid:80) x

i.e P(x|y)P(y) = P(x,y) = P(x)P(y|x)

P(y)

P(x)P(y|x)

P(x) : prior; P(x|y) : posterior

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Transformations

X,Y two r.v.s. Y = g(X), g is invertible p.d.f. of X is Px, p.d.f. of Y is Py, then Pk((cid:42)y) = Px(g−1((cid:42)y))

or Pk((cid:42)x) = Py(g−1((cid:42)x)) Let J = (∂yi ∂xj

)i,j.

(cid:12) (cid:12) (cid:12)

∂(cid:42)y ∂(cid:42)x

(cid:12) (cid:12) = Py(g((cid:42)x))|detJ|. (cid:12)

(cid:12) (cid:12) (cid:12)

References

∂(cid:42)x ∂(cid:42)y

(cid:12) (cid:12) (cid:12)

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Information Theory

information measures the amount of uncertainty :

(1) Likely events have low information

(2) Less likely events have higher information

(3) Information events have additive information. (toss a coin

twice)

Self-information at event X = x, I(x) ∆= −logP(x), other logarithms (base-2) is called bits or shannons. Shannon entropy : H(X) = Ex∼p[I(X)] = −Ex∼p[logP(x)]. It gives a lower bound on the number of bits need on average to encode symbols drawn from P. If X is continuous, H(P) is diﬀerential entropy. e.g. Discrete uniform distribution maximite discrete entropy within the distributions having the same number of states

max {pi}

k (cid:80) i=1

−pi logpi = E logp, s.t.

k (cid:80) i=1

pi = 1. ⇒ pi = 1 k.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Examples of Entropy

If the sun rises in the East there is no information content, the sun always rises in the East.

If you toss an unbiased coin then there is information in whether it lands heads or tails up. If the coin is biased towards heads then there is more information if it lands tails.

Surprisal associated with an event is the negative of the logarithm of the probability of the event −log2(p).

People use diﬀerent bases for the logarithm, but it doesn’t make much diﬀerence, it only makes a scaling diﬀerence. But if you use base 2 then the units are the familiar bits. If the event is certain,so that p = 1, the information associated with it is zero. The lower the probability of an event the higher the surprise, becoming inﬁnity when the event is impossible.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Examples of Entropy

But why logarithms? The logarithm function occurs naturally in information theory. Consider for example the tossing of four coins. There are 16 possible states for the coins, HHHH, HHHT, ..., TTTT. But only four bits of information are needed to describe the state. HTHH could be represented by 0100.

4 = log2(16) = −log2(1/16).

Going back to the biased coin, suppose that the probability of tossing heads is 3/4 and 1/4 of tossing tails. If I toss heads then that was almost expected, there’s not that much information. Technically it’s −log2(0.75) = 0.415 bits. But if I toss tails then it is −log2(0.25) = 2 bits.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Examples of Entropy

This leads naturally to looking at the average information, this is our entropy :

−

(cid:88)

p log2(p),

where the sum is taken over all possible outcomes.

(Note that when there are only two possible outcomes the formula for entropy must be the same when p is replaced by 1 − p. And this is true here.)

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Cross Entropy

Suppose you have a model for the probability of discrete events, call this pM k where the index k just means one of K possibilities. The sum of these probabilities must obviously be one.

And suppose that you have some empirical data for the probabilities of those events, pE

k. With the sum again being one.

The cross entropy is deﬁned as

−

(cid:88)

pE k ln(pM

k ).

k

It is a measure of how far apart the two distributions are.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Example of Cross Entropy

Suppose that you have a machine-learning algorithm that is meant to tell you whether a fruit is a passion fruit, orange or guava.

As a test you input the features for an orange. And the algorithm is going to output three numbers, perhaps thanks to the softmax function, which can be interpreted as the probabilities of the fruit in question (the orange) being one of P,O or G. Will it correctly identify it as an orange?

The model probabilities come out of the algorithm as

P = 0.13, pM pM

O = 0.69, and pM

G = 0.18.

Ok, it’s done quite well. It thinks the fruit is most likely to be an orange. But it wasn’t 100% sure.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Example of Cross Entropy

Empirically we know that

P = 0, pE pE

O = 1, and pE

G = 0,

because it deﬁnitely is an orange. The cross entropy is thus

−(0 × ln(0.13) + 1 × ln(0.69) + 0 × ln(0.18)) = 0.371.

The cross entropy is minimized when the model probabilities are the same as the empirical probabilities.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Example of Cross Entropy

To see this we can use Lagrange multipliers. Write

L = −

(cid:88)

k ln(pM pE

k ) − λ

(cid:32)

(cid:88)

pM k − 1

(cid:33)

.

k

k

The second term on the right is needed because the sum of the model probabilities is constrained to be one.

Now diﬀerentiate with respect to each model probability, and set the results to zero :

∂L ∂pM k

= −

pE k pM k

− λ = 0.

But since the sums of the two probabilities must be one we ﬁnd that λ = −1 and pM

k = pE k.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Example of Cross Entropy

Because the cross entropy is minimized when the model probabilities are the same as the empirical probabilities we can see that cross entropy is a candidate for a useful cost function when you have a classiﬁcation problem.

If you take another look at the sections on MLE and on cost functions, and compare with the above on entropy you’ll ﬁnd a great deal of overlap and similarities in the mathematics. The same ideas keep coming back in diﬀerent guises and with diﬀerent justiﬁcations and uses.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Kullback-Leibler (KL) divergence

Two distributions P(x),Q(x)

H(P,Q) = −Ex∼p logQ(x) cross-entropy

DKL(P||Q) = Ex∼p[log P(X) information “diatance”.

Q(X)] = −H(P) + H(P,Q) Extra

Properties : DKL(P||Q) = 0 iﬀ P = Q a.e. DkL(P||Q) ≥ 0 But DkL(P||Q) (cid:54)= DkL(Q||P), 0log0 = lim x→0

x logx = 0.

References

Linear Algebra

Probability and Information Theory

Outlines

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

References

Statistics and Machine Learning

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Tasks of Machine Learning

(1) Classiﬁcation : ﬁnd f : Rn → {1,··· ,k} s.t. y ≈ f (x). (2) Regression : ﬁnd f : Rn → R, s.t. y ≈ f (x). (3) Anomaly Detection : ﬁnd P(x) for normal samples, predict

abnormal samplesˆ˜x with small prob P(˜x) < ε. (4) Imputation of missing values : predict P(x) for

x = (x1,··· ,xn) then insert the value of xi for a new sample (cid:42)x with xi missing.

(5) Denoising : Given a corrupted example ˜x ∈ Rn, ﬁnd a clean example ˜x ∈ Rn s.t. x ≈ ˜x with some noise, i.e. ﬁnd P(x|˜x).

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Tasks of Machine Learning

(6) Density Estimation or PMF estimation :

ﬁnd Pmodel :Rn → R, Pmodel(x) for given samples (cid:42)x. All previous problems, and clustering dimensionality reduction etc, could fall into this category. This is rather diﬃcult, computationally intractable. supertrained learning : P(y|x) = P(x,y) P(x,y(cid:48))

.

(cid:80) (cid:48)

y

Learning conditional statistics (e.g. expectation). given a measure of diviation (loss function). L(y,f ) want to ﬁnd the best f ∗ that minimize it i.e. min If L = (cid:107)y − f (cid:107)2 If L = (cid:107)y − f (cid:107)1, then f ∗(x) = conditional median.

Ex,y∼PdataL(y,f (x)).

f 2, then f ∗(x) = Ey∼Pdata(y|x)[y].

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Linear Regression

Pdata(x,y) = 1 m

m (cid:80) i=1

δ(x − x(i),y − y(i))

f (x) = wTx,

m (cid:80) i=1

(cid:13)y(i) − wTx(i)(cid:13) (cid:13) 2 (cid:13) 2

Ex,y∼Pdata(x,y) (cid:107)y − f (x)(cid:107)2

1 m

2 ≈ min w

min f =wTx Pdata(x,y) ≈ Ptrain(x,y) ∇wMSEtrain = 0 ⇔ w = (X(train)TX(train))−1X(train)Ty(train)

Goals : 1) Make MSEtrain small (under ﬁtting if this is not achieved) 2) Make MSEtrain − MSEtest small (over ﬁtting if this is not

achieved)

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Linear Regression

e.g. y = b +

m (cid:80) i=1

wixi

m = 9, overﬁtting m = 1, underﬁtting m = 3, optimal

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Linear Regression

Performance : MSEtest = 1

m(test)

(cid:13)X(test)w − y(test)(cid:13) (cid:13) 2 2. (cid:13)

If (X(train),y(train)) and (X(test),y(test)) ∼ Pdata(x,y), then MSEtrain = MSEtest for ˆw computed from 1).

However, in general, MSEtrain ≤ MSEtest, since ˆw is computed from 2).

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Regularizaition

Insteat of minimizing MSEtrain, we take into account the model complexity, in the case of linear regression, λ(cid:107)w(cid:107)2 2, J(w) = MSEtrain + λ(cid:107)w(cid:107)2

e, λ → 0 over ﬁtting.

λ → +∞, underﬁtting, optimal λ ∈ (0,+∞).

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Statistical Estimators

Point Estimation : estimate Q in f (x;Q) by ˆQ.

ˆQ = argmin Q

Ex,y∼Pdata(x,y)L(y,f (x,Q))

= argmin

Q

1 m

m (cid:88)

i=1

L(y(i),f (x(i);Q))

⇒ ˆQm = g((x(i),y(n)) ∼ (x(m),y(m));Q)

Function Estimation : nonparameterized f (x) in some functional space H, then the least square rule gives ˆf as the best approximation of f among H.–point estimation in H. bais(ˆQm) = E[ˆQm] − Q ˆQ is unbiased if bais(ˆQm) = 0. lim asymptotically unbiased if m→∞

bais(ˆQm) = 0.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Statistical Estimators

e.g. Bernoulli, {x(1),··· ,x(m)}, P(x(i);Q) = Qx(i)(1 − Q)1−x(i). Let ˆQm = 1 m

m (cid:80) i=1 e.g. Gaussian, {x(1),··· ,x(m)}, P(x(i)) = N(x(i);µ,σ2) = 1√ 2σ m (cid:80) i=1

m (cid:80) i=1

x(i), E[ˆQm] = 1 m

E[x(i)] = 0, unbiased.

2

exp(−(x(i)−µ) 2σ2

)

m (cid:80) i=1 m (cid:80) m = 1 ˆσ2 m i=1 m] = m−1 E[ˆσ2 m = 1 m−1

E[x(i)] = µ. unbiased

x(i), E[ˆµm] = 1 m

ˆµm = 1 m

2 (x(i) − ˆµm)

sample variances

m σ2 (cid:54)= σ2 biased (x(i) − ˆµm)

m (cid:80) i=1

2

but ˆσ2

= m−1

m ˆσ2

m is unbiased.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Variance and Standard Error

ˆQm :estimator.

(cid:113)

Var(ˆQm) =standard error = SE(ˆQm)

e.g. SE(ˆµm) =

(cid:115)

Var[ 1 m

m (cid:80) i=1

x(i)] = σ√ m

CLT ⇒ ˆµm ∼ N(µ,SE2(ˆµm)) Conﬁdence interval : (ˆµm − 1.96SE(ˆµm), ˆµm + 1.96SE(ˆµm)). e.g. Bernoulli : Var(ˆQm) = 1 m2

m (cid:80) i=1

Var(x(i)) = 1

mQ(1 − Q),

√

Q(1−Q) m

SE(ˆQm) = Bias-variance Tradeoﬀ MSE = E[(ˆQm − Q)2] = Bais(ˆQm)2 + Var(ˆQm) = E[(ˆQm − E ˆQm)2 + 2(ˆQm − E ˆQm)(E ˆQm − Q) + (E ˆQm − Q)2]

decreasing function of m.

√

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Variance and Standard Error

Underﬁtting : High bias, low variance Overﬁtting : Low bias, high variance Consistency : ˆQm is a consistent estimator if ˆQm i.e. P(ˆQm − Q|>ε) → 0 (m → ∞) for ∀ε > 0. Consistency⇒ Asymptotic unbiasedness. A counter-example for the reverse statement : x(i) ∼ N(x;µ,σ2), ˆµ = x(1), then E[ˆµ] = E[x(1)] = µ. But ˆµ does not trends to µ as m → ∞.

P→Q (m → ∞).

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Maximum Likelihood Estimation (MLE)

Usually used in parametric density estimation. Pdata(x) ≈ Pmodel(x;Q), {x(i)}m Assume ∃Q, as if x(i) ∼ Pmodel(x;Q). MLE for Q is

i=1 drawed i.i.d from Pdata(x).

QML = argmax

Q

Pmodel(X;Q) = argmax

Q

m (cid:81) i=1

Pmodel(x(i);Q)

or

m (cid:81) Q i=1 EX∼ˆPdata

QML = argmax

= argmax

Q

logPmodel(x(i);Q)

logPmodel(X;Q)

Property : QML minimizes KL divergence (dissimilarity) between ˆPdata and Pmodel DKL(ˆPdata||Pmodel) = EX∼ˆPdata min Q

[log ˆPdata(X) − logPmodel(X;Q)]

EX∼ˆPdata

DKL ⇔ min Q

[−logPmodel(X;Q)]

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Examples of MLE

Maximum Likelihood Estimation (MLE) is a common method for estimating parameters in a statistical/probabilistic model.

In words, you simply ﬁnd the parameter (or parameters) that maximizes the likelihood of observing what actually happened.

Let’s see this in a few classical examples.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Examples of MLE

Example : Taxi numbers

You arrive at the train station in a city you’ve never been to before. You go to the taxi rank so as to get to your ﬁnal destination. There is one taxi, you take it. While discussing European politics with the taxi driver you notice that the cab’s number is 1234. How many taxis are in that city?

To answer this we need some assumptions. Taxi numbers are positive integers, starting at 1, no gaps and no repeats. We’ll need to assume that we are equally likely to get into any cab. And then we introduce the parameter N as the number of taxis.

What is the MLE for N ?

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Examples of MLE

Example : Taxi numbers

What is the MLE for N ?

Well, what is the probability of getting into taxi number 1234 when there are N taxis?

It is

1 N

for N ≥ 1234 and zero otherwise.

What value of N maximizes this expression? Obviously it is N = 1234. That is the MLE for the parameter N. It looks a bit disturbing because it seems a coincidence that you happened to get into the cab with the highest number. But then the probability of getting into any cab is equally likely. It is also disturbing that if there are N taxis then the average cab number is (N + 1)/2, and we somehow feel that should play a role.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Examples of MLE

Example : Coin tossing

Suppose you toss a coin n times and get h heads. What is the probability, p, of tossing a head next time?

The probability of getting h heads from n tosses is, assuming that the tosses are independent,

n! h!(n − h)!

ph(1 − p)n−h =

(cid:18)n h

(cid:19)

ph(1 − p)n−h.

Applying MLE is the same as maximizing this expression with respect to p.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Examples of MLE

Example : Coin tossing

This likelihood function (without the coeﬃcient in the front that is independent of p) is shown below for n = 100 and h = 55. There is a very obvious maximum.

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Examples of MLE

Example : Coin tossing

Often with MLE when multiplying probabilities, as here, you will take the logarithm of the likelihood and maximize that.

This doesn’t change the maximizing value but it does stop you from having to multiply many small numbers, which is going to be problematic with ﬁnite precision.

(Look at the scale of the numbers on the vertical axis in the ﬁgure.)

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Examples of MLE

Example : Coin tossing

Since the ﬁrst part of this expression is independent of p we maximize hlnp + (n − h)ln(1 − p) with respect to p. See below.

This just means diﬀerentiating with respect to p and setting the

derivative equal to zero. This results in p = eminently reasonable.

h n

, which seems

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Conditional Log-likelihood

MLE for Q in P(Y|X;Q)

QML = argmax

Q

P(Y|X;Q) = argmax

Q

m (cid:89)

i=1

P(y(i)|x(i);Q)

e.g. linear Regression : y = wTx + ε, 1√ Then P(y|x;w) ∼ N(y;wTx,σ2),

2πσ

ε ∼ N(0,σ2). exp(−(y−wTx)2

2σ2

).

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Conditional Log-likelihood

MLE ⇔max

w

=max w

m (cid:88)

i=1 m (cid:88)

i=1

logP(y(i)|x(i);w)

(−logσ −

1 2

log(2π) −

(y(i) − wTx(i)) 2σ2

2

= − mlogσ −

m 2

log(2π) − min

w

m (cid:88)

i=1

2 (y(i) − wTx(i))

⇔min

w

1 m

m (cid:88)

i=1

2 (y(i) − wTx(i))

= min w

MSEtrain

)

References

/σ2

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Properties of MLE

Under certain conditions (given below). MLE is a consistent estimator of the truth. (1) Pdata lies in {Pmodel(;Q);Q}; otherwise, no estimator can

recover Pdata.

(2) ∃Q, s.t. Pdata = Pmodel(;Q); otherwise, MLE can recover

Pdata, but not be able to determine Q.

EX∼Pdata[ˆQML − Q]2 (cid:38) Cramer-Rao bowd as m → ∞. But MLE is not always unbiased, e.g. ˆσ2

m (cid:80) i=1

ML = 1 m

(x(i) − ˆµ)

2

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Bayesian Statistics

Previously, Q is ﬁxed but unknown. ˆQ is a random variable as a function of data {x(i)}m i=1 (x(i) is random). Bayesian : {x(i)}m and uncertain (random). Prior prob distribution P(Q), before observing the data. Given Q, {x(i)}m i=1 is generated from P(x(1),··· ,x(m)|Q). Bayes’ rule ⇒ P(Q|x(1),··· ,x(m)) = P(x(1),···,x(m)|Q)P(Q) P(Q) is usually given e.g. uniform or Gaussian with high entropy, observation of data causes the posterior to loose entropy and concentrate around a few highly likely values of parameters. Bayesian estimates the distribution of Q instead of point estimate.

i=1 is observed and non-random. Q is unknown

.

P(x(1),···,x(m))

P ∈ X(m+1)|x(1),··· ,x(m)) =

(cid:90)

P(x(m+1)|Q)P(Q|x(1),··· ,x(m))dQ

As more observations are given, knowledge about Q becomes diﬀerent (more).

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Bayesian Statistics

uncertainty

Point Estimate variance of estimator through random sampling of data

Bayesian

distribution, integral over it

prior info

performance

computational cost

No

good as sample size increases low

Yes with human knowledge generalize better for limited training data high

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

References

Bayesian Statistics

e.g. Bayesian Linear Regression ˆy = wTx. Given (X(train),y(train)), P(y(train)|X(train), w) = N(y(train); X(train), w, I) exp(− 1 2 (y(train) − X(train)w)T (y(train) − X(train)w)) 2(w − µ0)TΛ−1 Prior of w : P(w) = N(w;µ0,Λ0)exp(−1 Posterior : P(w|X(train),y(train))P(y(∗)|X(train),w)P(w) exp(−1 exp(−1 exp(−1 exp(−1 terms without w are normalizing constant. If µ0 = 0, Λ0 = 1 regression estimator of w. also gives the variance Λm = (XTX + αI)−1

ˆy(train) = X(train)w

0 (w − µ0))

2(w − µ0)TΛ−1 0 w − 2µT mΛ−1 m µm)

2(y − Xw)T(y − Xw))exp(−1 2(−2yTXw + wTXTXw + wTΛ−1 2(w − µm)TΛ−1 2(w − µm)TΛ−1

0 (w − µ0)) 0 Λ−1 0 w))

2µT

m (w − µm) + 1 m (w − µm))

αI, µm = (XTX + αI)−1XTy is the ridge

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

Maximum A Posterior (MAP) similar to MLE

e.g. µm is MAP in the previous example. MAP choose the point of maximum posterior prob.

QMAP = argmax

Q

P(Q|X) = argmax

Q

{logP(X|Q) + logP(Q)}

Bayesian linear regression, log−prior (cid:107)w(cid:107)2 Additional information in prior helps to reduce the variance in the MAP, but does not increase bias. Diﬀerent regularizations (penalties) corresponds to diﬀerent log-prior. (but not all, smoe penalty may not be a logarithm of a prob distribution, some others depend on data) Lsso Penalty → Laplace distribution Laplace(x;0,λ−1).

2 ridge penalties.

References

Linear Algebra

Probability and Information Theory

Outlines

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

References

Statistics and Machine Learning

References

Linear Algebra

Probability and Information Theory

Statistics and Machine Learning

References

Numerical Analysis, 9th Edition, by Richard L. Burden, J. Douglas Faires, Brooks/Cole, 2011.

Machine learning : an applied mathematics introduction, by Wilmott, Paul. Panda Ohana Publishing, 2019.

Deep learning, by Ian Goodfellow, Yoshua Bengio, and Aaron Courville, MIT Press, 2016.

References

