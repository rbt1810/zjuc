import json
import os
import logging
import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel
from typing import List, Dict, Optional, Any

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler()]
)
logger = logging.getLogger("MinimalVectorizer")

class MinimalVectorizer:
    """æç®€ç‰ˆæ–‡æœ¬å‘é‡åŒ–å™¨ï¼Œé¿å…ä»»ä½•å¤æ‚ä¾èµ–"""
    
    def __init__(self, model_path: str, device: Optional[str] = None):
        """
        åˆå§‹åŒ–å‘é‡åŒ–å™¨
        
        å‚æ•°:
            model_path: æ¨¡å‹è·¯å¾„
            device: æŒ‡å®šè®¡ç®—è®¾å¤‡ (å¦‚'cpu'æˆ–'cuda')
        """
        logger.info("åˆå§‹åŒ–æç®€ç‰ˆå‘é‡åŒ–å™¨...")
        
        # éªŒè¯æ¨¡å‹è·¯å¾„
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        
        # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶
        required_files = ["config.json", "tokenizer_config.json"]
        weight_files = ["pytorch_model.bin", "model.safetensors"]
        
        for file in required_files:
            if not os.path.exists(os.path.join(model_path, file)):
                logger.warning(f"æ³¨æ„: ç¼ºå°‘æ–‡ä»¶ {file}ï¼Œä½†å°è¯•ç»§ç»­åŠ è½½")
        
        # è®¾ç½®è®¾å¤‡
        if device is None:
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        self.device = torch.device(device)
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½tokenizerå’Œæ¨¡å‹
        logger.info("åŠ è½½åˆ†è¯å™¨...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        logger.info("åŠ è½½æ¨¡å‹...")
        self.model = AutoModel.from_pretrained(model_path).to(self.device)
        
        # è·å–å‘é‡ç»´åº¦
        self.vector_dim = self.model.config.hidden_size
        logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ! å‘é‡ç»´åº¦: {self.vector_dim}")
    
    def vectorize(self, texts: List[str], batch_size: int = 8) -> np.ndarray:
        """å‘é‡åŒ–æ–‡æœ¬"""
        self.model.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        all_embeddings = []
        
        with torch.no_grad():
            for i in range(0, len(texts), batch_size):
                batch = texts[i:i + batch_size]
                
                # åˆ†è¯
                inputs = self.tokenizer(
                    batch,
                    padding=True,
                    truncation=True,
                    return_tensors="pt",
                    max_length=512
                ).to(self.device)
                
                # è·å–æ¨¡å‹è¾“å‡º
                outputs = self.model(**inputs)
                
                # ä½¿ç”¨å¹³å‡æ± åŒ–è·å–å¥å‘é‡
                embeddings = self.mean_pooling(outputs, inputs['attention_mask'])
                
                # å½’ä¸€åŒ–
                embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
                
                all_embeddings.append(embeddings.cpu().numpy())
                
                # è®°å½•è¿›åº¦
                progress = min(i + batch_size, len(texts))
                logger.info(f"å·²å¤„ç†: {progress}/{len(texts)} ({progress/len(texts)*100:.1f}%)")
        
        return np.vstack(all_embeddings)
    
    def mean_pooling(self, model_output, attention_mask):
        """å®ç°å¹³å‡æ± åŒ–ç­–ç•¥"""
        token_embeddings = model_output.last_hidden_state
        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()
        return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)

class JSONVectorizer:
    """JSONæ–‡æ¡£å‘é‡åŒ–å¤„ç†å™¨ - æç®€ç‰ˆ"""
    
    def __init__(self, model_path: str, device: Optional[str] = None):
        self.vectorizer = MinimalVectorizer(model_path, device)
        self.document = None
        self.text_blocks = []
        self.vectors = None
    
    def load_json(self, file_path: str):
        """åŠ è½½JSONæ–‡æ¡£"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        with open(file_path, 'r', encoding='utf-8') as f:
            self.document = json.load(f)
        
        title = self.document.get('metadata', {}).get('document_title', 'æœªçŸ¥æ–‡æ¡£')
        logger.info(f"âœ… å·²åŠ è½½æ–‡æ¡£: {title}")
    
    def extract_text_blocks(self):
        """æå–æ–‡æœ¬å— - ç®€åŒ–ç‰ˆæœ¬"""
        if self.document is None:
            raise ValueError("å°šæœªåŠ è½½æ–‡æ¡£")
        
        text_blocks = []
        
        # éå†æ‰€æœ‰é¡µé¢
        for page_idx, page in enumerate(self.document.get('pages', [])):
            # æå–é¡µé¢å†…çš„æ–‡æœ¬å†…å®¹
            for content_block in page.get('content', []):
                block_text = []
                
                # æ·»åŠ æ ‡é¢˜å’Œæ‘˜è¦
                if header := content_block.get('header'):
                    block_text.append(f"æ ‡é¢˜: {header}")
                if summary := content_block.get('summary'):
                    block_text.append(f"æ‘˜è¦: {summary}")
                
                # æå–å†…å®¹æ•°ç»„ä¸­çš„æ–‡æœ¬
                for element in content_block.get('content_array', []):
                    if element.get('type') == 'text' and element.get('value'):
                        block_text.append(element['value'])
                
                if not block_text:
                    continue
                    
                full_text = ". ".join(block_text)
                text_blocks.append({
                    "id": f"block_{page_idx}_{len(text_blocks)}",
                    "text": full_text,
                    "page_idx": page_idx
                })
        
        self.text_blocks = text_blocks
        logger.info(f"ğŸ“Š æå–åˆ° {len(text_blocks)} ä¸ªæ–‡æœ¬å—")
        return text_blocks
    
    def vectorize_blocks(self, batch_size: int = 4):
        """å‘é‡åŒ–æ–‡æœ¬å—"""
        if not self.text_blocks:
            self.extract_text_blocks()
            
        texts = [block["text"] for block in self.text_blocks]
        logger.info(f"ğŸš€ å¼€å§‹å‘é‡åŒ– {len(texts)} ä¸ªæ–‡æœ¬å—...")
        
        # å‘é‡åŒ–ï¼ˆå¸¦GPUå›é€€é€»è¾‘ï¼‰
        try:
            self.vectors = self.vectorizer.vectorize(texts, batch_size)
        except RuntimeError as e:
            if 'CUDA out of memory' in str(e):
                logger.warning("âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œåˆ‡æ¢åˆ°CPUæ¨¡å¼...")
                self.vectorizer.device = torch.device('cpu')
                self.vectorizer.model = self.vectorizer.model.to('cpu')
                self.vectors = self.vectorizer.vectorize(texts, batch_size)
            else:
                raise
        
        logger.info(f"âœ… å‘é‡åŒ–å®Œæˆ! å‘é‡ç»´åº¦: {self.vectors.shape[1]}")
        return self.vectors
    
    def save_results(self, output_file: str):
        """ä¿å­˜ç»“æœ"""
        if self.vectors is None or not self.text_blocks:
            raise ValueError("è¯·å…ˆæ‰§è¡Œvectorize_blocks()")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        
        results = {
            "document_title": self.document.get('metadata', {}).get('document_title', ''),
            "blocks": []
        }
        
        for i, block in enumerate(self.text_blocks):
            results["blocks"].append({
                "id": block["id"],
                "page": block["page_idx"],
                "text": block["text"][:500] + "..." if len(block["text"]) > 500 else block["text"],
                "vector": self.vectors[i].tolist()
            })
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        
        logger.info(f"ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {output_file}")
    
    def process_document(self, json_path: str, output_path: str):
        """å®Œæ•´å¤„ç†æµç¨‹"""
        self.load_json(json_path)
        self.extract_text_blocks()
        self.vectorize_blocks()
        self.save_results(output_path)
        return self

def main():
    """ä¸»å‡½æ•°"""
    # è·å–å½“å‰è„šæœ¬è·¯å¾„
    script_dir = os.path.dirname(os.path.abspath(__file__))
    
    # é…ç½®è·¯å¾„
    model_path = os.path.join(script_dir, "models", "sentence-transformers_all-mpnet-base-v2")
    input_json = os.path.join(script_dir, "lectures", "introduction", "introduction_Mathematical_Representation.json")
    output_json = os.path.join(script_dir, "lectures", "introduction", "introduction_Mathematical_Representation_vectorized.json")
    
    print("=" * 50)
    print("æç®€ç‰ˆJSONæ–‡æ¡£å‘é‡åŒ–ç³»ç»Ÿ")
    print("=" * 50)
    print(f"æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"è¾“å…¥æ–‡ä»¶: {input_json}")
    print(f"è¾“å‡ºæ–‡ä»¶: {output_json}")
    print()
    
    try:
        processor = JSONVectorizer(model_path)
        processor.process_document(input_json, output_json)
        
        print("\nå¤„ç†æ‘˜è¦:")
        print(f"- æ–‡æ¡£æ ‡é¢˜: {processor.document['metadata']['document_title']}")
        print(f"- æ–‡æœ¬å—æ•°é‡: {len(processor.text_blocks)}")
        print(f"- å‘é‡ç»´åº¦: {processor.vectors.shape[1]}")
        
        print("\nâœ¨ å¤„ç†å®Œæˆ! âœ¨")
        
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {str(e)}")
        logger.exception("å¤„ç†å¤±è´¥")

if __name__ == "__main__":
    main()
