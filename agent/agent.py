import os
import sys
import json
import time
import numpy as np
import torch
from flask import Flask, request, Response, render_template
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import faiss
from accelerate import dispatch_model, infer_auto_device_map
from accelerate.utils import get_balanced_memory
from flask_cors import CORS

# é…ç½®å¸¸é‡
QWEN_MODEL_PATH = "/home/wangzy/zjuc/models/Qwen3-32B"  # æœ¬åœ°Qwen3-32Bæ¨¡å‹è·¯å¾„
VECTOR_DB_INDEX_PATH = "/home/wangzy/zjuc/agent/lectures/introduction/introduction_Mathematical_Representation_db/index.faiss"  # FAISSç´¢å¼•æ–‡ä»¶è·¯å¾„
VECTOR_DB_METADATA_PATH = "/home/wangzy/zjuc/agent/lectures/introduction/introduction_Mathematical_Representation_db/metadata.json"  # å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
# os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/home/wangzy/zjuc/agent/models'
VECTOR_MODEL_PATH = "./models/sentence-transformers_all-mpnet-base-v2"  # æœ¬åœ°æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹è·¯å¾„
SERVER_HOST = "0.0.0.0"
SERVER_PORT = 5000

# åˆå§‹åŒ–Flaskåº”ç”¨
app = Flask(__name__, template_folder='/home/wangzy/zjuc/agent/templates/')
CORS(app)
app.config['JSON_AS_ASCII'] = False  # ç¦ç”¨ASCIIç¼–ç ï¼Œä¿®å¤ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
start_time = time.time()

# ä¿®å¤ä¸­æ–‡Unicodeç¼–ç é—®é¢˜çš„å“åº”å‡½æ•°
def json_response(data, status=200):
    """è¿”å›æ­£ç¡®ç¼–ç çš„JSONå“åº”ï¼ˆè§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ï¼‰"""
    return Response(
        json.dumps(data, ensure_ascii=False),
        status=status,
        mimetype='application/json; charset=utf-8'
    )

# æ—¥å¿—é…ç½®
def log(message, level="INFO"):
    timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] [{level}] {message}")

# GPUå·¥å…·å‡½æ•°
def clear_gpu_memory():
    """æ¸…ç†GPUæ˜¾å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
        log("GPUæ˜¾å­˜å·²æ¸…ç†")

def get_gpu_status():
    """è·å–GPUçŠ¶æ€ä¿¡æ¯"""
    status = []
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            torch.cuda.synchronize(i)
            status.append({
                "è®¾å¤‡": f"cuda:{i}",
                "åç§°": torch.cuda.get_device_name(i),
                "å†…å­˜(GB)": {
                    "æ€»é‡": round(torch.cuda.get_device_properties(i).total_memory / (1024**3), 2),
                    "å·²åˆ†é…": round(torch.cuda.memory_allocated(i) / (1024**3), 2),
                    "å·²é¢„ç•™": round(torch.cuda.memory_reserved(i) / (1024**3), 2),
                }
            })
    return status

# æ–‡æœ¬å‘é‡åŒ–å°è£…
class TextVectorizer:
    """æœ¬åœ°æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹å°è£…"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = None
        self.load_model()
    
    def load_model(self):
        """åŠ è½½æœ¬åœ°å‘é‡åŒ–æ¨¡å‹"""
        log(f"â³ æ­£åœ¨åŠ è½½æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹: {self.model_path}")
        try:
            self.model = SentenceTransformer(self.model_path, local_files_only=True, device="cuda")
            log(f"âœ… æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹åŠ è½½æˆåŠŸ! è®¾å¤‡: {self.model.device}")
        except Exception as e:
            log(f"âŒ æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", "ERROR")
            raise
    
    def vectorize(self, text: str) -> np.ndarray:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        log(f"å‘é‡åŒ–æ–‡æœ¬: {text[:50]}...")
        if not self.model:
            raise RuntimeError("å‘é‡åŒ–æ¨¡å‹æœªåˆå§‹åŒ–")
        
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(text, str):
            text = str(text)
        
        # å‘é‡åŒ–
        vector = self.model.encode(text, convert_to_numpy=True)
        return vector.reshape(1, -1)  # è¿”å›(1, dim)å½¢çŠ¶

# å‘é‡æ•°æ®åº“å°è£…
class VectorDatabase:
    """æœ¬åœ°å‘é‡æ•°æ®åº“å°è£…"""
    
    def __init__(self, index_path: str, metadata_path: str):
        self.index_path = index_path
        self.metadata_path = metadata_path
        self.index = None
        self.metadata = None
        self.å‘é‡ç»´åº¦ = None
        
        self.åŠ è½½æ•°æ®åº“()
    
    def åŠ è½½æ•°æ®åº“(self):
        """åŠ è½½å‘é‡æ•°æ®åº“"""
        log(f"â³ æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“: {self.index_path}")
        
        # åŠ è½½FAISSç´¢å¼•
        try:
            self.index = faiss.read_index(self.index_path)
            self.å‘é‡ç»´åº¦ = self.index.d
            log(f"âœ… FAISSç´¢å¼•åŠ è½½æˆåŠŸ! ç»´åº¦: {self.å‘é‡ç»´åº¦}")
        except Exception as e:
            log(f"âŒ FAISSç´¢å¼•åŠ è½½å¤±è´¥: {str(e)}", "ERROR")
            raise
        
        # åŠ è½½å…ƒæ•°æ®
        try:
            with open(self.metadata_path, 'r', encoding='utf-8') as f:
                self.metadata = json.load(f)
            log(f"âœ… å…ƒæ•°æ®åŠ è½½æˆåŠŸ! æ•°é‡: {len(self.metadata)}")
        except Exception as e:
            log(f"âŒ å…ƒæ•°æ®åŠ è½½å¤±è´¥: {str(e)}", "ERROR")
            raise
    
    def æœç´¢(self, æŸ¥è¯¢å‘é‡: np.ndarray, k: int = 5) -> list:
        """åœ¨æ•°æ®åº“ä¸­æœç´¢ç›¸ä¼¼é¡¹"""
        log(f"æ‰§è¡Œæœç´¢: k={k}")
        # è¾“å…¥éªŒè¯
        if æŸ¥è¯¢å‘é‡.shape[1] != self.å‘é‡ç»´åº¦:
            raise ValueError(f"æŸ¥è¯¢å‘é‡ç»´åº¦é”™è¯¯ã€‚æ•°æ®åº“ç»´åº¦: {self.å‘é‡ç»´åº¦}, è¾“å…¥ç»´åº¦: {æŸ¥è¯¢å‘é‡.shape[1]}")
        
        # æ‰§è¡Œæœç´¢
        try:
            è·ç¦», ç´¢å¼• = self.index.search(æŸ¥è¯¢å‘é‡, k)
        except Exception as e:
            log(f"âŒ å‘é‡æœç´¢å¤±è´¥: {str(e)}", "ERROR")
            return []
        
        # å¤„ç†ç»“æœ
        ç»“æœ = []
        if ç´¢å¼•.size == 0:
            log("âš ï¸ æœç´¢æœªè¿”å›ä»»ä½•ç»“æœ", "WARNING")
            return ç»“æœ
            
        for i in range(len(ç´¢å¼•[0])):
            ç´¢å¼•å€¼ = ç´¢å¼•[0][i]
            if ç´¢å¼•å€¼ < 0 or ç´¢å¼•å€¼ >= len(self.metadata):
                continue
                
            å…ƒæ•°æ® = self.metadata[ç´¢å¼•å€¼]
            
            # æ„å»ºç»“æœå¯¹è±¡
            ç»“æœé¡¹ = {
                "ç›¸ä¼¼åº¦": float(è·ç¦»[0][i]),
                "æ–‡æ¡£æ ‡é¢˜": å…ƒæ•°æ®.get('doc_title', 'æ— æ ‡é¢˜'),
                "ç« èŠ‚": å…ƒæ•°æ®.get("section", "æœªçŸ¥ç« èŠ‚"),
                "å†…å®¹": å…ƒæ•°æ®.get("text", ""),
                "æºç´¢å¼•": int(ç´¢å¼•å€¼)
            }
            ç»“æœ.append(ç»“æœé¡¹)
        
        return ç»“æœ
    
    def å¥åº·æ£€æŸ¥(self) -> dict:
        """æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶å†µ"""
        return {
            "ç´¢å¼•å·²åŠ è½½": self.index is not None,
            "å…ƒæ•°æ®å·²åŠ è½½": bool(self.metadata),
            "å…ƒæ•°æ®æ•°é‡": len(self.metadata) if self.metadata else 0,
            "å‘é‡ç»´åº¦": self.å‘é‡ç»´åº¦
        }

# Qwenæ¨¡å‹å°è£…ï¼ˆæ”¯æŒå¤šGPUï¼‰
class QwenModel:
    """æœ¬åœ°Qwen3-32Bæ¨¡å‹å°è£…ï¼ˆæ”¯æŒåŒGPUï¼‰"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.tokenizer = None
        self.model = None
        self.device_map = None
        self.device_count = torch.cuda.device_count()
        
        self.åŠ è½½æ¨¡å‹()
    
    def åŠ è½½æ¨¡å‹(self):
        """åŠ è½½Qwenæ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆæ”¯æŒå¤šGPUï¼‰"""
        log(f"â³ æ­£åœ¨åŠ è½½Qwenæ¨¡å‹ ({self.device_count} GPUs)...")
        start_time = time.time()
        
        try:
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            
            # åˆ›å»ºåŸºç¡€æ¨¡å‹
            model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
                device_map=None  # å…ˆä¸åˆ†é…è®¾å¤‡
            )
            
            # å¤šGPUè®¾å¤‡æ˜ å°„
            if self.device_count > 1:
                log(f"ğŸ”€ åˆ†é…æ¨¡å‹åˆ° {self.device_count} ä¸ªGPU...")
                
                # è®¡ç®—è®¾å¤‡å†…å­˜åˆ†å¸ƒ
                max_memory = get_balanced_memory(
                    model,
                    no_split_module_classes=model._no_split_modules,
                    dtype=model.dtype
                )
                
                # åˆ›å»ºè®¾å¤‡æ˜ å°„
                self.device_map = infer_auto_device_map(
                    model,
                    max_memory=max_memory,
                    no_split_module_classes=model._no_split_modules
                )
                
                # å°†æ¨¡å‹åˆ†å‘åˆ°å¤šä¸ªè®¾å¤‡
                self.model = dispatch_model(
                    model,
                    device_map=self.device_map,
                    main_device=0  # ä¸»è®¾å¤‡ä¸ºç¬¬ä¸€ä¸ªGPU
                )
                
                log(f"âœ… æ¨¡å‹å·²åˆ†é…åˆ°è®¾å¤‡: {self.device_map}")
            else:
                # å•GPUæƒ…å†µ
                device = "cuda:0" if torch.cuda.is_available() else "cpu"
                self.model = model.to(device)
                log(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {device}")
            
            load_time = time.time() - start_time
            log(f"åŠ è½½è€—æ—¶: {load_time:.1f}ç§’")
        except Exception as e:
            log(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", "ERROR")
            raise
    
    def ç”Ÿæˆ(self, prompt: str, max_length: int = 1024, temperature: float = 0.7) -> str:
        """ä½¿ç”¨Qwenæ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
        log(f"ç”Ÿæˆå“åº”: max_length={max_length}")
        if not self.model or not self.tokenizer:
            raise RuntimeError("æ¨¡å‹æœªæ­£ç¡®åˆå§‹åŒ–")
        
        # ç¼–ç è¾“å…¥å¹¶ç§»åŠ¨åˆ°ä¸»è®¾å¤‡
        device = self.ä¸»è®¾å¤‡
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(device)
        
        # ç”Ÿæˆå“åº”
        with torch.no_grad():
            outputs = self.model.generate(
                inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_new_tokens=max_length,
                temperature=temperature,
                top_p=0.9,
                repetition_penalty=1.1,
                do_sample=True,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id
            )
        
        # è§£ç è¾“å‡º
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:], 
            skip_special_tokens=True
        )
        
        return response.strip()
    
    @property
    def ä¸»è®¾å¤‡(self):
        """è·å–ä¸»è®¾å¤‡"""
        if self.device_count > 0:
            return "cuda:0"
        return "cpu"
    
    def å¥åº·æ£€æŸ¥(self) -> dict:
        """æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶å†µ"""
        return {
            "æ¨¡å‹å·²åŠ è½½": self.model is not None,
            "åˆ†è¯å™¨å·²åŠ è½½": self.tokenizer is not None,
            "è®¾å¤‡æ•°é‡": self.device_count,
            "è®¾å¤‡æ˜ å°„": self.device_map,
            "ä¸»è®¾å¤‡": self.ä¸»è®¾å¤‡
        }

# AIåŠ©æ•™æ™ºèƒ½ä½“
class AIåŠ©æ•™:
    """AIåŠ©æ•™æ™ºèƒ½ä½“æ ¸å¿ƒç±»"""
    
    def __init__(self, å‘é‡æ•°æ®åº“: VectorDatabase, å‘é‡åŒ–å™¨: TextVectorizer, qwenæ¨¡å‹: QwenModel):
        self.å‘é‡æ•°æ®åº“ = å‘é‡æ•°æ®åº“
        self.å‘é‡åŒ–å™¨ = å‘é‡åŒ–å™¨
        self.qwenæ¨¡å‹ = qwenæ¨¡å‹
    
    def æ£€ç´¢ç›¸å…³å†…å®¹(self, æŸ¥è¯¢: str, top_k: int = 5) -> list:
        """ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³å†…å®¹"""
        # å‘é‡åŒ–æŸ¥è¯¢æ–‡æœ¬
        try:
            æŸ¥è¯¢å‘é‡ = self.å‘é‡åŒ–å™¨.vectorize(æŸ¥è¯¢)
            log(f"ğŸ”¢ æŸ¥è¯¢å‘é‡åŒ–æˆåŠŸ, ç»´åº¦: {æŸ¥è¯¢å‘é‡.shape}")
        except Exception as e:
            log(f"âŒ å‘é‡åŒ–å¤±è´¥: {str(e)}", "ERROR")
            return []
        
        # æ‰§è¡Œæœç´¢
        ç»“æœ = self.å‘é‡æ•°æ®åº“.æœç´¢(æŸ¥è¯¢å‘é‡, top_k)
        
        # æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆ
        if not ç»“æœ:
            log("âš ï¸ æœç´¢æœªè¿”å›ä»»ä½•ç»“æœ", "WARNING")
            return []
            
        log(f"ğŸ” æ‰¾åˆ° {len(ç»“æœ)} ä¸ªç›¸å…³ç‰‡æ®µ")
        return ç»“æœ
    
    def ç”Ÿæˆå“åº”(self, æŸ¥è¯¢: str, ä¸Šä¸‹æ–‡: list, max_length: int = 1024) -> str:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå“åº”"""
        # æ„å»ºæç¤º
        æç¤º = self.æ„å»ºæç¤º(æŸ¥è¯¢, ä¸Šä¸‹æ–‡)
        
        # ç”Ÿæˆå“åº”
        log(f"ğŸ§  å¼€å§‹ç”Ÿæˆå“åº”: {æŸ¥è¯¢} (max_length={max_length})")
        try:
            log("âš¡ ä½¿ç”¨Qwenç”Ÿæˆå“åº”...")
            start_time = time.time()
            å“åº” = self.qwenæ¨¡å‹.ç”Ÿæˆ(æç¤º, max_length, temperature=0.7)
            gen_time = time.time() - start_time
            log(f"âœ… å“åº”ç”Ÿæˆå®Œæˆ (è€—æ—¶: {gen_time:.2f}ç§’)")
            return å“åº”
        except Exception as e:
            log(f"âŒ å“åº”ç”Ÿæˆå¤±è´¥: {str(e)}", "ERROR")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºé”™"
    
    def æ„å»ºæç¤º(self, æŸ¥è¯¢: str, ä¸Šä¸‹æ–‡: list) -> str:
        """æ„å»ºæç¤ºæ¨¡æ¿"""
        æç¤º = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ•™ï¼Œè¯·åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”å­¦ç”Ÿçš„é—®é¢˜ã€‚\n\n"
        æç¤º += f"å­¦ç”Ÿé—®é¢˜: {æŸ¥è¯¢}\n\n"
        æç¤º += "ç›¸å…³å‚è€ƒèµ„æ–™:\n"
        
        for i, é¡¹ in enumerate(ä¸Šä¸‹æ–‡):
            æç¤º += f"[å‚è€ƒèµ„æ–™ {i+1}]\n"
            æç¤º += f"æ ‡é¢˜: {é¡¹['æ–‡æ¡£æ ‡é¢˜']}\n"
            æç¤º += f"ç« èŠ‚: {é¡¹['ç« èŠ‚']}\n"
            æç¤º += f"å†…å®¹: {é¡¹['å†…å®¹'][:300]}...\n\n"
        
        æç¤º += "\nè¯·æ ¹æ®ä»¥ä¸Šèµ„æ–™ï¼Œç”¨ä¸“ä¸šã€ç®€æ´çš„è¯­è¨€å›ç­”å­¦ç”Ÿé—®é¢˜ã€‚"
        return æç¤º

    def respond_to_query(self, query: str, max_length=1000, top_k=5) -> dict:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢å¹¶ç”Ÿæˆå“åº”"""
        # 1. æ£€ç´¢ç›¸å…³å†…å®¹
        relevant_content = self.æ£€ç´¢ç›¸å…³å†…å®¹(query, top_k)
        
        # 2. ç”ŸæˆAIåŠ©æ•™å›ç­”
        ai_response = self.ç”Ÿæˆå“åº”(query, relevant_content, max_length)
        
        # 3. æ„å»ºå“åº”ç»“æ„
        return {
            "query": query,
            "response": ai_response,
            "sources": [
                {
                    "doc_title": item.get("æ–‡æ¡£æ ‡é¢˜", "æ— æ ‡é¢˜"),
                    "section": item.get("ç« èŠ‚", "æœªçŸ¥ç« èŠ‚"),
                    "page": item.get("page_idx", 0),
                    "similarity": item.get("ç›¸ä¼¼åº¦", 0.0),
                    "text_excerpt": item.get("å†…å®¹", "")[:300]
                }
                for item in relevant_content
            ]
        }

# å…¨å±€æ™ºèƒ½ä½“å®ä¾‹
åŠ©æ•™ = None

@app.route('/')
def home():
    """æä¾›å‰ç«¯ç•Œé¢"""
    return render_template('agent.html')

# åœ¨æé—®ç«¯ç‚¹æ·»åŠ å‚æ•°æ”¯æŒ

@app.route('/ask', methods=['POST'])
def handle_ask():
    try:
        # è·å–JSONæ•°æ®
        data = request.get_json()
        if not data or 'query' not in data:
            return json_response({"error": "ç¼ºå°‘æŸ¥è¯¢å‚æ•°"}, 400)
        
        # æå–å‚æ•°
        question = data['query']
        max_length = data.get('max_length', 1000)
        top_k = data.get('top_k', 5)
        
        # ç¡®ä¿åŠ©æ•™å®ä¾‹å·²åˆå§‹åŒ–
        global åŠ©æ•™
        if not åŠ©æ•™:
            return json_response({"error": "AIåŠ©æ•™æœªåˆå§‹åŒ–"}, 500)
        
        # ä½¿ç”¨AIåŠ©æ•™ç”Ÿæˆå“åº”
        response_data = åŠ©æ•™.respond_to_query(question, max_length, top_k)
        
        return json_response(response_data)
    except Exception as e:
        log(f"å¤„ç†æé—®æ—¶å‡ºé”™: {str(e)}\n{traceback.format_exc()}", "ERROR")
        return json_response({"error": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯"}, 500)

# def _build_cors_preflight_response():
#     response = Response()
#     response.headers.add("Access-Control-Allow-Origin", "*")
#     response.headers.add("Access-Control-Allow-Headers", "Content-Type")
#     response.headers.add("Access-Control-Allow-Methods", "POST")
#     return response

# åœ¨å¥åº·æ£€æŸ¥ç«¯ç‚¹æ·»åŠ æ›´å¤šçŠ¶æ€ä¿¡æ¯
@app.route('/health')
def health_check():
    try:
        # è·å–æ¨¡å‹ä¿¡æ¯
        model_info = {
            "model_loaded": assistant.qwen is not None,
            "model_device": assistant.qwen.device if assistant.qwen else None,
            "model_params": str(assistant.qwen.model.config) if assistant.qwen else None
        }
        
        # è·å–å‘é‡æ•°æ®åº“ä¿¡æ¯
        vector_db_info = {
            "vector_db_loaded": assistant.vector_db is not None,
            "vector_count": len(assistant.vector_db.metadata) if assistant.vector_db else 0
        }
        
        return json_response({
            "status": "healthy",
            "version": "1.0.0",
            "uptime": f"{time.time() - start_time:.2f} seconds",
            "model": model_info,
            "vector_db": vector_db_info
        })
    except Exception as e:
        return json_response({
            "status": "error",
            "message": str(e)
        }, 500)

@app.route('/gpu-status', methods=['GET'])
def gpuçŠ¶æ€():
    """GPUçŠ¶æ€ç«¯ç‚¹"""
    return json_response(get_gpu_status())

# åˆå§‹åŒ–å‡½æ•°
def åˆå§‹åŒ–åŠ©æ•™():
    """åˆå§‹åŒ–AIåŠ©æ•™æ™ºèƒ½ä½“"""
    global åŠ©æ•™
    
    try:
        # æ¸…ç†æ˜¾å­˜
        clear_gpu_memory()
        
        # åˆå§‹åŒ–ç»„ä»¶
        log("ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹...")
        å‘é‡åŒ–å™¨ = TextVectorizer(VECTOR_MODEL_PATH)
        
        log("ğŸš€ æ­£åœ¨åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
        å‘é‡æ•°æ®åº“ = VectorDatabase(VECTOR_DB_INDEX_PATH, VECTOR_DB_METADATA_PATH)
        
        log("ğŸš€ æ­£åœ¨åˆå§‹åŒ–Qwen3-32Bæ¨¡å‹...")
        qwenæ¨¡å‹ = QwenModel(QWEN_MODEL_PATH)
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        åŠ©æ•™ = AIåŠ©æ•™(å‘é‡æ•°æ®åº“, å‘é‡åŒ–å™¨, qwenæ¨¡å‹)
        log("ğŸ‰ AIåŠ©æ•™æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ!")
        
        # è¿”å›æˆåŠŸæ¶ˆæ¯
        return True
    except Exception as e:
        log(f"âŒ AIåŠ©æ•™åˆå§‹åŒ–å¤±è´¥: {str(e)}", "ERROR")
        return False

# ä¸»å‡½æ•°
if __name__ == '__main__':
    try:
        # åˆå§‹åŒ–æ™ºèƒ½ä½“
        if not åˆå§‹åŒ–åŠ©æ•™():
            log("âŒ AIåŠ©æ•™åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•å¯åŠ¨æœåŠ¡", "ERROR")
            sys.exit(1)
        
        # å¯åŠ¨æœåŠ¡å™¨
        app.run(host=SERVER_HOST, port=SERVER_PORT, threaded=True)
    except Exception as e:
        log(f"âŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {str(e)}", "ERROR")
        sys.exit(1)