import os
import sys
import json
import time
import numpy as np
import torch
import traceback
from flask import Flask, request, Response, render_template
from transformers import AutoTokenizer, AutoModelForCausalLM
from sentence_transformers import SentenceTransformer
import faiss
from accelerate import dispatch_model, infer_auto_device_map
from accelerate.utils import get_balanced_memory
from flask_cors import CORS

# é…ç½®å¸¸é‡
QWEN_MODEL_PATH = "/home/wangzy/zjuc/models/Qwen3-32B"  # æœ¬åœ°Qwen3-32Bæ¨¡å‹è·¯å¾„
VECTOR_DB_INDEX_PATH = "/home/wangzy/zjuc/agent/lectures/introduction/introduction_Mathematical_Representation_db/index.faiss"  # FAISSç´¢å¼•æ–‡ä»¶è·¯å¾„
VECTOR_DBS_CONFIG = [
    {
        "name": "Mathematical Representation",
        "index_path": "/home/wangzy/zjuc/agent/lectures/introduction/introduction_Mathematical_Representation_db/index.faiss",
        "metadata_path": "/home/wangzy/zjuc/agent/lectures/introduction/introduction_Mathematical_Representation_db/metadata.json"
    }, 
    {
        "name": "Course Syllabus", 
        "index_path": "/home/wangzy/zjuc/agent/lectures/introduction/introduction_Course_Syllabus_db/index.faiss", 
        "metadata_path": "/home/wangzy/zjuc/agent/lectures/introduction/introduction_Course_Syllabus_db/metadata.json"
    }, 
    {
        "name": "Machine Learning", 
        "index_path": "/home/wangzy/zjuc/agent/lectures/introduction/introduction_Machine_Learning_db/index.faiss", 
        "metadata_path": "/home/wangzy/zjuc/agent/lectures/introduction/introduction_Machine_Learning_db/metadata.json"
    }, 
    {
        "name": "What is Data Science", 
        "index_path": "/home/wangzy/zjuc/agent/lectures/introduction/introduction_What_is_Data_Science_db/index.faiss",
        "metadata_path": "/home/wangzy/zjuc/agent/lectures/introduction/introduction_What_is_Data_Science_db/metadata.json"
    }, 
    {
        "name": "Conclusion",
        "index_path": "/home/wangzy/zjuc/agent/lectures/introduction/introduction_Conclusion_db/index.faiss",
        "metadata_path": "/home/wangzy/zjuc/agent/lectures/introduction/introduction_Conclusion_db/metadata.json"
    }
]
VECTOR_DB_METADATA_PATH = "/home/wangzy/zjuc/agent/lectures/introduction/introduction_Mathematical_Representation_db/metadata.json"  # å…ƒæ•°æ®æ–‡ä»¶è·¯å¾„
# os.environ['SENTENCE_TRANSFORMERS_HOME'] = '/home/wangzy/zjuc/agent/models'
VECTOR_MODEL_PATH = "./models/sentence-transformers_all-mpnet-base-v2"  # æœ¬åœ°æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹è·¯å¾„
SERVER_HOST = "0.0.0.0"
SERVER_PORT = 5000

# åˆå§‹åŒ–Flaskåº”ç”¨
app = Flask(__name__, template_folder='/home/wangzy/zjuc/agent/templates/')
CORS(app)
app.config['JSON_AS_ASCII'] = False  # ç¦ç”¨ASCIIç¼–ç ï¼Œä¿®å¤ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜
start_time = time.time()

# ä¿®å¤ä¸­æ–‡Unicodeç¼–ç é—®é¢˜çš„å“åº”å‡½æ•°
def json_response(data, status=200):
    """è¿”å›æ­£ç¡®ç¼–ç çš„JSONå“åº”ï¼ˆè§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ï¼‰"""
    return Response(
        json.dumps(data, ensure_ascii=False),
        status=status,
        mimetype='application/json; charset=utf-8'
    )

# æ—¥å¿—é…ç½®
def log(message, level="INFO"):
    timestamp = time.strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{timestamp}] [{level}] {message}")

# GPUå·¥å…·å‡½æ•°
def clear_gpu_memory():
    """æ¸…ç†GPUæ˜¾å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
        log("GPUæ˜¾å­˜å·²æ¸…ç†")

def get_gpu_status():
    """è·å–GPUçŠ¶æ€ä¿¡æ¯"""
    status = []
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            torch.cuda.synchronize(i)
            status.append({
                "è®¾å¤‡": f"cuda:{i}",
                "åç§°": torch.cuda.get_device_name(i),
                "å†…å­˜(GB)": {
                    "æ€»é‡": round(torch.cuda.get_device_properties(i).total_memory / (1024**3), 2),
                    "å·²åˆ†é…": round(torch.cuda.memory_allocated(i) / (1024**3), 2),
                    "å·²é¢„ç•™": round(torch.cuda.memory_reserved(i) / (1024**3), 2),
                }
            })
    return status

# æ–‡æœ¬å‘é‡åŒ–å°è£…
class TextVectorizer:
    """æœ¬åœ°æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹å°è£…"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.model = None
        self.load_model()
    
    def load_model(self):
        """åŠ è½½æœ¬åœ°å‘é‡åŒ–æ¨¡å‹"""
        log(f"â³ æ­£åœ¨åŠ è½½æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹: {self.model_path}")
        try:
            self.model = SentenceTransformer(self.model_path, local_files_only=True, device="cuda")
            log(f"âœ… æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹åŠ è½½æˆåŠŸ! è®¾å¤‡: {self.model.device}")
        except Exception as e:
            log(f"âŒ æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", "ERROR")
            raise
    
    def vectorize(self, text: str) -> np.ndarray:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºå‘é‡"""
        log(f"å‘é‡åŒ–æ–‡æœ¬: {text[:50]}...")
        if not self.model:
            raise RuntimeError("å‘é‡åŒ–æ¨¡å‹æœªåˆå§‹åŒ–")
        
        # ç¡®ä¿è¾“å…¥æ˜¯å­—ç¬¦ä¸²
        if not isinstance(text, str):
            text = str(text)
        
        # å‘é‡åŒ–
        vector = self.model.encode(text, convert_to_numpy=True)
        return vector.reshape(1, -1)  # è¿”å›(1, dim)å½¢çŠ¶

# å‘é‡æ•°æ®åº“å°è£…
class VectorDatabase:
    """æœ¬åœ°å‘é‡æ•°æ®åº“å°è£…"""
    
    def __init__(self, db_configs: list):
        """åˆå§‹åŒ–å¤šæ•°æ®åº“ç³»ç»Ÿ"""
        # æ£€æŸ¥db_configsæ˜¯å¦ä¸ºåˆ—è¡¨
        if not isinstance(db_configs, list):
            raise TypeError(f"db_configså¿…é¡»æ˜¯åˆ—è¡¨ï¼Œå®é™…æ”¶åˆ°ç±»å‹: {type(db_configs)}")
        
        self.databases = []  # å­˜å‚¨æ‰€æœ‰æ•°æ®åº“
        self.å‘é‡ç»´åº¦ = None
        
        for config in db_configs:
            self.åŠ è½½æ•°æ®åº“(config)
    
    def åŠ è½½æ•°æ®åº“(self, config: dict):
        """åŠ è½½å•ä¸ªå‘é‡æ•°æ®åº“"""
        try:
            # ä»é…ç½®ä¸­æå–è·¯å¾„å‚æ•°
            name = config["name"]
            index_path = config["index_path"]
            metadata_path = config["metadata_path"]
            
            log(f"â³â³â³ æ­£åœ¨åŠ è½½å‘é‡æ•°æ®åº“ '{name}': {index_path}")
            
            # åŠ è½½FAISSç´¢å¼•
            index = faiss.read_index(index_path)
            å‘é‡ç»´åº¦ = index.d
            
            # åŠ è½½å…ƒæ•°æ®
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # å­˜å‚¨æ•°æ®åº“ä¿¡æ¯
            self.databases.append({
                "name": name,
                "index": index,
                "metadata": metadata,
                "ç»´åº¦": å‘é‡ç»´åº¦
            })
            
            # æ£€æŸ¥ç»´åº¦ä¸€è‡´æ€§
            if self.å‘é‡ç»´åº¦ is None:
                self.å‘é‡ç»´åº¦ = å‘é‡ç»´åº¦
            elif self.å‘é‡ç»´åº¦ != å‘é‡ç»´åº¦:
                log(f"âš ï¸ æ•°æ®åº“ '{name}' ç»´åº¦ä¸ä¸€è‡´ ({å‘é‡ç»´åº¦} vs {self.å‘é‡ç»´åº¦})", "WARNING")
            
            log(f"âœ… æ•°æ®åº“ '{name}' åŠ è½½æˆåŠŸ! æ–‡æ¡£æ•°: {len(metadata)}")
        except KeyError as e:
            log(f"âŒâŒ æ•°æ®åº“é…ç½®ç¼ºå°‘å¿…è¦é”®å€¼: {str(e)}", "ERROR")
            raise RuntimeError(f"æ•°æ®åº“é…ç½®ä¸å®Œæ•´ï¼Œç¼ºå°‘: {str(e)}")
        except FileNotFoundError as e:
            log(f"âŒâŒ æ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}", "ERROR")
            raise FileNotFoundError(f"æ–‡ä»¶æœªæ‰¾åˆ°: {str(e)}")
        except Exception as e:
            log(f"âŒâŒ æ•°æ®åº“åŠ è½½å¤±è´¥: {str(e)}", "ERROR")
            raise RuntimeError(f"æ•°æ®åº“åŠ è½½å¤±è´¥: {str(e)}")
    
    def æœç´¢(self, æŸ¥è¯¢å‘é‡: np.ndarray, k: int = 5) -> list:
        """åœ¨æ‰€æœ‰æ•°æ®åº“ä¸­æœç´¢ç›¸ä¼¼é¡¹"""
        if not self.databases:
            raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„å‘é‡æ•°æ®åº“")
        
        # åœ¨æ‰€æœ‰æ•°æ®åº“ä¸­æœç´¢
        æ‰€æœ‰ç»“æœ = []
        for db in self.databases:
            # æ£€æŸ¥ç»´åº¦
            if æŸ¥è¯¢å‘é‡.shape[1] != db["ç»´åº¦"]:
                log(f"âš ï¸ æŸ¥è¯¢å‘é‡ç»´åº¦({æŸ¥è¯¢å‘é‡.shape[1]})ä¸æ•°æ®åº“ '{db['name']}' ç»´åº¦({db['ç»´åº¦']})ä¸åŒ¹é…", "WARNING")
                continue
            
            try:
                # æ‰§è¡Œæœç´¢
                è·ç¦», ç´¢å¼• = db["index"].search(æŸ¥è¯¢å‘é‡, k)
            except Exception as e:
                log(f"âŒâŒ åœ¨æ•°æ®åº“ '{db['name']}' ä¸­æœç´¢å¤±è´¥: {str(e)}", "ERROR")
                continue
            
            # print(db)
            # print(ç´¢å¼•)
            # å¤„ç†ç»“æœ
            for i in range(len(ç´¢å¼•[0])):
                ç´¢å¼•å€¼ = int(ç´¢å¼•[0][i])  # æ·»åŠ int()è½¬æ¢
                if ç´¢å¼•å€¼ < 0 or ç´¢å¼•å€¼ >= len(db["metadata"]):
                    continue
                
                å…ƒæ•°æ® = db["metadata"]["metadata"][ç´¢å¼•å€¼]
                
                # æ„å»ºç»“æœå¯¹è±¡ (æ·»åŠ æ•°æ®åº“åç§°)
                ç»“æœé¡¹ = {
                    "ç›¸ä¼¼åº¦": float(è·ç¦»[0][i]),
                    "æ•°æ®åº“": db["name"],  # æ ‡è¯†æ¥æºæ•°æ®åº“
                    "æ–‡æ¡£æ ‡é¢˜": å…ƒæ•°æ®.get('doc_title', 'æ— æ ‡é¢˜'),
                    "ç« èŠ‚": å…ƒæ•°æ®.get("section", "æœªçŸ¥ç« èŠ‚"),
                    "å†…å®¹": å…ƒæ•°æ®.get("text", ""),
                    "æºç´¢å¼•": int(ç´¢å¼•å€¼)
                }
                æ‰€æœ‰ç»“æœ.append(ç»“æœé¡¹)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆå‡åºï¼Œè·ç¦»è¶Šå°è¶Šå¥½ï¼‰
        æ‰€æœ‰ç»“æœ.sort(key=lambda x: x["ç›¸ä¼¼åº¦"])
        
        # è¿”å›å‰kä¸ªç»“æœ
        return æ‰€æœ‰ç»“æœ[:k]
    
    def å¥åº·æ£€æŸ¥(self) -> dict:
        """æ£€æŸ¥æ•°æ®åº“å¥åº·çŠ¶å†µ"""
        status = {
            "æ•°æ®åº“æ•°é‡": len(self.databases),
            "å‘é‡ç»´åº¦": self.å‘é‡ç»´åº¦,
            "æ•°æ®åº“è¯¦æƒ…": []
        }
        
        for db in self.databases:
            status["æ•°æ®åº“è¯¦æƒ…"].append({
                "åç§°": db["name"],
                "æ–‡æ¡£æ•°é‡": len(db["metadata"]),
                "çŠ¶æ€": "æ­£å¸¸"
            })
        
        return status

# Qwenæ¨¡å‹å°è£…ï¼ˆæ”¯æŒå¤šGPUï¼‰
class QwenModel:
    """æœ¬åœ°Qwen3-32Bæ¨¡å‹å°è£…ï¼ˆæ”¯æŒåŒGPUï¼‰"""
    
    def __init__(self, model_path: str):
        self.model_path = model_path
        self.tokenizer = None
        self.model = None
        self.device_map = None
        self.device_count = torch.cuda.device_count()
        
        self.åŠ è½½æ¨¡å‹()
    
    def åŠ è½½æ¨¡å‹(self):
        """åŠ è½½Qwenæ¨¡å‹å’Œåˆ†è¯å™¨ï¼ˆæ”¯æŒå¤šGPUï¼‰"""
        log(f"â³ æ­£åœ¨åŠ è½½Qwenæ¨¡å‹ ({self.device_count} GPUs)...")
        start_time = time.time()
        
        try:
            # åŠ è½½åˆ†è¯å™¨
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                trust_remote_code=True
            )
            
            # åˆ›å»ºåŸºç¡€æ¨¡å‹
            model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                trust_remote_code=True,
                torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float16,
                device_map=None  # å…ˆä¸åˆ†é…è®¾å¤‡
            )
            
            # å¤šGPUè®¾å¤‡æ˜ å°„
            if self.device_count > 1:
                log(f"ğŸ”€ åˆ†é…æ¨¡å‹åˆ° {self.device_count} ä¸ªGPU...")
                
                # è®¡ç®—è®¾å¤‡å†…å­˜åˆ†å¸ƒ
                max_memory = get_balanced_memory(
                    model,
                    no_split_module_classes=model._no_split_modules,
                    dtype=model.dtype
                )
                
                # åˆ›å»ºè®¾å¤‡æ˜ å°„
                self.device_map = infer_auto_device_map(
                    model,
                    max_memory=max_memory,
                    no_split_module_classes=model._no_split_modules
                )
                
                # å°†æ¨¡å‹åˆ†å‘åˆ°å¤šä¸ªè®¾å¤‡
                self.model = dispatch_model(
                    model,
                    device_map=self.device_map,
                    main_device=0  # ä¸»è®¾å¤‡ä¸ºç¬¬ä¸€ä¸ªGPU
                )
                
                log(f"âœ… æ¨¡å‹å·²åˆ†é…åˆ°è®¾å¤‡: {self.device_map}")
            else:
                # å•GPUæƒ…å†µ
                device = "cuda:0" if torch.cuda.is_available() else "cpu"
                self.model = model.to(device)
                log(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {device}")
            
            load_time = time.time() - start_time
            log(f"åŠ è½½è€—æ—¶: {load_time:.1f}ç§’")
        except Exception as e:
            log(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}", "ERROR")
            raise
    
    def ç”Ÿæˆ(self, prompt: str, max_length: int = 512, temperature: float = 0.7) -> str:
        """ä½¿ç”¨Qwenæ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
        log(f"ç”Ÿæˆå“åº”: max_length={max_length}")
        if not self.model or not self.tokenizer:
            raise RuntimeError("æ¨¡å‹æœªæ­£ç¡®åˆå§‹åŒ–")
        
        # ç¼–ç è¾“å…¥å¹¶ç§»åŠ¨åˆ°ä¸»è®¾å¤‡
        device = self.ä¸»è®¾å¤‡
        inputs = self.tokenizer(
            prompt, 
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=512
        ).to(device)
        
        # ç”Ÿæˆå“åº”
        with torch.no_grad():
            outputs = self.model.generate(
                inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_new_tokens=max_length,
                temperature=temperature,
                top_p=0.9,
                repetition_penalty=1.1,
                do_sample=True,
                eos_token_id=self.tokenizer.eos_token_id,
                pad_token_id=self.tokenizer.pad_token_id
            )
        
        # è§£ç è¾“å‡º
        response = self.tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:], 
            skip_special_tokens=True
        )
        
        return response.strip()
    
    @property
    def ä¸»è®¾å¤‡(self):
        """è·å–ä¸»è®¾å¤‡"""
        if self.device_count > 0:
            return "cuda:0"
        return "cpu"
    
    def å¥åº·æ£€æŸ¥(self) -> dict:
        """æ£€æŸ¥æ¨¡å‹å¥åº·çŠ¶å†µ"""
        return {
            "æ¨¡å‹å·²åŠ è½½": self.model is not None,
            "åˆ†è¯å™¨å·²åŠ è½½": self.tokenizer is not None,
            "è®¾å¤‡æ•°é‡": self.device_count,
            "è®¾å¤‡æ˜ å°„": self.device_map,
            "ä¸»è®¾å¤‡": self.ä¸»è®¾å¤‡
        }

# AIåŠ©æ•™æ™ºèƒ½ä½“
class AIåŠ©æ•™:
    """AIåŠ©æ•™æ™ºèƒ½ä½“æ ¸å¿ƒç±»"""
    
    def __init__(self, å‘é‡åŒ–å™¨: TextVectorizer, qwenæ¨¡å‹: QwenModel, å‘é‡æ•°æ®åº“é…ç½®: list):
        self.å‘é‡åŒ–å™¨ = å‘é‡åŒ–å™¨
        self.qwenæ¨¡å‹ = qwenæ¨¡å‹
        self.å‘é‡æ•°æ®åº“ = VectorDatabase(å‘é‡æ•°æ®åº“é…ç½®)
    
    def æ£€ç´¢ç›¸å…³å†…å®¹(self, æŸ¥è¯¢: str, top_k: int = 3) -> list:
        """ä»çŸ¥è¯†åº“æ£€ç´¢ç›¸å…³å†…å®¹"""
        # å‘é‡åŒ–æŸ¥è¯¢æ–‡æœ¬
        try:
            æŸ¥è¯¢å‘é‡ = self.å‘é‡åŒ–å™¨.vectorize(æŸ¥è¯¢)
            log(f"ğŸ”¢ æŸ¥è¯¢å‘é‡åŒ–æˆåŠŸ, ç»´åº¦: {æŸ¥è¯¢å‘é‡.shape}")
        except Exception as e:
            log(f"âŒ å‘é‡åŒ–å¤±è´¥: {str(e)}", "ERROR")
            return []
        
        # æ‰§è¡Œæœç´¢
        ç»“æœ = self.å‘é‡æ•°æ®åº“.æœç´¢(æŸ¥è¯¢å‘é‡, top_k)
        
        # æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆ
        if not ç»“æœ:
            log("âš ï¸ æœç´¢æœªè¿”å›ä»»ä½•ç»“æœ", "WARNING")
            return []
            
        log(f"ğŸ” æ‰¾åˆ° {len(ç»“æœ)} ä¸ªç›¸å…³ç‰‡æ®µ")
        return ç»“æœ
    
    def ç”Ÿæˆå“åº”(self, æŸ¥è¯¢: str, ä¸Šä¸‹æ–‡: list, max_length: int = 512) -> str:
        """åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆå“åº”"""
        # æ„å»ºæç¤º
        æç¤º = self.æ„å»ºæç¤º(æŸ¥è¯¢, ä¸Šä¸‹æ–‡)
        
        # ç”Ÿæˆå“åº”
        log(f"ğŸ§  å¼€å§‹ç”Ÿæˆå“åº”: {æŸ¥è¯¢} (max_length={max_length})")
        try:
            log("âš¡ ä½¿ç”¨Qwenç”Ÿæˆå“åº”...")
            start_time = time.time()
            å“åº” = self.qwenæ¨¡å‹.ç”Ÿæˆ(æç¤º, max_length=max_length, temperature=0.7)
            gen_time = time.time() - start_time
            log(f"âœ… å“åº”ç”Ÿæˆå®Œæˆ (è€—æ—¶: {gen_time:.2f}ç§’)")
            return å“åº”
        except Exception as e:
            log(f"âŒ å“åº”ç”Ÿæˆå¤±è´¥: {str(e)}", "ERROR")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºé”™"
    
    def æ„å»ºæç¤º(self, æŸ¥è¯¢: str, ä¸Šä¸‹æ–‡: list) -> str:
        """æ„å»ºæç¤ºæ¨¡æ¿"""
        æç¤º = "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ•™ï¼Œè¯·åŸºäºä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”å­¦ç”Ÿçš„é—®é¢˜ã€‚\n\n"
        æç¤º += f"å­¦ç”Ÿé—®é¢˜: {æŸ¥è¯¢}\n\n"
        æç¤º += "ç›¸å…³å‚è€ƒèµ„æ–™:\n"
        
        for i, é¡¹ in enumerate(ä¸Šä¸‹æ–‡):
            æç¤º += f"[å‚è€ƒèµ„æ–™ {i+1}]\n"
            æç¤º += f"æ ‡é¢˜: {é¡¹['æ–‡æ¡£æ ‡é¢˜']}\n"
            æç¤º += f"ç« èŠ‚: {é¡¹['ç« èŠ‚']}\n"
            æç¤º += f"å†…å®¹: {é¡¹['å†…å®¹'][:300]}...\n\n"
        
        æç¤º += "\nè¯·æ ¹æ®ä»¥ä¸Šèµ„æ–™ï¼Œç”¨ä¸“ä¸šã€ç®€æ´çš„è¯­è¨€å›ç­”å­¦ç”Ÿé—®é¢˜ã€‚"
        æç¤º += """
                ### è¾“å‡ºæ ¼å¼è¦æ±‚ ###
                è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºï¼š
                1. å…ˆè¿›è¡Œè¯¦ç»†çš„æ€è€ƒè¿‡ç¨‹ï¼ˆåŒ…æ‹¬æ¨ç†ã€åˆ†æç­‰ï¼‰
                2. ç„¶åæ˜ç¡®æ ‡æ³¨"æœ€ç»ˆå›ç­”ï¼š"éƒ¨åˆ†
                3. åœ¨æœ€ç»ˆå›ç­”éƒ¨åˆ†ç»™å‡ºç®€æ´ä¸“ä¸šçš„å›ç­”
                æ€è€ƒè¿‡ç¨‹ï¼š
                """
        return æç¤º

    def respond_to_query(self, query: str, max_length=512, top_k=3) -> dict:
        """å¤„ç†ç”¨æˆ·æŸ¥è¯¢å¹¶ç”Ÿæˆå“åº”"""
        # 1. æ£€ç´¢ç›¸å…³å†…å®¹
        relevant_content = self.æ£€ç´¢ç›¸å…³å†…å®¹(query, top_k)
        
        # 2. ç”ŸæˆAIåŠ©æ•™å›ç­”
        ai_response = self.ç”Ÿæˆå“åº”(query, relevant_content, max_length)
        
        # 3. æ„å»ºå“åº”ç»“æ„
        return {
            "query": query,
            "response": ai_response,
            "sources": [
                {
                    "doc_title": item.get("æ–‡æ¡£æ ‡é¢˜", "æ— æ ‡é¢˜"),
                    "section": item.get("ç« èŠ‚", "æœªçŸ¥ç« èŠ‚"),
                    "page": item.get("page_idx", 0),
                    "similarity": item.get("ç›¸ä¼¼åº¦", 0.0),
                    "text_excerpt": item.get("å†…å®¹", "")[:300]
                }
                for item in relevant_content
            ]
        }

# å…¨å±€æ™ºèƒ½ä½“å®ä¾‹
åŠ©æ•™ = None

@app.route('/')
def home():
    """æä¾›å‰ç«¯ç•Œé¢"""
    return render_template('agent.html')

# åœ¨æé—®ç«¯ç‚¹æ·»åŠ å‚æ•°æ”¯æŒ

@app.route('/ask', methods=['POST'])
def handle_ask():
    try:
        # è·å–JSONæ•°æ®
        data = request.get_json()
        if not data or 'query' not in data:
            return json_response({"error": "ç¼ºå°‘æŸ¥è¯¢å‚æ•°"}, 400)
        
        start_time = time.time()

        # æå–å‚æ•°
        question = data['query']
        max_length = data.get('max_length', 512)
        top_k = data.get('top_k', 3)
        
        # ç¡®ä¿åŠ©æ•™å®ä¾‹å·²åˆå§‹åŒ–
        global åŠ©æ•™
        if not åŠ©æ•™:
            return json_response({"error": "AIåŠ©æ•™æœªåˆå§‹åŒ–"}, 500)
        
        # ä½¿ç”¨AIåŠ©æ•™ç”Ÿæˆå“åº”
        response_data = åŠ©æ•™.respond_to_query(question, max_length, top_k)

        # è®¡ç®—å®é™…è€—æ—¶ï¼ˆæ¯«ç§’ï¼‰
        elapsed_ms = (time.time() - start_time) * 1000
        response_data["response_time"] = round(elapsed_ms, 2)
        
        return json_response(response_data)
    except Exception as e:
        log(f"å¤„ç†æé—®æ—¶å‡ºé”™: {str(e)}\n{traceback.format_exc()}", "ERROR")
        return json_response({"error": "å†…éƒ¨æœåŠ¡å™¨é”™è¯¯"}, 500)

# def _build_cors_preflight_response():
#     response = Response()
#     response.headers.add("Access-Control-Allow-Origin", "*")
#     response.headers.add("Access-Control-Allow-Headers", "Content-Type")
#     response.headers.add("Access-Control-Allow-Methods", "POST")
#     return response

# åœ¨å¥åº·æ£€æŸ¥ç«¯ç‚¹æ·»åŠ æ›´å¤šçŠ¶æ€ä¿¡æ¯
@app.route('/health')
def health_check():
    try:
        # è·å–å‘é‡æ•°æ®åº“ä¿¡æ¯
        vector_db_info = {
            "vector_db_loaded": assistant.vector_db is not None,
            "database_count": len(assistant.vector_db.databases) if assistant.vector_db else 0,
            "databases": [db["name"] for db in assistant.vector_db.databases] if assistant.vector_db else []
        }
        
        return json_response({
            "status": "healthy",
            "version": "1.0.0",
            "uptime": f"{time.time() - start_time:.2f} seconds",
            "vector_db": vector_db_info
        })
    except Exception as e:
        return json_response({
            "status": "error",
            "message": str(e)
        }, 500)

@app.route('/gpu-status', methods=['GET'])
def gpuçŠ¶æ€():
    """GPUçŠ¶æ€ç«¯ç‚¹"""
    return json_response(get_gpu_status())

# åˆå§‹åŒ–å‡½æ•°
def åˆå§‹åŒ–åŠ©æ•™():
    """åˆå§‹åŒ–AIåŠ©æ•™æ™ºèƒ½ä½“ï¼ˆæ”¯æŒå¤šæ•°æ®åº“ï¼‰"""
    global åŠ©æ•™
    
    try:
        # æ¸…ç†æ˜¾å­˜
        clear_gpu_memory()
        
        # åˆå§‹åŒ–ç»„ä»¶
        log("ğŸš€ğŸš€ æ­£åœ¨åˆå§‹åŒ–æ–‡æœ¬å‘é‡åŒ–æ¨¡å‹...")
        å‘é‡åŒ–å™¨ = TextVectorizer(VECTOR_MODEL_PATH)
        
        log("ğŸš€ğŸš€ æ­£åœ¨åˆå§‹åŒ–å‘é‡æ•°æ®åº“...")
        # ä½¿ç”¨æ–°çš„å¤šæ•°æ®åº“é…ç½®
        å‘é‡æ•°æ®åº“é…ç½® = VECTOR_DBS_CONFIG
        
        log("ğŸš€ğŸš€ æ­£åœ¨åˆå§‹åŒ–Qwen3-32Bæ¨¡å‹...")
        qwenæ¨¡å‹ = QwenModel(QWEN_MODEL_PATH)
        
        # åˆ›å»ºæ™ºèƒ½ä½“ (ä¼ å…¥æ•°æ®åº“é…ç½®)
        åŠ©æ•™ = AIåŠ©æ•™(å‘é‡åŒ–å™¨, qwenæ¨¡å‹, å‘é‡æ•°æ®åº“é…ç½®)
        log(f"ğŸ‰ğŸ‰ AIåŠ©æ•™æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ! åŠ è½½äº† {len(å‘é‡æ•°æ®åº“é…ç½®)} ä¸ªçŸ¥è¯†åº“")
        
        return True
    except Exception as e:
        log(f"âŒâŒ AIåŠ©æ•™åˆå§‹åŒ–å¤±è´¥: {str(e)}", "ERROR")
        return False

# ä¸»å‡½æ•°
if __name__ == '__main__':
    try:
        # åˆå§‹åŒ–æ™ºèƒ½ä½“
        if not åˆå§‹åŒ–åŠ©æ•™():
            log("âŒ AIåŠ©æ•™åˆå§‹åŒ–å¤±è´¥ï¼Œæ— æ³•å¯åŠ¨æœåŠ¡", "ERROR")
            sys.exit(1)
        
        # å¯åŠ¨æœåŠ¡å™¨
        app.run(host=SERVER_HOST, port=SERVER_PORT, threaded=True, debug=True)
    except Exception as e:
        log(f"âŒ ç³»ç»Ÿå¯åŠ¨å¤±è´¥: {str(e)}", "ERROR")
        sys.exit(1)