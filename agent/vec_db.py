import json
import numpy as np
import os
import faiss
from typing import List, Dict, Any
from pathlib import Path

class VectorDatabase:
    """å‘é‡æ•°æ®åº“ç±»ï¼Œç”¨äºé«˜æ•ˆå­˜å‚¨å’Œæ£€ç´¢å‘é‡åŒ–å†…å®¹"""
    
    def __init__(self, index_path: str = "vector_index", metadata_path: str = "metadata.json"):
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        
        å‚æ•°:
            index_path: ä¿å­˜FAISSç´¢å¼•çš„è·¯å¾„
            metadata_path: ä¿å­˜å…ƒæ•°æ®çš„JSONè·¯å¾„
        """
        self.index_path = index_path
        self.metadata_path = metadata_path
        self.index = None
        self.metadata = []
        self.vector_dim = None
        
        # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        Path(self.index_path).parent.mkdir(parents=True, exist_ok=True)
    
    def build_from_vectors_file(self, vectors_file: str) -> None:
        """
        ä»å‘é‡åŒ–çš„JSONæ–‡ä»¶æ„å»ºæ•°æ®åº“
        
        å‚æ•°:
            vectors_file: åŒ…å«å‘é‡åŒ–æ•°æ®çš„JSONæ–‡ä»¶è·¯å¾„
        """
        # åŠ è½½æ•°æ®
        with open(vectors_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        self.vector_dim = 768
        self.metadata = []
        
        # æå–å‘é‡å’Œå…ƒæ•°æ®
        all_vectors = []
        for i, block in enumerate(data['blocks']):
            vector = block['vector']
            self.metadata.append({
                "block_id": block['id'],
                "text": block['text'][:500],  # ä¿å­˜å‰500ä¸ªå­—ç¬¦ä»¥ä¾¿å±•ç¤º
                "full_text": block['text'],
                "page": block['page'],
                # "section": block['section'],
                # "doc_title": block['doc_metadata']['document_title'],
                # "doc_author": block['doc_metadata']['author'],
                "index": i  # ä¿å­˜åŸå§‹ç´¢å¼•ä½ç½®
            })
            all_vectors.append(vector)
        
        # è½¬æ¢ä¸ºNumPyæ•°ç»„
        vectors = np.array(all_vectors, dtype=np.float32)
        
        # åˆ›å»ºå¹¶å¡«å……FAISSç´¢å¼•
        self.index = faiss.IndexFlatIP(self.vector_dim)  # ä½¿ç”¨å†…ç§¯ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        self.index.add(vectors)
        print(f"âœ… æˆåŠŸæ„å»ºå‘é‡æ•°æ®åº“ï¼åŒ…å« {len(vectors)} ä¸ªå‘é‡")
    
    def search(self, query_vector: np.ndarray, k: int = 5) -> List[Dict[str, Any]]:
        """
        åœ¨æ•°æ®åº“ä¸­æœç´¢ç›¸ä¼¼é¡¹
        
        å‚æ•°:
            query_vector: æŸ¥è¯¢å‘é‡ï¼ˆå¿…é¡»æ˜¯äºŒç»´æ•°ç»„ï¼Œshape=(1, vector_dim)ï¼‰
            k: è¿”å›æœ€ç›¸ä¼¼çš„kä¸ªç»“æœ
            
        è¿”å›:
            åŒ…å«åŒ¹é…ç»“æœå…ƒæ•°æ®çš„å­—å…¸åˆ—è¡¨
        """
        if self.index is None:
            raise RuntimeError("æ•°æ®åº“å°šæœªæ„å»ºï¼Œè¯·å…ˆè°ƒç”¨build_from_vectors_file()")
        
        # ç¡®ä¿æŸ¥è¯¢å‘é‡æ ¼å¼æ­£ç¡®ï¼ˆäºŒç»´æ•°ç»„ï¼‰
        if isinstance(query_vector, list):
            query_vector = np.array([query_vector], dtype=np.float32)
        elif query_vector.ndim == 1:
            query_vector = query_vector.reshape(1, -1)  # ä¿®æ­£ï¼šå°†ä¸€ç»´è½¬ä¸ºäºŒç»´
        
        # æ‰§è¡Œæœç´¢
        distances, indices = self.index.search(query_vector, k)
        
        # è·å–ç»“æœå…ƒæ•°æ®
        results = []
        for i in range(len(indices[0])):
            idx = indices[0][i]
            results.append({
                "similarity": float(distances[0][i]),
                **self.metadata[idx]
            })
        
        return results
    
    def text_search(self, query: str, model, k: int = 5) -> List[Dict[str, Any]]:
        """
        é€šè¿‡æ–‡æœ¬æŸ¥è¯¢è¿›è¡Œæœç´¢ï¼ˆéœ€è¦ä¼ å…¥å‘é‡åŒ–æ¨¡å‹ï¼‰
        
        å‚æ•°:
            query: æŸ¥è¯¢æ–‡æœ¬
            model: å‘é‡åŒ–æ¨¡å‹å®ä¾‹
            k: è¿”å›æœ€ç›¸ä¼¼çš„kä¸ªç»“æœ
            
        è¿”å›:
            åŒ…å«åŒ¹é…ç»“æœå…ƒæ•°æ®çš„å­—å…¸åˆ—è¡¨
        """
        # å‘é‡åŒ–æŸ¥è¯¢æ–‡æœ¬
        query_vector = model.vectorize([query])[0]
        return self.search(query_vector, k)
    
    def save(self) -> None:
        """ä¿å­˜æ•°æ®åº“åˆ°ç£ç›˜"""
        if self.index is None:
            raise RuntimeError("æ•°æ®åº“å°šæœªæ„å»ºï¼Œæ— æ³•ä¿å­˜")
        
        # ä¿å­˜FAISSç´¢å¼•
        faiss.write_index(self.index, self.index_path)
        
        # ä¿å­˜å…ƒæ•°æ®
        with open(self.metadata_path, 'w', encoding='utf-8') as f:
            json.dump({
                "metadata": self.metadata,
                "vector_dim": self.vector_dim
            }, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ’¾ æ•°æ®åº“å·²ä¿å­˜åˆ°: {self.index_path} å’Œ {self.metadata_path}")
    
    def load(self) -> None:
        """ä»ç£ç›˜åŠ è½½æ•°æ®åº“"""
        # åŠ è½½FAISSç´¢å¼•
        self.index = faiss.read_index(self.index_path)
        
        # åŠ è½½å…ƒæ•°æ®
        with open(self.metadata_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            self.metadata = data["metadata"]
            self.vector_dim = data["vector_dim"]
        
        print(f"ğŸ” æ•°æ®åº“å·²åŠ è½½! åŒ…å« {len(self.metadata)} ä¸ªå‘é‡")

# ä¸»å‡½æ•°ï¼šæ„å»ºå’Œä½¿ç”¨å‘é‡æ•°æ®åº“
if __name__ == "__main__":
    # é…ç½®æ–‡ä»¶è·¯å¾„
    VECTORS_FILE = "./lectures/introduction/introduction_Course_Syllabus_vectorized.json"  # ä¹‹å‰ç”Ÿæˆçš„å‘é‡åŒ–æ–‡ä»¶
    MODEL_PATH = "models/sentence-transformers_all-mpnet-base-v2"  # å‘é‡åŒ–æ¨¡å‹è·¯å¾„
    DB_PATH = "./lectures/introduction/introduction_Course_Syllabus_db"  # æ•°æ®åº“ä¿å­˜è·¯å¾„
    
    # 1. åˆ›å»ºå‘é‡æ•°æ®åº“å®ä¾‹
    print("=" * 60)
    print("ğŸ“š æ„å»ºå‘é‡æ•°æ®åº“...")
    db = VectorDatabase(
        index_path=f"{DB_PATH}/index.faiss", 
        metadata_path=f"{DB_PATH}/metadata.json"
    )
    
    # 2. ä»å‘é‡åŒ–æ–‡ä»¶æ„å»ºæ•°æ®åº“
    db.build_from_vectors_file(VECTORS_FILE)
    
    # 3. ä¿å­˜æ•°æ®åº“ä»¥ä¾¿åç»­ä½¿ç”¨
    os.makedirs(DB_PATH, exist_ok=True)
    db.save()
    
    # 4. æµ‹è¯•æŸ¥è¯¢ (éœ€å…ˆåŠ è½½æ¨¡å‹)
    print("\nğŸ” æµ‹è¯•æŸ¥è¯¢åŠŸèƒ½...")
    
    # åŠ è½½å‘é‡åŒ–æ¨¡å‹ (ä½¿ç”¨ä¹‹å‰çš„å®ç°)
    print(f"åŠ è½½æ¨¡å‹: {MODEL_PATH}")
    # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å¯¼å…¥æ‚¨ä¹‹å‰çš„å‘é‡åŒ–æ¨¡å‹å®ç°
    # from minimal_vectorizer import MinimalVectorizer  # å–æ¶ˆæ³¨é‡Šå¹¶ä½¿ç”¨æ‚¨çš„å®ç°
    # vectorizer = MinimalVectorizer(MODEL_PATH)
    
    # ç”±äºå¯¼å…¥å¯èƒ½å¤æ‚ï¼Œè¿™é‡Œç®€åŒ–æµ‹è¯•
    print("ï¼ˆåœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œéœ€è¦åŠ è½½å‘é‡åŒ–æ¨¡å‹æ¥å¤„ç†æ–‡æœ¬æŸ¥è¯¢ï¼‰")
    
    # 5. ç¤ºä¾‹æœç´¢ - ä½¿ç”¨é¢„å…ˆåŠ è½½çš„æ•°æ®åº“
    print("\nğŸ”„ é‡æ–°åŠ è½½æ•°æ®åº“è¿›è¡Œæµ‹è¯•...")
    reloaded_db = VectorDatabase(
        index_path=f"{DB_PATH}/index.faiss", 
        metadata_path=f"{DB_PATH}/metadata.json"
    )
    reloaded_db.load()
    
    # 6. æ¨¡æ‹ŸæŸ¥è¯¢ - ä¿®æ­£å‘é‡å½¢çŠ¶
    print("\nç¤ºä¾‹æœç´¢ç»“æœ:")
    # åˆ›å»ºæ­£ç¡®å½¢çŠ¶çš„æŸ¥è¯¢å‘é‡ (1 x vector_dim)
    example_query_vector = np.random.randn(1, reloaded_db.vector_dim).astype(np.float32)
    
    # æ‰§è¡Œæœç´¢
    results = reloaded_db.search(example_query_vector, k=3)
    
    # æ˜¾ç¤ºç»“æœ - ä½¿ç”¨å®é™…å­˜åœ¨çš„å…ƒæ•°æ®å­—æ®µ
    for i, res in enumerate(results):
        print(f"\nç»“æœ #{i+1}")
        print(f"  ç›¸ä¼¼åº¦: {res['similarity']:.4f}")
        # ä½¿ç”¨å®é™…ä¿å­˜çš„å­—æ®µ
        print(f"  å—ID: {res['block_id']}")
        print(f"  å†…å®¹: {res['text']}...")
        print(f"  é¡µç : {res['page']}")
        print(f"  åŸå§‹ç´¢å¼•: {res['index']}")
    
    print("\nâœ¨ å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆï¼Œå¯ä»¥å¼€å§‹ä½¿ç”¨ï¼")
